# Vanilla Transformer Baseline - 22M Scale, 5B tokens
# Goal: DAWN v17.1 20M vs Baseline 22M - equal parameter comparison
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
    - train/c4/c4_raw_001.pt
    - train/c4/c4_raw_002.pt
    - train/c4/c4_raw_003.pt
    - train/c4/c4_raw_004.pt
    - train/c4/c4_raw_005.pt
    - train/c4/c4_raw_006.pt
    - train/c4/c4_raw_007.pt
    - train/c4/c4_raw_008.pt
    - train/c4/c4_raw_009.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 5000000000
  max_val_tokens: 2000000
model:
  model_version: "baseline"
  d_model: 384
  n_layers: 6
  n_heads: 6
  d_ff: 1536
  max_seq_len: 512
  dropout: 0.1
training:
  batch_size: 96
  num_epochs: 1
  lr: 0.00055
  weight_decay: 0.1
  warmup_ratio: 0.06
  orthogonality_weight: 0.0
  diversity_weight: 0.0
  load_balance_weight: 0.0
  process_norm_weight: 0.0
use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_baseline_22M_c4_5B
log_dir: /content/drive/MyDrive/dawn/logs_baseline_22M_c4_5B
# Parameters: ~22.3M
# Standard config: d_ff=4Ã—d_model, head_dim=64, Pre-LN, Weight tying
