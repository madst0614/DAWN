# DAWN v8.0 Training Config (Householder + GELU Ablation)
# SharedNeurons + NeuronMemory with GELU after Householder transform

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/wikitext_5to1_texts.pkl
  val_files:
    - validation/wikitext_5to1_texts.pkl

# Model Architecture (v8.0 - Householder + GELU)
model:
  model_version: "8.0"
  d_model: 256
  n_layers: 4
  n_heads: 4

  # TransformNeurons parameters
  rank: 64
  n_input: 16
  n_process: 32
  n_output: 16
  process_k: 3

  # KnowledgeNeurons parameters
  n_knowledge: 64
  knowledge_k: 8

  # Architecture
  max_seq_len: 128
  dropout: 0.1

  # Ablation: add GELU after each Householder transform
  householder_nonlinearity: true

# Training
training:
  batch_size: 128
  num_epochs: 30
  lr: 0.0003
  weight_decay: 0.1
  warmup_epochs: 2

  orthogonality_weight: 0.01
  process_norm_weight: 0.01
  load_balance_weight: 0.01

use_amp: true

checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v8_householder_gelu
log_dir: /content/drive/MyDrive/dawn/logs_v8_householder_gelu

# Ablation study:
#   - Householder transform: H @ x = x - 2v(v.Tx)/||v||^2 (reflection)
#   - With GELU: output = GELU(H @ x)
#   - Adds nonlinearity after each reflection, breaking pure linear transform
#   - Hypothesis: GELU helps? Or does it break Householder's properties?
