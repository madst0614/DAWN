# DAWN v18.2 Training Config - 39M Scale, 2B tokens
# ReLU-Masked Learnable Tau Routing (Q/K separated, 8 pools)
# Token-level routing for both attention and knowledge (no SSM context)
# QK=256, V=1056, Knowledge=640, rank=16, knowledge_rank=16, d_space=64

seed: 1

data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
    - train/c4/c4_raw_001.pt
    - train/c4/c4_raw_002.pt
    - train/c4/c4_raw_003.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 2000000000
  max_val_tokens: 2000000

model:
  model_version: "18.2"
  vocab_size: 30000
  d_model: 384
  n_layers: 12
  n_heads: 6
  rank: 16
  state_dim: 64

  # v18.2: Multi-path routing parameters
  max_paths: 4
  path_max_k: 16
  learnable_tau: true
  fixed_tau: 0.0

  # Attention - Q/K shared pool (4x scale from 20M)
  n_feature_qk: 256
  n_feature_v: 1056
  n_restore_qk: 256
  n_restore_v: 1056

  # Knowledge - Feature-Restore pattern (4x scale from 20M)
  n_feature_know: 640
  n_restore_know: 640
  knowledge_rank: 16

  max_seq_len: 512
  dropout: 0.1
  gradient_checkpointing: true
  d_space: 64
  router_dropout: 0.1
  attention_token_routing: true
  knowledge_token_routing: true
  use_ssm_context: false

training:
  batch_size: 64
  gradient_accumulation_steps: 2
  num_epochs: 1
  lr: 0.0005
  weight_decay: 0.1
  warmup_ratio: 0.06
  orthogonality_weight: 0.01
  diversity_weight: 0.001
  load_balance_weight: 0.01
  tau_reg_weight: 0.001

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v18.2_39M_c4_2B
log_dir: /content/drive/MyDrive/dawn/logs_v18.2_39M_c4_2B

# v18.2 ReLU-Masked Learnable Tau Architecture
#
# Key differences from v18.1:
#   - ReLU-based hard mask instead of sigmoid penalty
#   - Q/K tau separated (8 pools instead of 6)
#   - Removed: soft_mask_temp, soft_mask_penalty, path_min_k
#
# Neuron selection flow:
#   x -> tau_proj(x) -> tau [B, S, 8] (token-level, Q/K separated)
#   gate = ReLU(scores - tau) + 1e-8
#   log_gate = log(gate).clamp(min=-20)
#   masked_scores = scores + log_gate
#   weights = softmax(masked_scores)
#   chunk by score rank -> 1~max_paths paths -> sum Q, K, V
#
# Architecture (39M, 3904 neurons):
#   Attention Circuit (per-token routing):
#   - Feature_QK=256: per-token Q/K compression
#   - Feature_V=1056: per-token V compression
#   - Restore_QK=256: per-token Q/K restoration
#   - Restore_V=1056: per-token V restoration
#
#   Knowledge Circuit (per-token routing):
#   - Feature_Know=640: per-token compression
#   - Restore_Know=640: per-token restoration
