# Vanilla Transformer Baseline - 125M Scale, 5B tokens
# Following OPT-125M official configuration (Meta AI, 2022)

data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
    - train/c4/c4_raw_001.pt
    - train/c4/c4_raw_002.pt
    - train/c4/c4_raw_003.pt
    - train/c4/c4_raw_004.pt
    - train/c4/c4_raw_005.pt
    - train/c4/c4_raw_006.pt
    - train/c4/c4_raw_007.pt
    - train/c4/c4_raw_008.pt
    - train/c4/c4_raw_009.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 5000000000
  max_val_tokens: 2000000

model:
  model_version: "baseline"
  d_model: 768
  n_layers: 12
  n_heads: 12
  d_ff: 3072
  max_seq_len: 512
  dropout: 0.1

training:
  batch_size: 64
  num_epochs: 1
  lr: 0.0004
  weight_decay: 0.1
  warmup_ratio: 0.06

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_baseline_125M_c4_5B
log_dir: /content/drive/MyDrive/dawn/logs_baseline_125M_c4_5B

# Parameter calculation:
# Embedding: 30522 × 768 = 23.4M
# Per layer: 768×768×4 (attn) + 768×3072×2 (ffn) = 2.36M + 4.72M = 7.08M
# 12 layers: 7.08M × 12 = 84.9M
# Total: ~108M (close to OPT-125M with different vocab size)
#
# Configuration source: OPT-125M (Meta AI, 2022)
# "OPT: Open Pre-trained Transformer Language Models"
# https://arxiv.org/abs/2205.01068
#
# OPT-125M official config:
# - d_model: 768
# - n_layers: 12
# - n_heads: 12
# - d_ff: 3072 (4 × d_model)
# - head_dim: 64
#
# Note: batch_size reduced to 64 for memory constraints
# Note: lr reduced following OPT scaling (larger models use smaller lr)
