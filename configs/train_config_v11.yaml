# DAWN v11.0 Training Config
# 실험: rank=16, 뉴런 4배 (v10 대비 동일 파라미터)
# 가설: 작은 뉴런 많이 조합 > 큰 뉴런 적게 조합

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 100000000  # 100M tokens
  max_val_tokens: 5000000      # 5M tokens

# Model Architecture
model:
  model_version: "10.0"  # v10 모델 사용, 설정만 다름 (rank=16 실험)
  d_model: 256
  n_layers: 4
  n_heads: 4
  rank: 16                # 64 → 16 (4배 압축)

  # 뉴런 수 4배 증가 (파라미터 동일 유지)
  n_compress: 256         # 64 → 256
  n_expand: 64            # 16 → 64

  # Top-K Selection (v11 핵심)
  compress_top_k: 16      # 256개 중 16개 선택 (6.25%)
  expand_top_k: 8         # 64개 중 8개 선택 (12.5%)
  router_noise: 0.1       # 학습 시 탐색용 noise

  n_knowledge: 64
  knowledge_k: 8
  max_seq_len: 128
  dropout: 0.1

# Training
training:
  batch_size: 128
  num_epochs: 3
  lr: 0.0003
  weight_decay: 0.1
  warmup_ratio: 0.05

  # Regularization
  orthogonality_weight: 0.01
  diversity_weight: 0.1
  load_balance_weight: 0.01   # top-k에서 중요
  process_norm_weight: 0.0

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v11
log_dir: /content/drive/MyDrive/dawn/logs_v11

# =============================================================================
# v10 vs v11 비교 (동일 파라미터)
# =============================================================================
#
# | 항목              | v10.0             | v11.0              |
# |-------------------|-------------------|---------------------|
# | rank              | 64                | 16                  |
# | n_compress        | 64                | 256                 |
# | n_expand          | 16                | 64                  |
# | Routing           | Soft (all)        | Hard (top-k)        |
# | compress params   | 64×256×64=1.05M   | 256×256×16=1.05M    |
# | expand params     | 16×64×256=0.26M   | 64×16×256=0.26M     |
# | 표현력 (top-k×rank)| 8×64=512         | 16×16=256           |
#
# =============================================================================
