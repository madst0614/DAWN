# DAWN v17 Training Config - 11M Scale, WikiText
# Hierarchical Neuron Circuits with Neuron Embeddings
# Scaled down from v17 22M with optimized ratios

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/wikitext_5to1_texts.pkl
  val_files:
    - val/wikitext_5to1_texts.pkl

# Model Architecture
model:
  model_version: "17.0"
  d_model: 320
  n_layers: 4
  n_heads: 4
  max_seq_len: 128
  dropout: 0.1

  # Circuit Architecture
  # Circuit = team of cooperating neurons [neurons_per_circuit, d_model]
  # 2-level routing: Circuit selection (top-k) + Neuron weighting (softmax)
  neurons_per_circuit: 64

  # Feature Circuits (compression: d_model → neurons)
  # 22M ratio applied: FR increased (bottleneck fix), FV decreased
  n_circuits_r: 24              # 12→24 (병목 해소, 22M 비율 적용)
  n_circuits_v: 6               # 12→6 (과잉 감소)
  top_k_circuits_r: 4           # 비율 6:1 유지
  top_k_circuits_v: 1

  # Expansion Circuits (expansion: neurons → d_model)
  # 22M ratio applied: Rel decreased, Val decreased
  n_circuits_rel: 24            # 48→24 (과잉 감소, 22M에서 68개만 active)
  n_circuits_val: 4             # 12→4 (과잉 감소)
  top_k_circuits_rel: 4         # 비율 6:1 유지
  top_k_circuits_val: 1

  # Knowledge Neurons
  n_knowledge: 128              # 80→128 (살짝 증가)
  coarse_k: 20
  fine_k: 10
  knowledge_rank: 128

  # SSM & Router
  state_dim: 64
  d_space: 64
  router_dropout: 0.1
  use_ssm_context: true

  # Excitability & Langevin dynamics
  excitability_tau: 1.5
  excitability_ema_alpha: 0.01
  langevin_alpha: 0.0003
  langevin_beta: 0.0006

  gradient_checkpointing: false

# Training
training:
  batch_size: 128
  num_epochs: 30
  lr: 0.0003
  weight_decay: 0.1
  warmup_epochs: 2

  # Auxiliary losses
  circuit_diversity_weight: 0.1       # circuit 간 다양성
  neuron_orthogonality_weight: 0.01   # circuit 내 neuron 직교성

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v17_11M_wikitext
log_dir: /content/drive/MyDrive/dawn/logs_v17_11M_wikitext

# =============================================================================
# v17 11M Architecture Summary
# =============================================================================
#
# Scaling from 22M → 11M:
#   - d_model: 384 → 320
#   - n_layers: 12 → 4
#   - circuits: 22M 비율 유지하며 축소
#
# 22M Optimization Insights Applied:
#   - Feature_R was bottleneck → increased ratio (24 circuits, was 12)
#   - Feature_V was excess → decreased (6 circuits, was 12)
#   - Relational only 68/96 active → decreased (24 circuits, was 48)
#   - Value was excess → decreased (4 circuits, was 12)
#
# Circuit Ratio Comparison:
#   v16 11M:  FR:FV:Rel:Val = 12:12:48:12 = 1:1:4:1
#   v17 11M:  FR:FV:Rel:Val = 24:6:24:4   = 6:1.5:6:1 (22M 비율)
#
# Parameter Breakdown:
#   Feature_R:   24 × 64 × 320 = 491K   (+245K, 병목 해소)
#   Feature_V:   6 × 64 × 320 = 123K    (-123K)
#   Relational:  24 × 64 × 320 = 491K   (-492K)
#   Value:       4 × 64 × 320 = 82K     (-164K)
#   Neuron Emb:  (24+6+24+4) × 64 × 64 = 237K (신규)
#   ─────────────────────────────────────────────
#   Circuit Total: ~1.4M (vs v16 11M ~1.7M)
#
# =============================================================================
