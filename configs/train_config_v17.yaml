# DAWN v17 Training Configuration
# Full Vector Neurons - No Excitability

model:
  model_version: "17.0"
  d_model: 320
  n_layers: 4
  n_heads: 4

  # Compression neurons
  n_feature_qk: 128
  n_feature_v: 64
  top_k_feature_qk: 64
  top_k_feature_v: 32

  # Expansion neurons
  n_relational_q: 256
  n_relational_k: 256
  n_value: 128
  top_k_relational: 64
  top_k_value: 32

  # Knowledge
  n_knowledge: 80
  coarse_k: 20
  fine_k: 10
  knowledge_rank: 128

  # Other
  state_dim: 64
  d_space: 64
  dropout: 0.1
  router_dropout: 0.1
  gradient_checkpointing: false
  use_ssm_context: true

training:
  batch_size: 32
  gradient_accumulation_steps: 4
  num_epochs: 10
  max_seq_len: 512
  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_steps: 2000
  max_grad_norm: 1.0
  aux_loss_weight: 0.03
  orth_loss_weight: 0.0  # No orthogonality loss (all vectors)
  knowledge_div_weight: 0.001

  optimizer:
    type: adamw
    betas: [0.9, 0.95]
    eps: 1.0e-8

data:
  dataset_path: "data/train.txt"
  val_dataset_path: "data/val.txt"
  vocab_size: 30000
  tokenizer_path: "tokenizers/bpe_30k.json"
  num_workers: 4
  prefetch_factor: 2

logging:
  log_interval: 100
  eval_interval: 1000
  save_interval: 5000
  output_dir: "outputs/v17"
  wandb_project: "dawn-v17"
  wandb_run_name: "v17-full-vector"

checkpoint:
  save_total_limit: 3
  resume_from: null
