# DAWN v13.3 Training Config - 20M Scale, 500M tokens
# Composition over Depth: 4 layers with large QK pool
# Based on train_config_v13_2_12M_c4_500M.yaml

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
    - train/c4/c4_raw_001.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 500000000   # 500M tokens
  max_val_tokens: 2000000        # 2M tokens

# Model Architecture
model:
  model_version: "13.3"
  d_model: 384                   # 320 → 384
  n_layers: 4
  n_heads: 4
  rank: 64
  state_dim: 64

  # QK 중심 뉴런 배분 (12M 대비 ~4배)
  n_compress: 96                 # 24 → 96
  n_expand_QK: 200               # 48 → 200 (핵심!)
  n_expand_V: 48                 # 12 → 48

  n_knowledge: 160               # 80 → 160
  knowledge_k: 14                # 10 → 14
  knowledge_rank: 64

  max_seq_len: 512
  dropout: 0.1
  gradient_checkpointing: true

  # Top-k (조합력 증가)
  top_k_compress: 12             # 8 → 12
  top_k_QK: 20                   # 10 → 20
  top_k_V: 8                     # 4 → 8

  d_space: 64
  router_dropout: 0.1
  token_routing: false
  use_ssm_context: true

# Training
training:
  batch_size: 256
  num_epochs: 10
  lr: 0.0004
  weight_decay: 0.1
  warmup_ratio: 0.06
  orthogonality_weight: 0.01
  diversity_weight: 0.1
  load_balance_weight: 0.0
  entropy_weight: 0.0
  process_norm_weight: 0.0

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v13_3_20M_c4_500M
log_dir: /content/drive/MyDrive/dawn/logs_v13_3_20M_c4_500M

# v13.3 Changes (vs 12M):
#   - d_model: 320 → 384
#   - n_compress: 24 → 96
#   - n_expand_QK: 48 → 200 (4배, 핵심)
#   - n_expand_V: 12 → 48
#   - n_knowledge: 80 → 160
#   - top_k: 8/10/4 → 12/20/8
#
# 핵심 가설:
#   "QK 뉴런 조합으로 관계 표현"
#   "Layer 수 유지, 뉴런 풀 확장"
#   "Depth 대신 Composition"
#
# 예상 파라미터: ~20M
