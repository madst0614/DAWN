# DAWN v12.5 Training Config - 20M Scale, 12 Layers
# C4 dataset, Global SSM + Global Router

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 100000000   # 100M tokens
  max_val_tokens: 1638400       # ~1.6M tokens (50 batches)

# Model Architecture
model:
  model_version: "12.5"
  d_model: 320
  n_layers: 12
  n_heads: 4
  rank: 64
  state_dim: 64
  n_compress: 288
  n_expand: 72
  n_knowledge: 320
  knowledge_k: 14
  max_seq_len: 128
  dropout: 0.1

# Training
training:
  batch_size: 256
  num_epochs: 4
  lr: 0.0004
  weight_decay: 0.1
  warmup_ratio: 0.05
  orthogonality_weight: 0.01
  diversity_weight: 0.1
  load_balance_weight: 0.01
  process_norm_weight: 0.0

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v12_5_20M_c4
log_dir: /content/drive/MyDrive/dawn/logs_v12_5_20M_c4

# Summary:
#   Train: 100M × 4 epochs = 400M tokens (Chinchilla optimal)
#   Val: 50 batches
#   Steps/epoch: ~3,052
#   Total steps: ~12,208
#
# v12.5 파라미터 절약 (vs v12.3):
#   Global SSM: 24 -> 1 (per-layer SSM 제거)
#   Global Router: 60 -> 5 (per-layer router 제거)
#
# 파라미터 계산:
#   Embedding: 30522 × 320 = 9.77M (weight tying)
#   Compress: 288 × 320 × 64 = 5.90M
#   Expand Pool (QKV): 72 × 64 × 320 = 1.47M
#   Knowledge: 320 × (64 + 320) = 0.12M
#   Global SSM: 64×64 + 320×64 + 64×320 + 64×320 = ~0.05M
#   Global Routers: 5 × (320 × ~avg) = ~0.5M
#   Per Layer × 12 (expand_O + norms only): ~0.11M × 12 = 1.32M
#   Total: ~19.1M (약간 적음, SSM/Router 공유로 절약)
