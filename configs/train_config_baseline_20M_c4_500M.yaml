# Vanilla Transformer Baseline - 20M Scale
# Goal: v15 20M vs Baseline 20M - 동등 파라미터 비교

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
    - train/c4/c4_raw_001.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 500000000  # 500M tokens
  max_val_tokens: 2000000      # 2M tokens

# Model Architecture
model:
  model_version: "baseline"
  d_model: 320
  n_layers: 8              # 20M 맞추기
  n_heads: 4
  d_ff: 1344               # 4.2 * d_model (20.1M 맞추기)
  max_seq_len: 512
  dropout: 0.1

# Training (same as DAWN v15)
training:
  batch_size: 96
  num_epochs: 3
  lr: 0.00055
  weight_decay: 0.1
  warmup_ratio: 0.06
  orthogonality_weight: 0.0
  diversity_weight: 0.0
  load_balance_weight: 0.0
  process_norm_weight: 0.0

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_baseline_20M_c4_500M
log_dir: /content/drive/MyDrive/dawn/logs_baseline_20M_c4_500M

# 파라미터 계산:
#   Embedding: 30522 × 320 = 9,767,040
#   Pos Embedding: 512 × 320 = 163,840
#   Per Layer: ~1,271,040 (d_ff=1344)
#   8 Layers: ~10,168,320
#   Total: ~20.1M (weight tying)
#
# 비교:
#   DAWN v15: ~20.7M params
#   Baseline: ~20.1M params
#   동등 파라미터에서 아키텍처 비교
