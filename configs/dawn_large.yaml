# DAWN v8 Large Configuration
# For full-scale training

# Data (~100M train / ~20M val tokens)
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4_5to1_texts.pkl
    - train/openwebtext_5to1_texts.pkl
    - train/wikitext_5to1_texts.pkl
  val_files:
    - validation/c4_5to1_texts.pkl
    - validation/openwebtext_5to1_texts.pkl
    - validation/wikitext_5to1_texts.pkl

# Model Architecture (v8.0)
model:
  model_version: "8.0"
  d_model: 768
  n_layers: 12
  n_heads: 12

  # TransformNeurons (shared across layers)
  rank: 192               # d_model / 4
  n_input: 32
  n_process: 128
  n_output: 32
  process_k: 5

  # KnowledgeNeurons
  n_knowledge: 512
  knowledge_k: 32

  max_seq_len: 512
  dropout: 0.1

# Training
training:
  batch_size: 32
  num_epochs: 30
  lr: 0.0001
  weight_decay: 0.1
  warmup_epochs: 3

  # Regularization
  orthogonality_weight: 0.01
  process_norm_weight: 0.01
  load_balance_weight: 0.01

# Mixed precision
use_amp: true

# Paths
checkpoint_dir: checkpoints/dawn_large
log_dir: logs/dawn_large
