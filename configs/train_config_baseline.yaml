# Vanilla Transformer Baseline Training Configuration
# DAWN과 공정한 비교를 위한 Standard Transformer

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_file: train/wikitext_5to1_texts.pkl
  val_file: validation/wikitext_5to1_texts.pkl

# Model Architecture (Baseline: Standard Transformer)
model:
  model_version: "baseline"  # Vanilla Transformer
  d_model: 256               # Hidden dimension (same as DAWN)
  n_layers: 4                # Number of transformer layers (same as DAWN)
  n_heads: 4                 # Number of attention heads (same as DAWN)

  # Standard FFN parameters
  d_ff: 1024                 # Feed-forward dimension (same as DAWN)
  max_seq_len: 128           # Maximum sequence length (same as DAWN)
  dropout: 0.1               # Dropout rate (same as DAWN)

# Training (same as DAWN for fair comparison)
training:
  batch_size: 128            # Batch size
  num_epochs: 30             # Total epochs
  lr: 0.0003                 # Learning rate
  weight_decay: 0.1          # Weight decay for regularization
  warmup_epochs: 2           # Warmup epochs

  # No DAWN-specific losses for baseline
  diversity_weight: 0.0      # Not applicable
  load_balance_weight: 0.0   # Not applicable

# Mixed precision (faster training on GPU)
use_amp: true

# Paths
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_baseline
log_dir: /content/drive/MyDrive/dawn/checkpoints_baseline
