# DAWN v8.0 Training Config
# SharedNeurons + NeuronMemory with Q/K/V/O/M separated neurons

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/wikitext_5to1_texts.pkl
  val_files:
    - validation/wikitext_5to1_texts.pkl

# Model Architecture (v8.0)
model:
  model_version: "8.0"
  d_model: 256            # Hidden dimension
  n_layers: 4             # Number of transformer layers
  n_heads: 4              # Number of attention heads

  # TransformNeurons parameters (Q/K/V/O/M 분리)
  rank: 64                # Projection rank (4 heads x 16 d_head)
  n_input: 8              # Number of input neurons per type (Q/K/V/M each)
  n_process: 32           # Number of Householder process neurons per type (Q/K/V/O/M each)
  n_output: 8             # Number of output neurons (O only)
  process_k: 3            # Number of process neurons to select (top-k)

  # KnowledgeNeurons parameters
  n_knowledge: 64         # Number of knowledge neurons
  knowledge_k: 8          # Number of knowledge neurons to retrieve (top-k)

  # Architecture
  max_seq_len: 128        # Maximum sequence length
  dropout: 0.1            # Dropout rate

# Training
training:
  batch_size: 128         # Batch size
  num_epochs: 30          # Total epochs
  lr: 0.0003              # Learning rate
  weight_decay: 0.1       # Weight decay for regularization
  warmup_epochs: 2        # Warmup epochs

  # Regularization
  orthogonality_weight: 0.01   # Input/Output neuron orthogonality loss
  process_norm_weight: 0.01    # Process neuron norm regularization (||v|| = 1)
  load_balance_weight: 0.01    # Load balance loss for routing

# Mixed precision
use_amp: true

# Paths
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v8
log_dir: /content/drive/MyDrive/dawn/logs_v8

# Architecture Summary:
#   SharedNeurons (shared across all layers):
#     InputNeurons (4 separate pools):
#       - input_neurons_q: [8, 256, 64] for Q
#       - input_neurons_k: [8, 256, 64] for K
#       - input_neurons_v: [8, 256, 64] for V
#       - input_neurons_m: [8, 256, 64] for Memory Query
#     ProcessNeurons (5 separate pools, Householder vectors):
#       - process_neurons_q: [32, 64] for Q
#       - process_neurons_k: [32, 64] for K
#       - process_neurons_v: [32, 64] for V
#       - process_neurons_o: [32, 64] for O
#       - process_neurons_m: [32, 64] for Memory Query
#     OutputNeurons:
#       - output_neurons_o: [8, 64, 256] for O
#     KnowledgeNeurons:
#       - knowledge_K: [64, 64]
#       - knowledge_V: [64, 256]
#
#   Total SharedNeurons params:
#     input: 4 × 8 × 256 × 64 = 524,288
#     process: 5 × 32 × 64 = 10,240
#     output: 8 × 64 × 256 = 131,072
#     knowledge: 64×64 + 64×256 = 20,480
