# Vanilla Transformer Baseline - 40M Scale, 5B tokens
# Matched to DAWN v17.1 parameter count (~39M)

data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
    - train/c4/c4_raw_001.pt
    - train/c4/c4_raw_002.pt
    - train/c4/c4_raw_003.pt
    - train/c4/c4_raw_004.pt
    - train/c4/c4_raw_005.pt
    - train/c4/c4_raw_006.pt
    - train/c4/c4_raw_007.pt
    - train/c4/c4_raw_008.pt
    - train/c4/c4_raw_009.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 5000000000
  max_val_tokens: 2000000

model:
  model_version: "baseline"
  d_model: 432
  n_layers: 12
  n_heads: 6
  d_ff: 1728
  max_seq_len: 512
  dropout: 0.1

training:
  batch_size: 128
  num_epochs: 1
  lr: 0.00065
  weight_decay: 0.1
  warmup_ratio: 0.06

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_baseline_40M_c4_5B
log_dir: /content/drive/MyDrive/dawn/logs_baseline_40M_c4_5B

# Parameter calculation:
# Embedding: 30522 × 432 = 13.2M
# Per layer:
#   - Attention: 432 × 432 × 4 = 0.75M
#   - FFN: 432 × 1728 × 2 = 1.49M
#   - Total: 2.24M
# 12 layers: 2.24M × 12 = 26.9M
# LM head: tied with embedding
# Total: ~40.1M
#
# Configuration:
# - d_model=432, matching DAWN's 384 effective capacity
# - d_ff = 4 × d_model (standard ratio)
# - head_dim = 72 (432 / 6)
# - Same depth, training setup as DAWN for fair comparison
# - lr=0.00065 matched to DAWN config
