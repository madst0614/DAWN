# DAWN v17.1 Training Config - 40M Scale, 5B tokens (Seed 2, Token Routing, V Expand 6)
# Q/K shared pool + Knowledge Feature-Restore pattern
# Token-level routing for both attention and knowledge (no SSM importance)
# QK=64(k=12), V=256(k=12), Knowledge=128(k=12), knowledge_rank=64, d_space=256

seed: 2

data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
    - train/c4/c4_raw_001.pt
    - train/c4/c4_raw_002.pt
    - train/c4/c4_raw_003.pt
    - train/c4/c4_raw_004.pt
    - train/c4/c4_raw_005.pt
    - train/c4/c4_raw_006.pt
    - train/c4/c4_raw_007.pt
    - train/c4/c4_raw_008.pt
    - train/c4/c4_raw_009.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 5000000000
  max_val_tokens: 2000000

model:
  model_version: "17.1"
  d_model: 384
  n_layers: 12
  n_heads: 6
  rank: 64
  state_dim: 64

  # Attention - Q/K shared pool (QK=64 k=12, V=256 k=12)
  n_feature_qk: 64
  n_feature_v: 256
  top_k_feature_qk: 12
  top_k_feature_v: 12

  n_restore_qk: 64
  n_restore_v: 256
  top_k_restore_qk: 12
  top_k_restore_v: 12

  # Knowledge - Feature-Restore pattern (rank=64)
  n_feature_know: 128
  n_restore_know: 128
  knowledge_rank: 64
  top_k_feature_know: 12
  top_k_restore_know: 12

  max_seq_len: 512
  dropout: 0.1
  gradient_checkpointing: true
  d_space: 256
  router_dropout: 0.1
  attention_token_routing: true
  knowledge_token_routing: true
  use_ssm_context: false  # SSM not used with token routing

training:
  batch_size: 128
  num_epochs: 1
  lr: 0.00065
  weight_decay: 0.1
  warmup_ratio: 0.06
  orthogonality_weight: 0.01
  diversity_weight: 0.1
  load_balance_weight: 0.00001
  entropy_weight: 0.0
  process_norm_weight: 0.0

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v17.1_40M_c4_5B_seed2_token_routing_v_expand6
log_dir: /content/drive/MyDrive/dawn/logs_v17.1_40M_c4_5B_seed2_token_routing_v_expand6

# v17.1 Token Routing V-Expand6 Architecture
#
#   Changes from V-Expand4:
#   - QK: 64, top-k increased: 6→12
#   - V expanded: 80→256, top-k increased: 6→12
#   - Knowledge: 92→128, top-k increased: 6→12
#
#   Attention Circuit (per-token routing):
#   - Feature_QK=64 (k=12): per-token Q/K compression
#   - Feature_V=256 (k=12): per-token V compression
#   - Restore_QK=64 (k=12): per-token Q/K restoration
#   - Restore_V=256 (k=12): per-token V restoration
#
#   Knowledge Circuit (per-token routing):
#   - Feature_Know=128 (k=12): per-token compression
#   - Restore_Know=128 (k=12): per-token restoration
