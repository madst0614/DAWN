# DAWN v15 Training Config
# 2-Stage Hierarchical Knowledge Retrieval
# F(eature), R(elational), V(alue), K(nowledge) neurons

# vs v14: NeuronMemory uses 2-stage retrieval (router coarse → h fine)

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/wikitext_5to1_texts.pkl
  val_files:
    - val/wikitext_5to1_texts.pkl

# Model Architecture
model:
  model_version: "15.0"
  d_model: 320
  n_layers: 4
  n_heads: 4
  rank: 64
  state_dim: 64

  # FRVK Neurons
  n_feature: 12
  n_relational: 48
  n_value: 12
  n_knowledge: 80
  coarse_k: 20                   # Stage 1: router-based candidate selection
  fine_k: 10                     # Stage 2: h-based fine matching
  knowledge_rank: 128            # v15: 64 → 128 (larger matching space)
  max_seq_len: 128
  dropout: 0.1

  # Top-k sparse mixing
  top_k_feature: 3
  top_k_relational: 12
  top_k_value: 3

  # Router settings
  d_space: 64
  router_dropout: 0.1
  token_routing: false
  use_ssm_context: true
  gradient_checkpointing: false

# Training
training:
  batch_size: 128
  num_epochs: 30
  lr: 0.0003
  weight_decay: 0.1
  warmup_epochs: 2
  # Regularization
  orthogonality_weight: 0.01
  diversity_weight: 0.1
  load_balance_weight: 0.0
  entropy_weight: 0.0
  process_norm_weight: 0.0

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v15
log_dir: /content/drive/MyDrive/dawn/logs_v15

# v15 Changes from v14:
#   - NeuronMemory: 2-stage hierarchical retrieval
#     * Stage 1: x → router → top-coarse_k candidates
#     * Stage 2: h → proj_q → similarity → top-fine_k
#   - Knowledge neurons integrated into unified router space
#   - knowledge_rank: 64 → 128 (larger matching space)
