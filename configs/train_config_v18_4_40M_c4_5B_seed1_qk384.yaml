# DAWN v18.4 Training Config - 40M Scale, 5B tokens (Seed 1, QK-384)
# v18.4: Relative Confidence Scaling
# - gate = ReLU(scores - tau), can be 0
# - confidence = gate / gate_sum (relative, sums to 1)
# - tau decoupled from lb_loss, healthy gradient flow

seed: 1

data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
    - train/c4/c4_raw_001.pt
    - train/c4/c4_raw_002.pt
    - train/c4/c4_raw_003.pt
    - train/c4/c4_raw_004.pt
    - train/c4/c4_raw_005.pt
    - train/c4/c4_raw_006.pt
    - train/c4/c4_raw_007.pt
    - train/c4/c4_raw_008.pt
    - train/c4/c4_raw_009.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 5000000000
  max_val_tokens: 2000000

model:
  model_version: "18.4"
  d_model: 384
  n_layers: 12
  n_heads: 6
  rank: 16
  state_dim: 64

  # Attention - 4x neurons from v17.1
  n_feature_qk: 384
  n_feature_v: 1000
  n_restore_qk: 384
  n_restore_v: 1000

  # Knowledge - 4x neurons from v17.1
  n_feature_know: 580
  n_restore_know: 580
  knowledge_rank: 16

  max_seq_len: 512
  dropout: 0.1
  gradient_checkpointing: true
  d_space: 256
  router_dropout: 0.1
  attention_token_routing: true
  knowledge_token_routing: true
  use_ssm_context: false

  # v18.4 specific - relative confidence scaling
  max_paths: 2
  path_max_k: 16
  learnable_tau: true

training:
  batch_size: 128
  gradient_accumulation_steps: 1
  num_epochs: 1
  lr: 0.00065
  weight_decay: 0.1
  warmup_ratio: 0.06
  orthogonality_weight: 0.01
  diversity_weight: 0.1
  load_balance_weight: 0.00002
  tau_reg_weight: 0.00002

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v18.4_40M_c4_5B_seed1_qk384
log_dir: /content/drive/MyDrive/dawn/logs_v18.4_40M_c4_5B_seed1_qk384

# v18.4 Relative Confidence Scaling Architecture
#
#   Key difference from v18.3:
#   - confidence = gate / gate_sum (relative proportion)
#   - vs v18.3: gate / (gate + 1) (absolute sigmoid-like)
#
#   Benefits:
#   - tau stays in reasonable range (-3 ~ +3 instead of -170)
#   - conf distribution is differentiated (not all 0.99)
#   - lb_loss doesn't push tau to extremes
#   - True zero possible (hard sparsity)
#   - Healthy gradients (no vanishing)
#
#   Attention Circuit (2-path routing):
#   - Feature_QK=384 (2 paths x 16): per-token Q/K compression
#   - Feature_V=1000 (2 paths x 16): per-token V compression
#   - Restore_QK=384 (2 paths x 16): per-token Q/K restoration
#   - Restore_V=1000 (2 paths x 16): per-token V restoration
#
#   Knowledge Circuit (2-path routing):
#   - Feature_Know=580 (2 paths x 16): per-token compression
#   - Restore_Know=580 (2 paths x 16): per-token restoration
