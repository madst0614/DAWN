# Vanilla Transformer Baseline - 18M Scale
# Fair comparison with DAWN v10.0 18M

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 100000000  # 100M tokens
  max_val_tokens: 2000000      # 2M tokens

# Model Architecture
model:
  model_version: "baseline"
  d_model: 320
  n_layers: 6
  n_heads: 4
  d_ff: 1280              # 4 * d_model
  max_seq_len: 512
  dropout: 0.1

# Training (same as DAWN)
training:
  batch_size: 128
  num_epochs: 4
  lr: 0.0006
  weight_decay: 0.1
  warmup_ratio: 0.06

  # Regularization (ignored for baseline)
  orthogonality_weight: 0.0
  diversity_weight: 0.0
  load_balance_weight: 0.0
  process_norm_weight: 0.0

use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_baseline_18M
log_dir: /content/drive/MyDrive/dawn/logs_baseline_18M

# Summary:
#   Train: 100M × 4 epochs = 400M tokens (same as DAWN v12.7/v12.8)
#   Seq length: 512, batch_size: 128, lr: 0.0006
#
# 파라미터 계산:
#   Embedding: 30000 × 320 = 9,600,000
#   Pos Embedding: 512 × 320 = 163,840
#   Per Layer:
#     - Attention (Q,K,V,O): 4 × 320² = 409,600
#     - FFN: 320×1280 + 1280×320 = 819,200
#     - LayerNorm: 2 × 320 × 2 = 1,280
#     - Total: ~1,230,080
#   6 Layers: ~7,380,480
#   Final LayerNorm: 640
#
#   Total: ~17M (weight tying 적용)
