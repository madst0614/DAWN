# DAWN v18.1 Training Config - 20M Scale, 500M tokens
# Soft Mask + Token-level Learnable Tau Routing
# rank=16, max_paths=4, soft mask + learnable tau per token
seed: 42
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/c4/c4_raw_000.pt
  val_files:
    - val/c4/c4_val_50M.pt
  max_train_tokens: 500000000
  max_val_tokens: 2000000
model:
  model_version: "18.1"
  d_model: 256
  n_layers: 8
  n_heads: 4
  rank: 16
  state_dim: 64
  # v18: Multi-path parameters
  max_paths: 4
  fixed_tau: 0.0
  path_min_k: 8
  path_max_k: 16
  # v18.1: Soft mask + learnable tau
  use_soft_mask: true
  learnable_tau: true
  soft_mask_temp: 1.0
  soft_mask_penalty: 10.0
  # Attention - Q/K shared pool
  n_feature_qk: 64
  n_feature_v: 264
  n_restore_qk: 64
  n_restore_v: 264
  # Knowledge - Feature-Restore pattern
  n_feature_know: 160
  n_restore_know: 160
  knowledge_rank: 16
  max_seq_len: 512
  dropout: 0.1
  gradient_checkpointing: false
  d_space: 256
  router_dropout: 0.1
  attention_token_routing: true
  knowledge_token_routing: true
  use_ssm_context: false
training:
  batch_size: 32
  num_epochs: 1
  lr: 0.0003
  weight_decay: 0.1
  warmup_ratio: 0.06
  orthogonality_weight: 0.01
  diversity_weight: 0.1
  load_balance_weight: 0.00002
  entropy_weight: 0.0
  process_norm_weight: 0.0
use_amp: true
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v18.1_20M_500M
log_dir: /content/drive/MyDrive/dawn/logs_v18.1_20M_500M
# v18.1 Architecture
#
# Key differences from v18.0:
#   - use_soft_mask: sigmoid mask instead of hard threshold
#   - learnable_tau: token-level tau via nn.Linear(d_model, 6)
#   - soft_mask_temp: sigmoid temperature (sharpness)
#   - soft_mask_penalty: penalty for low-confidence neurons
#
# Neuron selection flow:
#   x -> tau_proj(x) -> tau [B, S, 1] per pool (token-level!)
#   soft_mask = sigmoid((scores - tau) / temp)
#   penalty = (1 - soft_mask) * (-100)
#   masked_scores = scores + penalty + min_k relief
#   weights = softmax(masked_scores)
#   chunk by score rank -> 1~max_paths paths -> sum Q, K, V
#
# Why soft mask + softmax prevents tau divergence:
#   - tau -> ∞: all penalties -> -100, softmax -> uniform (not dead)
#   - tau -> -∞: all pass, normal softmax competition
#   - Gradient always flows through sigmoid and softmax
