# DAWN v9.1 Training Config
# Hard selection + Gated reflection + Separate reflect pools

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_files:
    - train/wikitext_5to1_texts.pkl
  val_files:
    - validation/wikitext_5to1_texts.pkl

# Model Architecture (v9.1)
model:
  model_version: "9.1"    # DAWN v9.1
  d_model: 256            # Hidden dimension
  n_layers: 4             # Number of transformer layers
  n_heads: 4              # Number of attention heads

  # Compress/Expand Neurons parameters
  rank: 64                # Projection rank (4 heads x 16 d_head)
  n_compress: 32          # Number of compress neurons (soft weighted sum)
  n_expand: 32            # Number of expand neurons (soft weighted sum)

  # ReflectionNeurons parameters (separate pools)
  n_reflect_d: 64         # Size of d_model space reflection pool
  n_reflect_r: 64         # Size of rank space reflection pool
  reflect_k: 3            # Number of reflections to select (top-k, gated)

  # KnowledgeNeurons parameters
  n_knowledge: 64         # Number of knowledge neurons
  knowledge_k: 8          # Number of knowledge neurons to retrieve (top-k)

  # Architecture
  max_seq_len: 128        # Maximum sequence length
  dropout: 0.1            # Dropout rate

# Training
training:
  batch_size: 128         # Batch size
  num_epochs: 30          # Total epochs
  lr: 0.0003              # Learning rate
  weight_decay: 0.1       # Weight decay for regularization
  warmup_ratio: 0.1       # Warmup ratio (10% of total steps)

  # Regularization
  orthogonality_weight: 0.01   # Compress/Expand orthogonality loss
  process_norm_weight: 0.01    # Reflection norm regularization (||v|| = 1)
  load_balance_weight: 0.01    # Load balance loss for routing

# Mixed precision
use_amp: true

# Paths
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints_v91
log_dir: /content/drive/MyDrive/dawn/logs_v91

# v9.1 특징:
#   1. Compress/Expand: soft weighted sum (배치 병렬화 가능)
#      - weights = softmax(router(x))
#      - all_proj = einsum(x, neurons)
#      - output = (all_proj * weights).sum()
#
#   2. Reflection: gated (sigmoid gate)
#      - gate = sigmoid(score)
#      - x = gate * reflected + (1 - gate) * x
#
#   3. ReflectionNeurons: separate pools
#      - reflect_d: [n_reflect_d, d_model] for d_model space
#      - reflect_r: [n_reflect_r, rank] for rank space
#
# Architecture v9.1:
#   SharedNeurons (shared across all layers):
#     CompressNeurons: [n_compress, d_model, rank]   # Soft weighted sum
#     ExpandNeurons: [n_expand, rank, d_model]       # Soft weighted sum
#
#     ReflectionNeurons (gated, separate pools):
#       - reflect_d: [n_reflect_d, d_model]   # d_model space reflections
#       - reflect_r: [n_reflect_r, rank]      # rank space reflections
#
#     KnowledgeNeurons:
#       - knowledge_K: [n_knowledge, rank]
#       - knowledge_V: [n_knowledge, d_model]
#
#   Parameter breakdown (n_compress=32, n_expand=32, n_reflect_d=64, n_reflect_r=64):
#     - compress_neurons: 32 x 256 x 64 = 524K
#     - expand_neurons: 32 x 64 x 256 = 524K
#     - reflect_d: 64 x 256 = 16K
#     - reflect_r: 64 x 64 = 4K
#     - knowledge_K: 64 x 64 = 4K
#     - knowledge_V: 64 x 256 = 16K
#     - Total SharedNeurons: ~1.1M
