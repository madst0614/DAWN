# DAWN Training Configuration
# Dynamic Neuron Transformer Architecture

# Data
data:
  base_dir: /content/drive/MyDrive/data
  train_file: train/wikitext_5to1_texts.pkl
  val_file: validation/wikitext_5to1_texts.pkl

# Model Architecture (v7.0: Fixed Orthogonal Basis)
model:
  model_version: "7.0"    # DAWN model version
  d_model: 256            # Hidden dimension
  n_layers: 4             # Number of transformer layers
  n_heads: 4              # Number of attention heads

  # Neurons (v7.0: Recipe-based, derived from fixed basis)
  n_neurons: 64           # Size of neuron pool per layer (256 → 64, 더 집중적)
  neuron_k: 8             # Top-k neurons to select (16 → 8, 더 선택적)

  # Basis FFN (v7.0: Fixed Orthogonal Basis, shared across layers)
  n_basis: 32             # Number of orthogonal basis (8 → 32, 더 풍부한 좌표계)
  basis_rank: 64          # Basis rank (동일, 강력한 표현력 유지)
  # v7.0 변경: Basis 고정 (학습 X), 전체 Layer 공유, neuron_emb = recipe @ basis_emb

  # Architecture
  d_ff: 1024              # Feed-forward dimension (512 → 1024, basis 증가에 맞춤)
  max_seq_len: 128        # Maximum sequence length
  dropout: 0.1            # Dropout rate

# Training
training:
  batch_size: 128         # Batch size
  num_epochs: 30          # Total epochs
  lr: 0.0003              # Learning rate
  weight_decay: 0.1       # Weight decay for regularization
  warmup_epochs: 2        # Warmup epochs

  # v7.0 Regularization
  # orthogonality_weight: 0.0 (not needed - basis is fixed and perfectly orthogonal)
  diversity_weight: 0.05  # Recipe diversity loss (권장: 0.01~0.1, 0으로 비활성화 가능)
  load_balance_weight: 0.01  # Load balance loss (권장: 0.01~0.05, 0으로 비활성화 가능)

# Mixed precision (faster training on GPU)
use_amp: true

# Paths
checkpoint_dir: /content/drive/MyDrive/dawn/checkpoints
log_dir: /content/drive/MyDrive/dawn/checkpoints
