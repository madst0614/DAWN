#!/usr/bin/env python3
"""
DAWN v8.0 Comprehensive Semantic Analysis Script

Analyzes semantic patterns in neuron routing, knowledge neurons,
and Q/K/V/O component differentiation.

Usage:
    python scripts/analyze_semantic_v8.py \
        --checkpoint /path/to/checkpoint.pt \
        --data /path/to/validation_data.pkl \
        --output /path/to/output_dir/
"""

import sys
import os
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
import torch.nn.functional as F
import numpy as np
import pickle
import json
import argparse
from datetime import datetime
from tqdm import tqdm
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# Visualization
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
from wordcloud import WordCloud

# Dimensionality reduction - Try GPU-accelerated cuML first
HAS_CUML = False
try:
    from cuml.manifold import TSNE as cuTSNE
    from cuml.manifold import UMAP as cuUMAP
    from cuml.cluster import KMeans as cuKMeans
    import cupy as cp
    HAS_CUML = True
    print("âœ“ cuML (RAPIDS) available - GPU-accelerated t-SNE/UMAP/KMeans enabled")
except ImportError:
    from sklearn.manifold import TSNE
    print("Note: cuML not installed. Using CPU-based sklearn (slower).")

try:
    import umap
    HAS_UMAP = True
except ImportError:
    HAS_UMAP = False
    if not HAS_CUML:
        print("Warning: umap-learn not installed. Using t-SNE only.")

# NLP tools
try:
    import nltk
    from nltk import pos_tag, word_tokenize
    from nltk.corpus import wordnet
    nltk.download('punkt', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('punkt_tab', quiet=True)
    nltk.download('averaged_perceptron_tagger_eng', quiet=True)
    HAS_NLTK = True
except ImportError:
    HAS_NLTK = False
    print("Warning: nltk not installed. POS tagging will be limited.")

# Sankey diagram
try:
    import plotly.graph_objects as go
    HAS_PLOTLY = True
except ImportError:
    HAS_PLOTLY = False
    print("Warning: plotly not installed. Sankey diagrams will be skipped.")

from transformers import AutoTokenizer
from models import create_model_by_version


# ============================================================
# Utility Functions
# ============================================================

def load_model_and_tokenizer(checkpoint_path, device='cuda'):
    """Load DAWN v8.0 model from checkpoint"""
    print(f"Loading checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)

    config = checkpoint.get('config', {})
    model_version = config.get('model_version', '8.0')

    print(f"Model version: {model_version}")
    print(f"Config: {config}")

    # Create model
    model = create_model_by_version(model_version, config)

    # Load weights
    state_dict = checkpoint['model_state_dict']
    # Remove '_orig_mod.' prefix if present (from torch.compile)
    cleaned_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith('_orig_mod.'):
            cleaned_state_dict[k[10:]] = v
        else:
            cleaned_state_dict[k] = v

    model.load_state_dict(cleaned_state_dict, strict=False)
    model = model.to(device)
    model.eval()

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

    print(f"Model loaded successfully!")
    return model, tokenizer, config


def load_data(data_path, max_samples=1000):
    """Load validation data"""
    print(f"Loading data: {data_path}")
    with open(data_path, 'rb') as f:
        texts = pickle.load(f)

    if len(texts) > max_samples:
        texts = texts[:max_samples]

    print(f"Loaded {len(texts)} samples")
    return texts


def get_pos_color_map():
    """Get color map for POS tags"""
    return {
        'NN': '#e41a1c',    # Noun - red
        'NNS': '#e41a1c',
        'NNP': '#984ea3',   # Proper noun - purple
        'NNPS': '#984ea3',
        'VB': '#377eb8',    # Verb - blue
        'VBD': '#377eb8',
        'VBG': '#377eb8',
        'VBN': '#377eb8',
        'VBP': '#377eb8',
        'VBZ': '#377eb8',
        'JJ': '#4daf4a',    # Adjective - green
        'JJR': '#4daf4a',
        'JJS': '#4daf4a',
        'RB': '#ff7f00',    # Adverb - orange
        'RBR': '#ff7f00',
        'RBS': '#ff7f00',
        'DT': '#a65628',    # Determiner - brown
        'IN': '#f781bf',    # Preposition - pink
        'CC': '#999999',    # Conjunction - gray
        'PRP': '#ffff33',   # Pronoun - yellow
        'PRP$': '#ffff33',
        'OTHER': '#666666'  # Other - dark gray
    }


def get_pos_category(tag):
    """Get simplified POS category"""
    if tag.startswith('NN'):
        return 'Noun'
    elif tag.startswith('VB'):
        return 'Verb'
    elif tag.startswith('JJ'):
        return 'Adjective'
    elif tag.startswith('RB'):
        return 'Adverb'
    elif tag in ['DT', 'PDT']:
        return 'Determiner'
    elif tag == 'IN':
        return 'Preposition'
    elif tag in ['CC']:
        return 'Conjunction'
    elif tag.startswith('PRP'):
        return 'Pronoun'
    elif tag.startswith('NNP'):
        return 'Proper Noun'
    else:
        return 'Other'


# ============================================================
# Analysis Functions
# ============================================================

class SemanticAnalyzer:
    def __init__(self, model, tokenizer, config, device='cuda'):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.device = torch.device(device) if isinstance(device, str) else device

        # Extract model info
        self.n_layers = config.get('n_layers', 12)
        self.n_process = config.get('n_process', 32)
        self.n_knowledge = config.get('n_knowledge', 128)

    def collect_routing_info(self, texts, max_tokens=50000, batch_size=32):
        """Collect routing information for all tokens (GPU optimized with batching)"""
        print("\nCollecting routing information (GPU optimized)...")

        all_tokens = []
        all_routing = []
        all_knowledge_routing = []
        all_qkvo_routing = []
        all_positions = []
        all_sentence_ids = []

        token_count = 0
        successful_texts = 0

        # Process in batches for GPU efficiency
        for batch_start in tqdm(range(0, len(texts), batch_size), desc="Processing batches"):
            if token_count >= max_tokens:
                break

            batch_texts = texts[batch_start:batch_start + batch_size]

            # Batch tokenize
            encodings = self.tokenizer(
                batch_texts,
                return_tensors='pt',
                max_length=256,
                truncation=True,
                padding=True
            )
            input_ids = encodings['input_ids'].to(self.device)
            attention_mask = encodings['attention_mask'].to(self.device)

            routing_infos = None

            with torch.no_grad(), torch.amp.autocast('cuda', enabled=self.device.type == 'cuda'):
                # Try multiple methods to get routing info
                try:
                    # Method 1: return_routing_info=True
                    outputs = self.model(input_ids, return_routing_info=True)
                    if isinstance(outputs, tuple):
                        if len(outputs) >= 3:
                            routing_infos = outputs[2]
                        elif len(outputs) == 2 and isinstance(outputs[1], list):
                            routing_infos = outputs[1]
                except:
                    pass

                if routing_infos is None:
                    try:
                        # Method 2: forward_with_analysis
                        outputs = self.model.forward_with_analysis(input_ids)
                        if isinstance(outputs, dict):
                            routing_infos = outputs.get('routing_infos', outputs.get('routing', []))
                    except:
                        pass

                if routing_infos is None:
                    try:
                        # Method 3: Just forward pass, extract from model internals
                        _ = self.model(input_ids)
                        # Try to get routing from layers
                        routing_infos = []
                        for layer in self.model.layers:
                            if hasattr(layer, 'last_routing_info'):
                                routing_infos.append(layer.last_routing_info)
                            elif hasattr(layer, 'attention') and hasattr(layer.attention, 'last_routing'):
                                routing_infos.append(layer.attention.last_routing)
                    except:
                        pass

            if not routing_infos:
                continue

            # Process all samples in batch
            batch_size_actual = input_ids.shape[0]
            for batch_idx in range(batch_size_actual):
                sent_id = batch_start + batch_idx
                successful_texts += 1

                # Get tokens for this sample
                sample_ids = input_ids[batch_idx].cpu().numpy()
                sample_mask = attention_mask[batch_idx].cpu().numpy()
                tokens = self.tokenizer.convert_ids_to_tokens(sample_ids)
                seq_len = int(sample_mask.sum())  # Actual sequence length

                for pos, token in enumerate(tokens):
                    if token in ['[CLS]', '[SEP]', '[PAD]'] or sample_mask[pos] == 0:
                        continue

                    all_tokens.append(token)
                    all_positions.append(pos / seq_len)  # Normalized position
                    all_sentence_ids.append(sent_id)

                    # Collect routing per layer
                    layer_routing = []
                    layer_knowledge = []
                    layer_qkvo = []

                    for layer_info in routing_infos:
                        try:
                            if isinstance(layer_info, dict):
                                # Process neuron routing - try multiple keys
                                for key in ['neuron_indices', 'process_indices', 'indices', 'selected_neurons']:
                                    if key in layer_info:
                                        tensor = layer_info[key]
                                        if tensor.dim() >= 2 and batch_idx < tensor.shape[0] and pos < tensor.shape[1]:
                                            indices = tensor[batch_idx, pos].cpu().numpy()
                                            layer_routing.append(indices)
                                            break

                                # Process knowledge routing
                                for key in ['knowledge_indices', 'memory_indices', 'k_indices']:
                                    if key in layer_info:
                                        tensor = layer_info[key]
                                        if tensor.dim() >= 2 and batch_idx < tensor.shape[0] and pos < tensor.shape[1]:
                                            k_indices = tensor[batch_idx, pos].cpu().numpy()
                                            layer_knowledge.append(k_indices)
                                            break

                                # Process Q/K/V/O routing
                                if 'qkvo_routing' in layer_info:
                                    qkvo = layer_info['qkvo_routing'][batch_idx, pos].cpu().numpy()
                                    layer_qkvo.append(qkvo)

                            elif isinstance(layer_info, torch.Tensor):
                                # Direct tensor - assume it's neuron indices
                                if layer_info.dim() >= 2 and batch_idx < layer_info.shape[0] and pos < layer_info.shape[1]:
                                    indices = layer_info[batch_idx, pos].cpu().numpy()
                                    layer_routing.append(indices)
                        except Exception as e:
                            continue

                    all_routing.append(layer_routing)
                    all_knowledge_routing.append(layer_knowledge)
                    all_qkvo_routing.append(layer_qkvo)

                    token_count += 1
                    if token_count >= max_tokens:
                        break

                if token_count >= max_tokens:
                    break

        print(f"Collected {len(all_tokens)} tokens from {successful_texts} texts")

        # If no routing data, create synthetic based on embeddings
        if len(all_routing) > 0 and not any(all_routing):
            print("No routing data found. Creating embedding-based clusters...")
            all_routing = self._create_embedding_routing(texts, all_tokens, max_tokens)

        return {
            'tokens': all_tokens,
            'routing': all_routing,
            'knowledge_routing': all_knowledge_routing,
            'qkvo_routing': all_qkvo_routing,
            'positions': all_positions,
            'sentence_ids': all_sentence_ids
        }

    def _create_embedding_routing(self, texts, tokens, max_tokens):
        """Create synthetic routing based on token embeddings (GPU optimized)"""
        print("Creating embedding-based pseudo-routing (GPU accelerated)...")

        # Get all unique tokens and their IDs
        unique_tokens = list(set(tokens[:max_tokens]))
        token_to_idx = {t: i for i, t in enumerate(unique_tokens)}

        # Batch get embeddings on GPU
        token_ids = [self.tokenizer.convert_tokens_to_ids(t) for t in unique_tokens]
        token_ids_tensor = torch.tensor(token_ids, device=self.device)

        with torch.no_grad():
            if hasattr(self.model, 'token_emb'):
                unique_embeddings = self.model.token_emb(token_ids_tensor).cpu().numpy()
            elif hasattr(self.model, 'embedding'):
                unique_embeddings = self.model.embedding(token_ids_tensor).cpu().numpy()
            else:
                unique_embeddings = np.random.randn(len(unique_tokens), 256)

        # Map back to original token order
        embeddings = np.array([unique_embeddings[token_to_idx[t]] for t in tokens[:max_tokens]])

        # Cluster embeddings to create pseudo-routing (GPU-accelerated if cuML available)
        n_clusters = min(self.n_process, len(embeddings) // 10)
        if n_clusters < 2:
            n_clusters = 2

        if HAS_CUML:
            print("  Using cuML GPU-accelerated KMeans...")
            embeddings_gpu = cp.asarray(embeddings.astype(np.float32))
            kmeans = cuKMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(embeddings_gpu)
            cluster_labels = cp.asnumpy(cluster_labels)
        else:
            from sklearn.cluster import KMeans
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)

        # Convert to routing format (list of lists)
        routing = []
        for label in cluster_labels:
            layer_routing = [np.array([label])]
            routing.append(layer_routing)

        return routing

    def analyze_token_clusters(self, data, output_dir):
        """1. Token cluster visualization with t-SNE/UMAP"""
        print("\n=== 1. Token Cluster Visualization ===")

        tokens = data['tokens']
        routing = data['routing']

        if not tokens or not routing:
            print("No token/routing data available")
            return {'n_tokens': 0, 'error': 'No data'}

        # Create routing vectors (flatten all layers)
        vectors = []
        valid_tokens = []
        for i, r in enumerate(routing):
            if r and len(r) > 0:
                try:
                    flat = np.concatenate([np.array(layer).flatten() for layer in r if len(layer) > 0])
                    if len(flat) > 0:
                        vectors.append(flat)
                        valid_tokens.append(tokens[i])
                except:
                    pass

        if len(vectors) < 10:
            print(f"Insufficient routing data: only {len(vectors)} valid vectors")
            return {'n_tokens': len(vectors), 'error': 'Insufficient data'}

        # Pad vectors to consistent size
        max_len = max(len(v) for v in vectors)
        padded_vectors = np.zeros((len(vectors), max_len))
        for i, v in enumerate(vectors):
            padded_vectors[i, :len(v)] = v
        vectors = padded_vectors
        tokens = valid_tokens

        # t-SNE (GPU-accelerated if cuML available)
        print(f"Running t-SNE on {len(vectors)} vectors...")
        perplexity = max(5, min(30, len(vectors) // 4))

        if HAS_CUML:
            print("  Using cuML GPU-accelerated t-SNE...")
            vectors_gpu = cp.asarray(vectors.astype(np.float32))
            tsne = cuTSNE(n_components=2, random_state=42, perplexity=perplexity)
            coords_tsne = tsne.fit_transform(vectors_gpu)
            coords_tsne = cp.asnumpy(coords_tsne)
        else:
            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
            coords_tsne = tsne.fit_transform(vectors)

        # Get POS tags
        if HAS_NLTK:
            pos_tags = []
            for token in tokens:
                clean_token = token.replace('##', '')
                try:
                    tag = pos_tag([clean_token])[0][1]
                except:
                    tag = 'OTHER'
                pos_tags.append(get_pos_category(tag))
        else:
            pos_tags = ['Other'] * len(tokens)

        # Plot t-SNE
        fig, ax = plt.subplots(figsize=(14, 10))

        categories = list(set(pos_tags))
        colors = plt.cm.tab10(np.linspace(0, 1, len(categories)))
        color_map = {cat: colors[i] for i, cat in enumerate(categories)}

        for cat in categories:
            mask = [p == cat for p in pos_tags]
            ax.scatter(
                coords_tsne[mask, 0],
                coords_tsne[mask, 1],
                c=[color_map[cat]],
                label=cat,
                alpha=0.6,
                s=20
            )

        ax.legend(loc='upper right', fontsize=10)
        ax.set_title('Token Clusters by POS (t-SNE)', fontsize=14)
        ax.set_xlabel('t-SNE 1')
        ax.set_ylabel('t-SNE 2')

        plt.tight_layout()
        plt.savefig(output_dir / 'token_clusters_tsne.png', dpi=150)
        plt.close()

        # UMAP (GPU-accelerated if cuML available)
        if HAS_CUML or HAS_UMAP:
            print("Running UMAP...")
            if HAS_CUML:
                print("  Using cuML GPU-accelerated UMAP...")
                vectors_gpu = cp.asarray(vectors.astype(np.float32))
                reducer = cuUMAP(n_components=2, random_state=42)
                coords_umap = reducer.fit_transform(vectors_gpu)
                coords_umap = cp.asnumpy(coords_umap)
            else:
                reducer = umap.UMAP(n_components=2, random_state=42)
                coords_umap = reducer.fit_transform(vectors)

            fig, ax = plt.subplots(figsize=(14, 10))
            for cat in categories:
                mask = [p == cat for p in pos_tags]
                ax.scatter(
                    coords_umap[mask, 0],
                    coords_umap[mask, 1],
                    c=[color_map[cat]],
                    label=cat,
                    alpha=0.6,
                    s=20
                )

            ax.legend(loc='upper right', fontsize=10)
            ax.set_title('Token Clusters by POS (UMAP)', fontsize=14)
            ax.set_xlabel('UMAP 1')
            ax.set_ylabel('UMAP 2')

            plt.tight_layout()
            plt.savefig(output_dir / 'token_clusters_umap.png', dpi=150)
            plt.close()

        # Save data
        result = {
            'n_tokens': len(tokens),
            'pos_distribution': dict(Counter(pos_tags)),
            'tsne_coords': coords_tsne.tolist()[:100]  # Sample
        }

        with open(output_dir / 'token_clusters.json', 'w') as f:
            json.dump(result, f, indent=2)

        print(f"Saved token cluster visualizations")
        return result

    def analyze_neuron_wordclouds(self, data, output_dir):
        """2. WordCloud for each neuron"""
        print("\n=== 2. Neuron WordClouds ===")

        tokens = data['tokens']
        routing = data['routing']

        # Collect tokens per neuron (using first layer)
        neuron_tokens = defaultdict(list)

        for token, route in zip(tokens, routing):
            if route and len(route) > 0:
                for neuron_idx in route[0]:  # First layer
                    clean_token = token.replace('##', '')
                    if len(clean_token) > 1:  # Skip single chars
                        neuron_tokens[int(neuron_idx)].append(clean_token)

        # Create wordclouds directory
        wc_dir = output_dir / 'wordclouds'
        wc_dir.mkdir(exist_ok=True)

        # Generate wordclouds for top neurons
        neuron_stats = {}

        n_neurons = min(32, self.n_process)
        fig, axes = plt.subplots(4, 8, figsize=(24, 12))
        axes = axes.flatten()

        for i in range(n_neurons):
            ax = axes[i]

            if i in neuron_tokens and len(neuron_tokens[i]) > 5:
                token_freq = Counter(neuron_tokens[i])

                try:
                    wc = WordCloud(
                        width=300, height=200,
                        background_color='white',
                        max_words=50,
                        colormap='viridis'
                    ).generate_from_frequencies(token_freq)

                    ax.imshow(wc, interpolation='bilinear')
                    ax.set_title(f'Neuron {i}', fontsize=10)
                except:
                    ax.text(0.5, 0.5, f'Neuron {i}\n(insufficient data)',
                           ha='center', va='center', fontsize=8)

                neuron_stats[i] = {
                    'top_tokens': token_freq.most_common(20),
                    'total_activations': len(neuron_tokens[i])
                }
            else:
                ax.text(0.5, 0.5, f'Neuron {i}\n(no data)',
                       ha='center', va='center', fontsize=8)

            ax.axis('off')

        plt.suptitle('Neuron Token Preferences (Layer 0)', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'neuron_wordclouds_grid.png', dpi=150)
        plt.close()

        # Save individual wordclouds for top neurons
        for i in sorted(neuron_tokens.keys(), key=lambda x: len(neuron_tokens[x]), reverse=True)[:10]:
            if len(neuron_tokens[i]) > 10:
                token_freq = Counter(neuron_tokens[i])
                try:
                    wc = WordCloud(
                        width=800, height=400,
                        background_color='white',
                        max_words=100,
                        colormap='viridis'
                    ).generate_from_frequencies(token_freq)

                    plt.figure(figsize=(10, 5))
                    plt.imshow(wc, interpolation='bilinear')
                    plt.axis('off')
                    plt.title(f'Neuron {i} - Top Tokens')
                    plt.savefig(wc_dir / f'neuron_{i}_wordcloud.png', dpi=150)
                    plt.close()
                except:
                    pass

        # Save stats
        with open(output_dir / 'neuron_wordclouds.json', 'w') as f:
            # Convert to serializable format
            serializable_stats = {}
            for k, v in neuron_stats.items():
                serializable_stats[str(k)] = {
                    'top_tokens': v['top_tokens'],
                    'total_activations': v['total_activations']
                }
            json.dump(serializable_stats, f, indent=2)

        print(f"Saved wordclouds for {len(neuron_stats)} neurons")
        return neuron_stats

    def analyze_sentence_paths(self, texts, output_dir, n_sentences=10):
        """3. Sankey diagram for sentence routing paths"""
        print("\n=== 3. Sentence Path Sankey Diagrams ===")

        if not HAS_PLOTLY:
            print("Skipping Sankey diagrams (plotly not installed)")
            return {}

        sankey_dir = output_dir / 'sankey'
        sankey_dir.mkdir(exist_ok=True)

        results = []

        for sent_id, text in enumerate(texts[:n_sentences]):
            # Tokenize
            encoding = self.tokenizer(
                text,
                return_tensors='pt',
                max_length=64,
                truncation=True,
                padding=False
            )
            input_ids = encoding['input_ids'].to(self.device)

            routing_infos = None

            with torch.no_grad():
                # Try multiple methods to get routing info
                try:
                    outputs = self.model(input_ids, return_routing_info=True)
                    if isinstance(outputs, tuple):
                        if len(outputs) >= 3:
                            routing_infos = outputs[2]
                        elif len(outputs) == 2 and isinstance(outputs[1], (list, dict)):
                            routing_infos = outputs[1]
                except:
                    pass

                if routing_infos is None:
                    try:
                        _ = self.model(input_ids)
                        routing_infos = []
                        for layer in self.model.layers:
                            if hasattr(layer, 'last_routing_info'):
                                routing_infos.append(layer.last_routing_info)
                            elif hasattr(layer, 'attention') and hasattr(layer.attention, 'last_routing'):
                                routing_infos.append(layer.attention.last_routing)
                    except:
                        pass

            if not routing_infos:
                print(f"  Sentence {sent_id}: No routing info available")
                continue

            tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())

            # Build Sankey data
            sources = []
            targets = []
            values = []
            labels = []

            # Add token labels
            for i, token in enumerate(tokens[:20]):
                labels.append(f"T:{token[:10]}")

            # Add layer labels
            n_layers_to_show = min(4, len(routing_infos))
            for layer in range(n_layers_to_show):
                for neuron in range(self.n_process):
                    labels.append(f"L{layer}:N{neuron}")

            # Build connections
            token_offset = 0
            layer_offset = len(tokens[:20])

            # Helper to get indices from layer_info
            def get_layer_indices(layer_info, batch_idx, pos):
                if isinstance(layer_info, dict):
                    for key in ['neuron_indices', 'process_indices', 'indices', 'selected_neurons']:
                        if key in layer_info:
                            tensor = layer_info[key]
                            if tensor.dim() >= 2 and pos < tensor.shape[-1]:
                                return tensor[batch_idx, pos].cpu().numpy()
                elif isinstance(layer_info, torch.Tensor):
                    if layer_info.dim() >= 2 and pos < layer_info.shape[-1]:
                        return layer_info[batch_idx, pos].cpu().numpy()
                return None

            for pos, token in enumerate(tokens[:20]):
                if token in ['[CLS]', '[SEP]', '[PAD]']:
                    continue

                for layer_idx, layer_info in enumerate(routing_infos[:n_layers_to_show]):
                    indices = get_layer_indices(layer_info, 0, pos)
                    if indices is not None:
                        for neuron_idx in indices:
                            if layer_idx == 0:
                                source = pos
                            else:
                                # Previous layer neuron
                                prev_indices = get_layer_indices(routing_infos[layer_idx-1], 0, pos)
                                if prev_indices is not None and len(prev_indices) > 0:
                                    source = layer_offset + (layer_idx - 1) * self.n_process + int(prev_indices[0])
                                else:
                                    source = pos

                            target = layer_offset + layer_idx * self.n_process + int(neuron_idx)

                            sources.append(source)
                            targets.append(target)
                            values.append(1)

            if sources:
                # Create Sankey diagram
                fig = go.Figure(data=[go.Sankey(
                    node=dict(
                        pad=15,
                        thickness=20,
                        line=dict(color="black", width=0.5),
                        label=labels[:layer_offset + n_layers_to_show * self.n_process],
                        color="lightblue"
                    ),
                    link=dict(
                        source=sources,
                        target=targets,
                        value=values
                    )
                )])

                fig.update_layout(
                    title=f"Sentence {sent_id}: Token Routing Path",
                    font_size=10,
                    width=1200,
                    height=800
                )

                # Try PNG first (requires kaleido), fallback to HTML
                try:
                    fig.write_image(str(sankey_dir / f'sentence_{sent_id}_sankey.png'))
                except (ValueError, ImportError):
                    fig.write_html(str(sankey_dir / f'sentence_{sent_id}_sankey.html'))

                results.append({
                    'sentence_id': sent_id,
                    'text': text[:100],
                    'n_tokens': len(tokens),
                    'n_connections': len(sources)
                })

        with open(output_dir / 'sentence_paths.json', 'w') as f:
            json.dump(results, f, indent=2)

        print(f"Created Sankey diagrams for {len(results)} sentences")
        return results

    def analyze_layer_cluster_evolution(self, data, output_dir):
        """4. Cluster evolution across layers"""
        print("\n=== 4. Layer Cluster Evolution ===")

        tokens = data['tokens']
        routing = data['routing']

        # Get routing for specific layers
        target_layers = [0, self.n_layers // 2, self.n_layers - 1]

        fig, axes = plt.subplots(1, len(target_layers), figsize=(18, 6))

        # Get POS tags
        if HAS_NLTK:
            pos_tags = []
            for token in tokens:
                clean_token = token.replace('##', '')
                try:
                    tag = pos_tag([clean_token])[0][1]
                except:
                    tag = 'OTHER'
                pos_tags.append(get_pos_category(tag))
        else:
            pos_tags = ['Other'] * len(tokens)

        categories = list(set(pos_tags))
        colors = plt.cm.tab10(np.linspace(0, 1, len(categories)))
        color_map = {cat: colors[i] for i, cat in enumerate(categories)}

        layer_results = {}

        for ax_idx, layer_idx in enumerate(target_layers):
            # Extract layer-specific routing
            layer_vectors = []
            valid_indices = []

            for i, r in enumerate(routing):
                if r and len(r) > layer_idx:
                    layer_vectors.append(np.array(r[layer_idx]).flatten())
                    valid_indices.append(i)

            if not layer_vectors:
                continue

            # Pad vectors
            max_len = max(len(v) for v in layer_vectors)
            padded = np.zeros((len(layer_vectors), max_len))
            for i, v in enumerate(layer_vectors):
                padded[i, :len(v)] = v

            # t-SNE (GPU-accelerated if cuML available)
            if len(padded) > 10:
                perp = min(30, len(padded)//4)
                if HAS_CUML:
                    padded_gpu = cp.asarray(padded.astype(np.float32))
                    tsne = cuTSNE(n_components=2, random_state=42, perplexity=perp)
                    coords = cp.asnumpy(tsne.fit_transform(padded_gpu))
                else:
                    tsne = TSNE(n_components=2, random_state=42, perplexity=perp)
                    coords = tsne.fit_transform(padded)

                ax = axes[ax_idx]
                for cat in categories:
                    mask = [pos_tags[valid_indices[i]] == cat for i in range(len(valid_indices))]
                    if any(mask):
                        ax.scatter(
                            coords[mask, 0],
                            coords[mask, 1],
                            c=[color_map[cat]],
                            label=cat,
                            alpha=0.6,
                            s=20
                        )

                ax.set_title(f'Layer {layer_idx}', fontsize=12)
                ax.legend(loc='upper right', fontsize=8)

                layer_results[f'layer_{layer_idx}'] = {
                    'n_tokens': len(valid_indices)
                }

        plt.suptitle('Token Cluster Evolution Across Layers', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'layer_cluster_evolution.png', dpi=150)
        plt.close()

        with open(output_dir / 'layer_cluster_evolution.json', 'w') as f:
            json.dump(layer_results, f, indent=2)

        print("Saved layer cluster evolution analysis")
        return layer_results

    def analyze_synonyms(self, data, output_dir):
        """5. Synonym/similar word clustering"""
        print("\n=== 5. Synonym Clustering ===")

        synonym_groups = [
            ['big', 'large', 'huge', 'enormous', 'giant', 'massive'],
            ['good', 'great', 'nice', 'excellent', 'wonderful', 'fantastic'],
            ['bad', 'poor', 'terrible', 'awful', 'horrible', 'dreadful'],
            ['fast', 'quick', 'rapid', 'swift', 'speedy'],
            ['small', 'tiny', 'little', 'mini', 'minute'],
            ['happy', 'glad', 'joyful', 'pleased', 'delighted'],
            ['sad', 'unhappy', 'miserable', 'depressed', 'sorrowful'],
            ['said', 'told', 'spoke', 'stated', 'mentioned', 'declared'],
            ['walk', 'run', 'move', 'go', 'travel', 'journey'],
            ['think', 'believe', 'consider', 'assume', 'suppose']
        ]

        tokens = data['tokens']
        routing = data['routing']

        # Build token -> routing map
        token_routing = defaultdict(list)
        for token, route in zip(tokens, routing):
            clean_token = token.replace('##', '').lower()
            if route:
                token_routing[clean_token].append(route)

        results = {}

        fig, axes = plt.subplots(2, 5, figsize=(20, 8))
        axes = axes.flatten()

        for idx, group in enumerate(synonym_groups):
            ax = axes[idx]

            group_neurons = defaultdict(list)
            found_words = []

            for word in group:
                if word in token_routing:
                    found_words.append(word)
                    for route in token_routing[word]:
                        if route and len(route) > 0:
                            for neuron in route[0]:  # First layer
                                group_neurons[word].append(int(neuron))

            if found_words:
                # Count neuron usage per word
                word_neuron_counts = {}
                for word in found_words:
                    counts = Counter(group_neurons[word])
                    word_neuron_counts[word] = counts

                # Plot heatmap
                all_neurons = sorted(set(
                    n for counts in word_neuron_counts.values()
                    for n in counts.keys()
                ))[:20]  # Top 20 neurons

                if all_neurons:
                    matrix = np.zeros((len(found_words), len(all_neurons)))
                    for i, word in enumerate(found_words):
                        for j, neuron in enumerate(all_neurons):
                            matrix[i, j] = word_neuron_counts[word].get(neuron, 0)

                    # Normalize
                    row_sums = matrix.sum(axis=1, keepdims=True)
                    row_sums[row_sums == 0] = 1
                    matrix = matrix / row_sums

                    sns.heatmap(matrix, ax=ax, cmap='YlOrRd',
                               xticklabels=all_neurons, yticklabels=found_words,
                               cbar=False)
                    ax.set_title(f'{group[0]}-like words', fontsize=10)
                    ax.tick_params(labelsize=7)

                results[group[0]] = {
                    'words_found': found_words,
                    'shared_neurons': list(all_neurons) if all_neurons else []
                }
            else:
                ax.text(0.5, 0.5, f'No data for\n{group[0]} group',
                       ha='center', va='center')
                ax.axis('off')

        plt.suptitle('Synonym Neuron Sharing Analysis', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'synonym_clustering.png', dpi=150)
        plt.close()

        with open(output_dir / 'synonym_clustering.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("Saved synonym clustering analysis")
        return results

    def analyze_antonyms(self, data, output_dir):
        """6. Antonym comparison"""
        print("\n=== 6. Antonym Comparison ===")

        antonym_pairs = [
            ('good', 'bad'),
            ('up', 'down'),
            ('hot', 'cold'),
            ('big', 'small'),
            ('fast', 'slow'),
            ('happy', 'sad'),
            ('light', 'dark'),
            ('new', 'old'),
            ('high', 'low'),
            ('open', 'close')
        ]

        tokens = data['tokens']
        routing = data['routing']

        # Build token -> routing map
        token_routing = defaultdict(list)
        for token, route in zip(tokens, routing):
            clean_token = token.replace('##', '').lower()
            if route:
                token_routing[clean_token].append(route)

        results = {}

        fig, axes = plt.subplots(2, 5, figsize=(20, 8))
        axes = axes.flatten()

        for idx, (word1, word2) in enumerate(antonym_pairs):
            ax = axes[idx]

            neurons1 = []
            neurons2 = []

            for route in token_routing.get(word1, []):
                if route and len(route) > 0:
                    neurons1.extend([int(n) for n in route[0]])

            for route in token_routing.get(word2, []):
                if route and len(route) > 0:
                    neurons2.extend([int(n) for n in route[0]])

            if neurons1 and neurons2:
                counts1 = Counter(neurons1)
                counts2 = Counter(neurons2)

                all_neurons = sorted(set(counts1.keys()) | set(counts2.keys()))[:15]

                x = np.arange(len(all_neurons))
                width = 0.35

                vals1 = [counts1.get(n, 0) for n in all_neurons]
                vals2 = [counts2.get(n, 0) for n in all_neurons]

                # Normalize
                sum1 = sum(vals1) or 1
                sum2 = sum(vals2) or 1
                vals1 = [v / sum1 for v in vals1]
                vals2 = [v / sum2 for v in vals2]

                ax.bar(x - width/2, vals1, width, label=word1, color='blue', alpha=0.7)
                ax.bar(x + width/2, vals2, width, label=word2, color='red', alpha=0.7)
                ax.set_xticks(x)
                ax.set_xticklabels(all_neurons, fontsize=7)
                ax.legend(fontsize=8)
                ax.set_title(f'{word1} vs {word2}', fontsize=10)

                # Calculate overlap
                shared = set(counts1.keys()) & set(counts2.keys())
                overlap = len(shared) / len(set(counts1.keys()) | set(counts2.keys())) if counts1 or counts2 else 0

                results[f'{word1}_vs_{word2}'] = {
                    'word1_neurons': list(counts1.keys()),
                    'word2_neurons': list(counts2.keys()),
                    'overlap_ratio': overlap,
                    'shared_neurons': list(shared)
                }
            else:
                ax.text(0.5, 0.5, f'No data for\n{word1}/{word2}',
                       ha='center', va='center')
                ax.axis('off')

        plt.suptitle('Antonym Neuron Differentiation', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'antonym_comparison.png', dpi=150)
        plt.close()

        with open(output_dir / 'antonym_comparison.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("Saved antonym comparison analysis")
        return results

    def analyze_polysemy(self, data, output_dir):
        """7. Context-dependent token analysis (polysemy)"""
        print("\n=== 7. Polysemy Analysis ===")

        polysemous_words = ['bank', 'run', 'play', 'set', 'light', 'right', 'left', 'book', 'watch', 'spring']

        tokens = data['tokens']
        routing = data['routing']
        sentence_ids = data['sentence_ids']

        # Debug: Check token presence and routing structure
        print(f"\n[DEBUG] Polysemy Analysis Diagnostics:")
        print(f"  Total tokens collected: {len(tokens)}")
        print(f"  Total routing entries: {len(routing)}")

        # Check tokenizer IDs for polysemous words
        print(f"\n  Tokenizer IDs:")
        for word in polysemous_words[:5]:  # First 5
            try:
                token_id = self.tokenizer.convert_tokens_to_ids(word)
                # Also check with ## prefix
                subword_id = self.tokenizer.convert_tokens_to_ids(f"##{word}")
                print(f"    '{word}': id={token_id}, '##'{word}': id={subword_id}")
            except:
                print(f"    '{word}': error getting ID")

        # Count occurrences
        token_counts = defaultdict(int)
        for token in tokens:
            clean = token.replace('##', '').lower()
            if clean in polysemous_words:
                token_counts[clean] += 1

        print(f"\n  Token occurrences in data:")
        for word in polysemous_words:
            print(f"    '{word}': {token_counts[word]} times")

        # Collect contexts for polysemous words
        word_contexts = defaultdict(list)

        for i, (token, route, sent_id) in enumerate(zip(tokens, routing, sentence_ids)):
            clean_token = token.replace('##', '').lower()
            if clean_token in polysemous_words and route:
                word_contexts[clean_token].append({
                    'sentence_id': sent_id,
                    'routing': route,
                    'position': i
                })

        # Debug: Check routing structure for each word
        print(f"\n  Contexts with routing data:")
        for word in polysemous_words:
            if word in word_contexts:
                contexts = word_contexts[word]
                valid_vectors = 0
                empty_first_layer = 0
                for ctx in contexts[:50]:
                    if ctx['routing'] and len(ctx['routing']) > 0:
                        first_layer = ctx['routing'][0]
                        if len(first_layer) > 0:
                            valid_vectors += 1
                        else:
                            empty_first_layer += 1
                print(f"    '{word}': {len(contexts)} contexts, {valid_vectors} valid vectors, {empty_first_layer} empty first layer")
            else:
                print(f"    '{word}': NOT in word_contexts")

        results = {}

        fig, axes = plt.subplots(2, 5, figsize=(20, 8))
        axes = axes.flatten()

        for idx, word in enumerate(polysemous_words):
            ax = axes[idx]

            if word in word_contexts and len(word_contexts[word]) > 2:
                contexts = word_contexts[word]

                # Get routing vectors
                vectors = []
                for ctx in contexts[:50]:  # Limit samples
                    if ctx['routing'] and len(ctx['routing']) > 0:
                        vec = np.array(ctx['routing'][0]).flatten()
                        vectors.append(vec)

                if len(vectors) > 5:
                    # Pad vectors
                    max_len = max(len(v) for v in vectors)
                    padded = np.zeros((len(vectors), max_len))
                    for i, v in enumerate(vectors):
                        padded[i, :len(v)] = v

                    # t-SNE (GPU-accelerated if cuML available)
                    if len(padded) > 5:
                        perp = min(5, len(padded) - 1)
                        if HAS_CUML:
                            padded_gpu = cp.asarray(padded.astype(np.float32))
                            tsne = cuTSNE(n_components=2, random_state=42, perplexity=perp)
                            coords = cp.asnumpy(tsne.fit_transform(padded_gpu))
                        else:
                            tsne = TSNE(n_components=2, random_state=42, perplexity=perp)
                            coords = tsne.fit_transform(padded)

                        ax.scatter(coords[:, 0], coords[:, 1], alpha=0.6, s=30)
                        ax.set_title(f'"{word}" contexts', fontsize=10)

                        # Calculate variance (proxy for context sensitivity)
                        variance = np.var(coords, axis=0).sum()

                        results[word] = {
                            'n_occurrences': len(contexts),
                            'variance': float(variance),
                            'context_sensitive': bool(variance > 1.0)
                        }
                    else:
                        ax.text(0.5, 0.5, f'"{word}"\ninsufficient data', ha='center', va='center')
                        ax.axis('off')
                else:
                    ax.text(0.5, 0.5, f'"{word}"\nno valid routing', ha='center', va='center')
                    ax.axis('off')
            else:
                ax.text(0.5, 0.5, f'"{word}"\nnot found', ha='center', va='center')
                ax.axis('off')

        plt.suptitle('Polysemous Word Context Variation', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'polysemy_analysis.png', dpi=150)
        plt.close()

        with open(output_dir / 'polysemy_analysis.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("Saved polysemy analysis")
        return results

    def analyze_position_patterns(self, data, output_dir):
        """8. Position-based routing patterns"""
        print("\n=== 8. Position Pattern Analysis ===")

        tokens = data['tokens']
        routing = data['routing']
        positions = data['positions']

        # Bin positions
        position_bins = ['Start (0-0.2)', 'Early (0.2-0.4)', 'Middle (0.4-0.6)',
                         'Late (0.6-0.8)', 'End (0.8-1.0)']

        position_neurons = {bin_name: [] for bin_name in position_bins}

        for route, pos in zip(routing, positions):
            if route and len(route) > 0:
                if pos < 0.2:
                    bin_name = position_bins[0]
                elif pos < 0.4:
                    bin_name = position_bins[1]
                elif pos < 0.6:
                    bin_name = position_bins[2]
                elif pos < 0.8:
                    bin_name = position_bins[3]
                else:
                    bin_name = position_bins[4]

                position_neurons[bin_name].extend([int(n) for n in route[0]])

        # Create heatmap
        all_neurons = sorted(set(n for neurons in position_neurons.values() for n in neurons))[:30]

        matrix = np.zeros((len(position_bins), len(all_neurons)))
        for i, bin_name in enumerate(position_bins):
            counts = Counter(position_neurons[bin_name])
            total = sum(counts.values()) or 1
            for j, neuron in enumerate(all_neurons):
                matrix[i, j] = counts.get(neuron, 0) / total

        fig, ax = plt.subplots(figsize=(14, 6))
        sns.heatmap(matrix, ax=ax, cmap='YlOrRd',
                   xticklabels=all_neurons, yticklabels=position_bins,
                   annot=False)
        ax.set_xlabel('Neuron ID')
        ax.set_ylabel('Position in Sequence')
        ax.set_title('Position-based Neuron Activation Patterns')

        plt.tight_layout()
        plt.savefig(output_dir / 'position_patterns.png', dpi=150)
        plt.close()

        results = {
            bin_name: {
                'top_neurons': Counter(position_neurons[bin_name]).most_common(10),
                'n_tokens': len(position_neurons[bin_name])
            }
            for bin_name in position_bins
        }

        with open(output_dir / 'position_patterns.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("Saved position pattern analysis")
        return results

    def analyze_named_entities(self, data, output_dir):
        """9. Named Entity analysis"""
        print("\n=== 9. Named Entity Analysis ===")

        tokens = data['tokens']
        routing = data['routing']

        # Simple NE detection (capitalized words, common patterns)
        entity_neurons = {
            'PERSON': [],
            'LOCATION': [],
            'ORGANIZATION': [],
            'OTHER_PROPER': []
        }

        # Common name/location/org patterns (simplified)
        person_names = {'john', 'mary', 'david', 'james', 'michael', 'sarah', 'robert', 'william'}
        locations = {'london', 'paris', 'york', 'washington', 'california', 'america', 'europe', 'china'}
        orgs = {'google', 'microsoft', 'apple', 'amazon', 'facebook', 'twitter', 'university', 'company'}

        for token, route in zip(tokens, routing):
            if not route or len(route) == 0:
                continue

            clean_token = token.replace('##', '').lower()
            neurons = [int(n) for n in route[0]]

            if clean_token in person_names:
                entity_neurons['PERSON'].extend(neurons)
            elif clean_token in locations:
                entity_neurons['LOCATION'].extend(neurons)
            elif clean_token in orgs:
                entity_neurons['ORGANIZATION'].extend(neurons)
            elif token[0].isupper() if token else False:
                entity_neurons['OTHER_PROPER'].extend(neurons)

        # Plot
        fig, axes = plt.subplots(1, 4, figsize=(16, 4))

        results = {}
        for idx, (entity_type, neurons) in enumerate(entity_neurons.items()):
            ax = axes[idx]

            if neurons:
                counts = Counter(neurons)
                top_neurons = counts.most_common(15)

                ax.barh([str(n) for n, _ in top_neurons],
                       [c for _, c in top_neurons],
                       color='steelblue')
                ax.set_xlabel('Count')
                ax.set_title(f'{entity_type}\n({len(neurons)} tokens)')
                ax.invert_yaxis()

                results[entity_type] = {
                    'top_neurons': top_neurons,
                    'n_tokens': len(neurons)
                }
            else:
                ax.text(0.5, 0.5, f'{entity_type}\nNo data', ha='center', va='center')
                ax.axis('off')

        plt.suptitle('Named Entity Neuron Preferences', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'named_entity_analysis.png', dpi=150)
        plt.close()

        with open(output_dir / 'named_entity_analysis.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("Saved named entity analysis")
        return results

    def analyze_frequency_patterns(self, data, output_dir):
        """10. Token frequency analysis"""
        print("\n=== 10. Frequency Pattern Analysis ===")

        tokens = data['tokens']
        routing = data['routing']

        # Count token frequencies
        token_freq = Counter(tokens)

        # Categorize by frequency
        high_freq_tokens = {t for t, c in token_freq.items() if c >= 50}
        mid_freq_tokens = {t for t, c in token_freq.items() if 10 <= c < 50}
        low_freq_tokens = {t for t, c in token_freq.items() if c < 10}

        freq_categories = {
            'High Freq (>=50)': high_freq_tokens,
            'Mid Freq (10-49)': mid_freq_tokens,
            'Low Freq (<10)': low_freq_tokens
        }

        freq_neurons = {cat: [] for cat in freq_categories}

        for token, route in zip(tokens, routing):
            if not route or len(route) == 0:
                continue

            neurons = [int(n) for n in route[0]]

            for cat, cat_tokens in freq_categories.items():
                if token in cat_tokens:
                    freq_neurons[cat].extend(neurons)
                    break

        # Plot
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        results = {}
        for idx, (cat, neurons) in enumerate(freq_neurons.items()):
            ax = axes[idx]

            if neurons:
                counts = Counter(neurons)

                # Histogram of neuron distribution
                all_counts = list(counts.values())
                ax.hist(all_counts, bins=20, color='steelblue', edgecolor='black', alpha=0.7)
                ax.set_xlabel('Activation Count')
                ax.set_ylabel('Number of Neurons')
                ax.set_title(f'{cat}\n(Gini: {self._gini_coefficient(all_counts):.3f})')

                results[cat] = {
                    'n_tokens': len(neurons),
                    'unique_neurons': len(counts),
                    'gini': self._gini_coefficient(all_counts),
                    'top_neurons': counts.most_common(10)
                }
            else:
                ax.text(0.5, 0.5, f'{cat}\nNo data', ha='center', va='center')
                ax.axis('off')

        plt.suptitle('Frequency-based Neuron Distribution', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'frequency_patterns.png', dpi=150)
        plt.close()

        with open(output_dir / 'frequency_patterns.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("Saved frequency pattern analysis")
        return results

    def _gini_coefficient(self, values):
        """Calculate Gini coefficient"""
        if not values:
            return 0
        sorted_values = sorted(values)
        n = len(sorted_values)
        cumsum = np.cumsum(sorted_values)
        return (2 * np.sum((np.arange(1, n+1) * sorted_values)) / (n * cumsum[-1])) - (n + 1) / n

    def analyze_knowledge_neurons(self, data, output_dir):
        """11. Knowledge Neuron semantic analysis"""
        print("\n=== 11. Knowledge Neuron Analysis ===")

        tokens = data['tokens']
        knowledge_routing = data['knowledge_routing']

        if not knowledge_routing or not any(knowledge_routing):
            print("No knowledge routing data available")
            return {}

        # Collect tokens per knowledge neuron
        knowledge_tokens = defaultdict(list)

        for token, k_route in zip(tokens, knowledge_routing):
            if k_route and len(k_route) > 0:
                for k_idx in k_route[0]:  # First layer
                    clean_token = token.replace('##', '')
                    if len(clean_token) > 1:
                        knowledge_tokens[int(k_idx)].append(clean_token)

        if not knowledge_tokens:
            print("No knowledge neuron activations found")
            return {}

        # Create wordclouds for top knowledge neurons
        k_dir = output_dir / 'knowledge_neurons'
        k_dir.mkdir(exist_ok=True)

        results = {}

        # Sort by activation count
        sorted_k_neurons = sorted(knowledge_tokens.keys(),
                                  key=lambda x: len(knowledge_tokens[x]),
                                  reverse=True)[:20]

        fig, axes = plt.subplots(4, 5, figsize=(20, 16))
        axes = axes.flatten()

        for idx, k_idx in enumerate(sorted_k_neurons):
            ax = axes[idx]

            token_freq = Counter(knowledge_tokens[k_idx])

            if len(token_freq) > 3:
                try:
                    wc = WordCloud(
                        width=300, height=200,
                        background_color='white',
                        max_words=30,
                        colormap='plasma'
                    ).generate_from_frequencies(token_freq)

                    ax.imshow(wc, interpolation='bilinear')
                    ax.set_title(f'Knowledge Neuron {k_idx}\n({len(knowledge_tokens[k_idx])} tokens)', fontsize=9)
                except:
                    ax.text(0.5, 0.5, f'K-Neuron {k_idx}', ha='center', va='center')
            else:
                ax.text(0.5, 0.5, f'K-Neuron {k_idx}\n(sparse)', ha='center', va='center')

            ax.axis('off')

            results[str(k_idx)] = {
                'top_tokens': token_freq.most_common(15),
                'n_activations': len(knowledge_tokens[k_idx])
            }

        plt.suptitle('Knowledge Neuron Token Associations', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'knowledge_neurons.png', dpi=150)
        plt.close()

        with open(output_dir / 'knowledge_neurons.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("Saved knowledge neuron analysis")
        return results

    def analyze_qkvo_differentiation(self, data, output_dir):
        """12. Q/K/V/O role differentiation analysis"""
        print("\n=== 12. Q/K/V/O Differentiation Analysis ===")

        tokens = data['tokens']
        qkvo_routing = data['qkvo_routing']

        if not qkvo_routing or not any(qkvo_routing):
            print("No Q/K/V/O routing data available")
            # Create synthetic analysis based on general routing
            print("Using general routing for component analysis...")
            return self._synthetic_qkvo_analysis(data, output_dir)

        # Analyze Q/K/V/O neuron preferences
        component_neurons = {
            'Q': defaultdict(list),
            'K': defaultdict(list),
            'V': defaultdict(list),
            'O': defaultdict(list)
        }

        for token, qkvo in zip(tokens, qkvo_routing):
            if qkvo and len(qkvo) > 0:
                clean_token = token.replace('##', '')
                # Assuming qkvo has [Q_idx, K_idx, V_idx, O_idx] structure
                for layer_qkvo in qkvo:
                    if len(layer_qkvo) >= 4:
                        component_neurons['Q'][clean_token].append(layer_qkvo[0])
                        component_neurons['K'][clean_token].append(layer_qkvo[1])
                        component_neurons['V'][clean_token].append(layer_qkvo[2])
                        component_neurons['O'][clean_token].append(layer_qkvo[3])

        results = {}

        # Analyze overlap between components
        fig, ax = plt.subplots(figsize=(8, 6))

        overlap_matrix = np.zeros((4, 4))
        components = ['Q', 'K', 'V', 'O']

        for i, comp1 in enumerate(components):
            for j, comp2 in enumerate(components):
                if i == j:
                    overlap_matrix[i, j] = 1.0
                else:
                    # Calculate Jaccard similarity
                    all_neurons1 = set(n for neurons in component_neurons[comp1].values() for n in neurons)
                    all_neurons2 = set(n for neurons in component_neurons[comp2].values() for n in neurons)

                    if all_neurons1 or all_neurons2:
                        jaccard = len(all_neurons1 & all_neurons2) / len(all_neurons1 | all_neurons2)
                        overlap_matrix[i, j] = jaccard

        sns.heatmap(overlap_matrix, ax=ax, cmap='RdYlBu_r',
                   xticklabels=components, yticklabels=components,
                   annot=True, fmt='.2f', vmin=0, vmax=1)
        ax.set_title('Q/K/V/O Neuron Overlap (Jaccard Similarity)')

        plt.tight_layout()
        plt.savefig(output_dir / 'qkvo_differentiation.png', dpi=150)
        plt.close()

        for comp in components:
            all_neurons = [n for neurons in component_neurons[comp].values() for n in neurons]
            results[comp] = {
                'unique_neurons': len(set(all_neurons)),
                'total_activations': len(all_neurons),
                'top_neurons': Counter(all_neurons).most_common(10)
            }

        with open(output_dir / 'qkvo_differentiation.json', 'w') as f:
            json.dump(results, f, indent=2)

        print("Saved Q/K/V/O differentiation analysis")
        return results

    def _synthetic_qkvo_analysis(self, data, output_dir):
        """Synthetic Q/K/V/O analysis when actual data not available"""
        tokens = data['tokens']
        routing = data['routing']

        # Analyze based on POS tags (verbs might need different Q/K patterns)
        if HAS_NLTK:
            pos_component_affinity = defaultdict(lambda: defaultdict(int))

            for token, route in zip(tokens, routing):
                if not route or len(route) == 0:
                    continue

                clean_token = token.replace('##', '')
                try:
                    tag = pos_tag([clean_token])[0][1]
                    category = get_pos_category(tag)
                except:
                    category = 'Other'

                for neuron in route[0]:
                    pos_component_affinity[category][int(neuron)] += 1

            # Plot POS-based neuron preferences
            fig, ax = plt.subplots(figsize=(12, 8))

            categories = list(pos_component_affinity.keys())
            all_neurons = sorted(set(
                n for counts in pos_component_affinity.values()
                for n in counts.keys()
            ))[:25]

            matrix = np.zeros((len(categories), len(all_neurons)))
            for i, cat in enumerate(categories):
                total = sum(pos_component_affinity[cat].values()) or 1
                for j, neuron in enumerate(all_neurons):
                    matrix[i, j] = pos_component_affinity[cat].get(neuron, 0) / total

            sns.heatmap(matrix, ax=ax, cmap='YlOrRd',
                       xticklabels=all_neurons, yticklabels=categories)
            ax.set_xlabel('Neuron ID')
            ax.set_ylabel('POS Category')
            ax.set_title('POS-based Neuron Preferences\n(Proxy for Q/K/V/O Role Analysis)')

            plt.tight_layout()
            plt.savefig(output_dir / 'qkvo_differentiation.png', dpi=150)
            plt.close()

            results = {
                'analysis_type': 'pos_proxy',
                'categories': {
                    cat: {
                        'top_neurons': Counter(pos_component_affinity[cat]).most_common(10)
                    }
                    for cat in categories
                }
            }
        else:
            results = {'analysis_type': 'unavailable'}

        with open(output_dir / 'qkvo_differentiation.json', 'w') as f:
            json.dump(results, f, indent=2)

        return results

    def generate_report(self, all_results, output_dir):
        """Generate markdown summary report"""
        print("\n=== Generating Summary Report ===")

        report = f"""# DAWN v8.0 Semantic Analysis Report

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Overview

This report analyzes semantic patterns in neuron routing for DAWN v8.0 model.

### Model Configuration
- Layers: {self.n_layers}
- Process Neurons: {self.n_process}
- Knowledge Neurons: {self.n_knowledge}

---

## 1. Token Cluster Visualization

![Token Clusters](token_clusters_tsne.png)

Tokens are clustered based on their routing patterns. Colors represent POS categories.

**Key Findings:**
- Total tokens analyzed: {all_results.get('token_clusters', {}).get('n_tokens', 'N/A')}
- POS distribution: {json.dumps(all_results.get('token_clusters', {}).get('pos_distribution', {}), indent=2)}

---

## 2. Neuron WordClouds

![Neuron WordClouds](neuron_wordclouds_grid.png)

Each neuron shows preference for certain types of tokens.

**Top Neurons by Activation:**
"""

        neuron_stats = all_results.get('neuron_wordclouds', {})
        for neuron_id in list(neuron_stats.keys())[:5]:
            stats = neuron_stats[neuron_id]
            top_tokens = [t for t, _ in stats.get('top_tokens', [])[:5]]
            report += f"- Neuron {neuron_id}: {', '.join(top_tokens)}\n"

        report += """
---

## 3. Sentence Routing Paths

See `sankey/` folder for individual sentence Sankey diagrams.

---

## 4. Layer Cluster Evolution

![Layer Evolution](layer_cluster_evolution.png)

Shows how token clusters change as they pass through layers.

---

## 5. Synonym Clustering

![Synonyms](synonym_clustering.png)

Analysis of whether synonyms share similar neuron patterns.

**Findings:**
"""

        synonym_results = all_results.get('synonyms', {})
        for word, data in list(synonym_results.items())[:3]:
            words_found = data.get('words_found', [])
            report += f"- {word}-like words: {', '.join(words_found)}\n"

        report += """
---

## 6. Antonym Comparison

![Antonyms](antonym_comparison.png)

Analysis of neuron differentiation between antonyms.

**Overlap Analysis:**
"""

        antonym_results = all_results.get('antonyms', {})
        for pair, data in list(antonym_results.items())[:3]:
            overlap = data.get('overlap_ratio', 0)
            report += f"- {pair}: {overlap:.1%} neuron overlap\n"

        report += """
---

## 7. Polysemy Analysis

![Polysemy](polysemy_analysis.png)

Analysis of context-dependent routing for polysemous words.

---

## 8. Position Patterns

![Position](position_patterns.png)

Neuron preferences based on token position in sequence.

---

## 9. Named Entity Analysis

![Named Entities](named_entity_analysis.png)

Neuron preferences for different entity types.

---

## 10. Frequency Patterns

![Frequency](frequency_patterns.png)

Comparison of routing patterns for high vs low frequency tokens.

---

## 11. Knowledge Neurons

![Knowledge Neurons](knowledge_neurons.png)

Semantic associations for knowledge neurons.

---

## 12. Q/K/V/O Differentiation

![QKVO](qkvo_differentiation.png)

Analysis of role differentiation between Q/K/V/O components.

---

## Conclusions

This analysis reveals the semantic organization of DAWN v8.0's neuron routing:

1. **POS-based clustering**: Tokens cluster by grammatical function
2. **Semantic similarity**: Synonyms tend to share neurons
3. **Antonym differentiation**: Opposite meanings use different neurons
4. **Context sensitivity**: Polysemous words show varied routing
5. **Positional patterns**: Token position influences neuron selection

---

*Generated by DAWN Semantic Analyzer*
"""

        report_path = output_dir / 'analysis_report.md'
        with open(report_path, 'w') as f:
            f.write(report)

        print(f"Report saved to {report_path}")
        return report_path


# ============================================================
# Main
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='DAWN v8.0 Semantic Analysis')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to checkpoint file')
    parser.add_argument('--data', type=str, required=True,
                       help='Path to validation data pickle')
    parser.add_argument('--output', type=str, required=True,
                       help='Output directory for results')
    parser.add_argument('--max-samples', type=int, default=500,
                       help='Maximum number of text samples to process')
    parser.add_argument('--max-tokens', type=int, default=30000,
                       help='Maximum number of tokens to analyze')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='Batch size for GPU processing')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (cuda/cpu)')

    args = parser.parse_args()

    # Create output directory
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 60)
    print("DAWN v8.0 Comprehensive Semantic Analysis (GPU Optimized)")
    print("=" * 60)

    # Check GPU
    device = args.device if torch.cuda.is_available() else 'cpu'
    if device == 'cuda':
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("Running on CPU (slower)")
    print(f"Batch size: {args.batch_size}")
    print("=" * 60)

    # Load model
    model, tokenizer, config = load_model_and_tokenizer(args.checkpoint, device)

    # Load data
    texts = load_data(args.data, max_samples=args.max_samples)

    # Create analyzer
    analyzer = SemanticAnalyzer(model, tokenizer, config, device)

    # Collect routing information (GPU optimized batching)
    data = analyzer.collect_routing_info(texts, max_tokens=args.max_tokens, batch_size=args.batch_size)

    # Run all analyses
    all_results = {}

    # 1. Token clusters
    all_results['token_clusters'] = analyzer.analyze_token_clusters(data, output_dir)

    # 2. Neuron wordclouds
    all_results['neuron_wordclouds'] = analyzer.analyze_neuron_wordclouds(data, output_dir)

    # 3. Sentence paths (Sankey)
    all_results['sentence_paths'] = analyzer.analyze_sentence_paths(texts, output_dir)

    # 4. Layer cluster evolution
    all_results['layer_evolution'] = analyzer.analyze_layer_cluster_evolution(data, output_dir)

    # 5. Synonyms
    all_results['synonyms'] = analyzer.analyze_synonyms(data, output_dir)

    # 6. Antonyms
    all_results['antonyms'] = analyzer.analyze_antonyms(data, output_dir)

    # 7. Polysemy
    all_results['polysemy'] = analyzer.analyze_polysemy(data, output_dir)

    # 8. Position patterns
    all_results['position_patterns'] = analyzer.analyze_position_patterns(data, output_dir)

    # 9. Named entities
    all_results['named_entities'] = analyzer.analyze_named_entities(data, output_dir)

    # 10. Frequency patterns
    all_results['frequency_patterns'] = analyzer.analyze_frequency_patterns(data, output_dir)

    # 11. Knowledge neurons
    all_results['knowledge_neurons'] = analyzer.analyze_knowledge_neurons(data, output_dir)

    # 12. Q/K/V/O differentiation
    all_results['qkvo'] = analyzer.analyze_qkvo_differentiation(data, output_dir)

    # Generate report
    analyzer.generate_report(all_results, output_dir)

    # Save all results
    with open(output_dir / 'all_results.json', 'w') as f:
        # Convert non-serializable items
        serializable = {}
        for k, v in all_results.items():
            if isinstance(v, dict):
                serializable[k] = {
                    str(k2): v2 if isinstance(v2, (str, int, float, list, dict, type(None)))
                    else str(v2)
                    for k2, v2 in v.items()
                }
            else:
                serializable[k] = str(v)
        json.dump(serializable, f, indent=2, default=str)

    print("\n" + "=" * 60)
    print("Analysis Complete!")
    print("=" * 60)
    print(f"Results saved to: {output_dir}")
    print(f"Report: {output_dir / 'analysis_report.md'}")


if __name__ == '__main__':
    main()
