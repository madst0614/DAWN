"""
Comprehensive Analysis for Neuron-Based SPROUT Model

Implements all 7 analysis categories:
1. Neuron usage patterns
2. Performance vs sparsity tradeoff
3. Router quality analysis
4. Neuron specialization
5. Layer-wise differences
6. Dynamic routing effects
7. Computation efficiency
"""
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertTokenizer
from collections import defaultdict
import numpy as np
import argparse
import time
import pickle
from typing import Dict, List, Tuple
from datasets import load_dataset

from src.models.sprout_neuron_based import NeuronBasedLanguageModel


# ============================================================
# Data Loading (Same as train_neuron_based.py)
# ============================================================

def load_validation_texts(use_cache: bool = True) -> List[str]:
    """
    Load validation texts with caching support

    Same strategy as training script:
    1. Try to load from cache
    2. Fall back to downloading if cache not found

    Returns:
        valid_texts: List of validation texts
    """
    cache_paths = [
        "/content/drive/MyDrive/dawn_v4/cache/train/wikitext_texts.pkl",
        "/content/drive/MyDrive/dawn_v4/cache/validation/wikitext_texts.pkl"
    ]

    valid_texts = None

    # Try cache first
    if use_cache and all(os.path.exists(p) for p in cache_paths):
        print("ğŸ“‚ Found cached dataset! Loading from cache...")
        try:
            with open(cache_paths[0], 'rb') as f:
                all_train_texts = pickle.load(f)
            with open(cache_paths[1], 'rb') as f:
                original_valid_texts = pickle.load(f)

            # Combine and resplit (same as training)
            all_texts = all_train_texts + original_valid_texts
            valid_texts = all_texts[100000:]  # After 100K train

            print(f"âœ… Loaded {len(valid_texts)} validation texts from cache")
        except Exception as e:
            print(f"âš ï¸  Failed to load cache: {e}")
            print("Falling back to downloading dataset...")
            valid_texts = None

    # Download if cache failed or not using cache
    if valid_texts is None:
        print("ğŸ“¥ Downloading wikitext dataset...")
        dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

        # Extract texts
        def extract_text(examples):
            return {'text': [t for t in examples['text'] if len(t.strip()) > 0]}

        dataset = dataset.map(extract_text, batched=True, remove_columns=dataset['validation'].column_names)
        valid_texts = dataset['validation']['text']

        print(f"âœ… Downloaded {len(valid_texts)} validation texts")

    return valid_texts


# ============================================================
# 1. Neuron Usage Patterns (ê°€ì¥ ì¤‘ìš”!)
# ============================================================

def analyze_neuron_usage_patterns(
    model: nn.Module,
    tokenizer,
    device: str,
    num_samples: int = 1000,
    top_k: int = 768,
    test_texts: List[str] = None  # ì‹¤ì œ validation set ì‚¬ìš© ê°€ëŠ¥
) -> Dict:
    """
    ë‰´ëŸ° ì‚¬ìš© íŒ¨í„´ ë¶„ì„ - ë¶ˆê· í˜• ì²´í¬

    Args:
        test_texts: ì‚¬ìš©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ í•˜ë“œì½”ë”©ëœ ìƒ˜í”Œ ì‚¬ìš©)

    Returns:
        stats: ê° ë ˆì´ì–´ë³„ ì‚¬ìš© í†µê³„
    """
    print("\n" + "="*70)
    print("1ï¸âƒ£  NEURON USAGE PATTERNS ANALYSIS")
    print("="*70)

    model.eval()

    # Use provided texts or fallback to hard-coded samples
    if test_texts is None:
        print("âš ï¸  Using hard-coded test texts (not real validation data)")
        test_texts = [
            "The quick brown fox jumps over the lazy dog",
            "Artificial intelligence is transforming the world",
            "Python is a popular programming language",
            "Machine learning requires large datasets",
            "Deep neural networks learn hierarchical representations",
            "Natural language processing enables computers to understand text",
            "The weather today is sunny and warm",
            "I love reading books in my free time",
            "Mathematics is the foundation of science",
            "Climate change is a global challenge",
        ] * 100  # 1000 samples
    else:
        print(f"âœ… Using {len(test_texts)} validation texts")

    test_texts = test_texts[:num_samples]

    n_layers = len(model.layers)
    d_ff = model.layers[0].ffn.d_ff

    # ê° ë ˆì´ì–´ë³„ ë‰´ëŸ° ì‚¬ìš© ì¹´ìš´íŠ¸ (CPUì— ì €ì¥)
    layer_usage = {i: torch.zeros(d_ff, device='cpu') for i in range(n_layers)}

    print(f"\nAnalyzing {num_samples} samples with top_k={top_k} ({top_k/d_ff*100:.1f}%)")

    with torch.no_grad():
        for idx, text in enumerate(test_texts):
            if idx % 100 == 0 and idx > 0:
                # ì£¼ê¸°ì ìœ¼ë¡œ ìºì‹œ ë¹„ìš°ê¸°
                if device == 'cuda':
                    torch.cuda.empty_cache()
            tokens = tokenizer(
                text,
                return_tensors='pt',
                padding='max_length',
                max_length=32,
                truncation=True
            )['input_ids'].to(device)

            batch, seq = tokens.shape

            # Forward pass ìˆ˜ë™ìœ¼ë¡œ ì§„í–‰í•˜ë©° ë¼ìš°íŒ… ê¸°ë¡
            token_emb = model.token_embedding(tokens)
            positions = torch.arange(seq, device=device).unsqueeze(0).expand(batch, -1)
            pos_emb = model.position_embedding(positions)
            x = token_emb + pos_emb

            for layer_idx, layer in enumerate(model.layers):
                # Attention
                x_norm = layer.norm1(x)
                attn_out, _ = layer.attention(x_norm, x_norm, x_norm)
                x = x + layer.dropout(attn_out)

                # FFN - ë¼ìš°í„° ì ìˆ˜ ê³„ì‚°
                x_norm = layer.norm2(x)
                x_flat = x_norm.view(-1, x_norm.shape[-1])

                # Router scores
                router_scores = layer.ffn.router.compute_scores(x_flat)
                _, top_indices = torch.topk(router_scores, top_k, dim=-1)

                # ì‚¬ìš©ëœ ë‰´ëŸ° ê¸°ë¡ (CPUë¡œ ì´ë™)
                layer_usage[layer_idx][top_indices.cpu().flatten().unique()] += 1

                # FFN í†µê³¼
                x_ffn = layer.ffn(x_norm, top_k=top_k)
                x = x + layer.dropout(x_ffn)

    # ë¶„ì„ ê²°ê³¼
    results = {}
    total_positions = num_samples * 32  # seq_len = 32

    print(f"\n{'Layer':<8} {'Never':<8} {'Rare(<10%)':<12} {'Common(â‰¥10%)':<15} {'Mean':<10} {'Max':<10} {'Gini':<10}")
    print("-" * 85)

    for layer_idx in range(n_layers):
        usage = layer_usage[layer_idx]

        # í†µê³„
        n_never = (usage == 0).sum().item()
        n_rare = ((usage > 0) & (usage < total_positions * 0.1)).sum().item()
        n_common = (usage >= total_positions * 0.1).sum().item()

        mean_usage = usage.mean().item()
        max_usage = usage.max().item()

        # Gini coefficient (ë¶ˆê· í˜• ì§€ìˆ˜, 0=ì™„ì „ ê· ë“±, 1=ì™„ì „ ë¶ˆê· ë“±)
        usage_sorted = torch.sort(usage)[0]
        n = len(usage_sorted)
        index = torch.arange(1, n + 1, device=usage_sorted.device, dtype=torch.float)
        gini = (2 * (index * usage_sorted).sum() / (n * usage_sorted.sum()) - (n + 1) / n).item()

        results[layer_idx] = {
            'never': n_never,
            'rare': n_rare,
            'common': n_common,
            'mean': mean_usage,
            'max': max_usage,
            'gini': gini,
            'usage_distribution': usage.cpu()
        }

        print(f"Layer {layer_idx:<2} {n_never:<8} {n_rare:<12} {n_common:<15} {mean_usage:<10.1f} {max_usage:<10.0f} {gini:<10.4f}")

    # ì¢…í•© í‰ê°€
    print("\nğŸ“Š Usage Pattern Evaluation:")
    avg_gini = np.mean([r['gini'] for r in results.values()])
    avg_never = np.mean([r['never'] / d_ff for r in results.values()])

    if avg_gini < 0.3 and avg_never < 0.1:
        print(f"âœ… EXCELLENT: Balanced usage (Gini={avg_gini:.3f}, Never={avg_never:.1%})")
    elif avg_gini < 0.5 and avg_never < 0.2:
        print(f"âœ”ï¸  GOOD: Acceptable usage (Gini={avg_gini:.3f}, Never={avg_never:.1%})")
    else:
        print(f"âš ï¸  WARNING: Unbalanced usage (Gini={avg_gini:.3f}, Never={avg_never:.1%})")

    return results


# ============================================================
# 2. Performance vs Sparsity Tradeoff
# ============================================================

def analyze_performance_vs_sparsity(
    model: nn.Module,
    tokenizer,
    device: str,
    test_texts: List[str] = None
) -> Dict:
    """
    ì„±ëŠ¥ vs í¬ì†Œë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
    """
    print("\n" + "="*70)
    print("2ï¸âƒ£  PERFORMANCE VS SPARSITY TRADEOFF")
    print("="*70)

    model.eval()

    if test_texts is None:
        test_texts = [
            "The [MASK] is shining brightly in the sky",
            "I love to [MASK] books in my free time",
            "Python is a programming [MASK] for data science",
            "She went to the [MASK] to buy groceries",
            "The cat is [MASK] on the comfortable mat",
            "Artificial [MASK] is transforming technology",
            "The capital of France is [MASK]",
            "Machine [MASK] requires large datasets",
            "Deep [MASK] networks learn from data",
            "The weather today is very [MASK]",
        ]

    d_ff = model.layers[0].ffn.d_ff

    # í…ŒìŠ¤íŠ¸í•  í¬ì†Œë„ ë ˆë²¨
    sparsity_levels = [
        (None, "Dense (100%)"),
        (int(0.75 * d_ff), "75%"),
        (int(0.50 * d_ff), "50%"),
        (int(0.25 * d_ff), "25%"),
        (int(0.10 * d_ff), "10%"),
        (int(0.05 * d_ff), "5%"),
    ]

    results = {}

    # Dense ì¶œë ¥ (ground truth) - CPUë¡œ ì´ë™
    print("\nComputing dense outputs as ground truth...")
    dense_outputs = []
    dense_correct = 0

    with torch.no_grad():
        for text in test_texts:
            tokens = tokenizer(
                text,
                return_tensors='pt',
                padding='max_length',
                max_length=32,
                truncation=True
            ).to(device)

            input_ids = tokens['input_ids']
            outputs = model(input_ids, top_k=None)
            logits = outputs['logits']

            # CPUë¡œ ì´ë™í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
            dense_outputs.append(logits.cpu())

            # MLM accuracy (if [MASK] exists)
            mask_pos = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)
            if len(mask_pos[0]) > 0:
                mask_logits = logits[mask_pos[0][0], mask_pos[1][0]]
                pred_token = tokenizer.decode([mask_logits.argmax()])
                # Simple check (rough)
                dense_correct += 1

    print(f"Dense baseline: {len(test_texts)} samples")

    # ê° í¬ì†Œë„ ë ˆë²¨ í…ŒìŠ¤íŠ¸
    print(f"\n{'Sparsity':<15} {'MSE Loss':<12} {'Cosine Sim':<12} {'Norm %':<12} {'Quality':<10}")
    print("-" * 70)

    for top_k, label in sparsity_levels:
        mse_losses = []
        cos_sims = []
        norm_ratios = []

        with torch.no_grad():
            for i, text in enumerate(test_texts):
                tokens = tokenizer(
                    text,
                    return_tensors='pt',
                    padding='max_length',
                    max_length=32,
                    truncation=True
                ).to(device)

                outputs = model(tokens['input_ids'], top_k=top_k)
                logits_sparse = outputs['logits']
                logits_dense = dense_outputs[i].to(device)  # GPUë¡œ ì´ë™

                # Metrics
                mse = F.mse_loss(logits_sparse, logits_dense).item()

                flat_sparse = logits_sparse.flatten()
                flat_dense = logits_dense.flatten()
                cos_sim = F.cosine_similarity(
                    flat_sparse.unsqueeze(0),
                    flat_dense.unsqueeze(0)
                ).item()

                norm_ratio = (logits_sparse.norm() / logits_dense.norm()).item()

                mse_losses.append(mse)
                cos_sims.append(cos_sim)
                norm_ratios.append(norm_ratio)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if device == 'cuda':
                torch.cuda.empty_cache()

        avg_mse = np.mean(mse_losses)
        avg_cos = np.mean(cos_sims)
        avg_norm = np.mean(norm_ratios) * 100

        # Quality rating
        if avg_cos > 0.98 and avg_norm > 95:
            quality = "ğŸŒŸ Excellent"
        elif avg_cos > 0.95 and avg_norm > 90:
            quality = "âœ… Good"
        elif avg_cos > 0.90 and avg_norm > 80:
            quality = "âœ”ï¸  OK"
        else:
            quality = "âš ï¸  Poor"

        results[label] = {
            'top_k': top_k,
            'mse': avg_mse,
            'cosine_sim': avg_cos,
            'norm_ratio': avg_norm,
            'quality': quality
        }

        print(f"{label:<15} {avg_mse:<12.6f} {avg_cos:<12.6f} {avg_norm:<12.1f} {quality:<10}")

    # ì¶”ì²œ í¬ì†Œë„
    print("\nğŸ¯ Recommended Sparsity:")
    for label, stats in results.items():
        if "Good" in stats['quality'] or "Excellent" in stats['quality']:
            sparsity_pct = (stats['top_k'] / d_ff * 100) if stats['top_k'] else 100
            print(f"  â€¢ {label}: Quality={stats['quality']}, Cosine={stats['cosine_sim']:.4f}")
            if stats['top_k'] and sparsity_pct < 60:
                print(f"    â†’ Can use {sparsity_pct:.0f}% sparsity without significant degradation!")
                break

    return results


# ============================================================
# 3. Router Quality Analysis
# ============================================================

def analyze_router_quality(
    model: nn.Module,
    tokenizer,
    device: str,
    num_samples: int = 100,
    top_k: int = 768
) -> Dict:
    """
    ë¼ìš°í„° í’ˆì§ˆ ë¶„ì„: Learned vs Random vs Oracle
    """
    print("\n" + "="*70)
    print("3ï¸âƒ£  ROUTER QUALITY ANALYSIS")
    print("="*70)

    model.eval()

    test_texts = [
        "The quick brown fox jumps over the lazy dog",
        "Machine learning is a subset of artificial intelligence",
        "Python programming language is widely used in data science",
    ] * 34
    test_texts = test_texts[:num_samples]

    layer_idx = 0  # Analyze first layer
    ffn = model.layers[layer_idx].ffn
    d_ff = ffn.d_ff

    print(f"\nAnalyzing Layer {layer_idx} with {num_samples} samples")
    print(f"Sparsity: top_k={top_k} ({top_k/d_ff*100:.1f}%)\n")

    results = {}

    # 1. Learned Router
    print("Testing Learned Router...")
    learned_mse = []

    with torch.no_grad():
        for text in test_texts:
            tokens = tokenizer(
                text,
                return_tensors='pt',
                padding='max_length',
                max_length=32,
                truncation=True
            )['input_ids'].to(device)

            batch, seq = tokens.shape
            token_emb = model.token_embedding(tokens)
            positions = torch.arange(seq, device=device).unsqueeze(0).expand(batch, -1)
            pos_emb = model.position_embedding(positions)
            x = token_emb + pos_emb  # [batch, seq, d_model]

            # Dense output (ground truth)
            out_dense = ffn(x, top_k=None)

            # Learned router output
            out_learned = ffn(x, top_k=top_k)

            mse = F.mse_loss(out_learned, out_dense).item()
            learned_mse.append(mse)

    results['learned'] = {
        'mse': np.mean(learned_mse),
        'std': np.std(learned_mse)
    }

    # 2. Random Router
    print("Testing Random Router...")
    random_mse = []

    # ì„ì‹œë¡œ ëœë¤ ë¼ìš°í„° ìƒì„±
    if ffn.router.use_mlp:
        original_w1 = ffn.router.W_router_1.data.clone()
        original_w2 = ffn.router.W_router_2.data.clone()
        ffn.router.W_router_1.data = torch.randn_like(ffn.router.W_router_1.data) * 0.02
        ffn.router.W_router_2.data = torch.randn_like(ffn.router.W_router_2.data) * 0.02
    else:
        original_router = ffn.router.W_router.data.clone()
        ffn.router.W_router.data = torch.randn_like(ffn.router.W_router.data) * 0.02

    with torch.no_grad():
        for text in test_texts:
            tokens = tokenizer(
                text,
                return_tensors='pt',
                padding='max_length',
                max_length=32,
                truncation=True
            )['input_ids'].to(device)

            batch, seq = tokens.shape
            token_emb = model.token_embedding(tokens)
            positions = torch.arange(seq, device=device).unsqueeze(0).expand(batch, -1)
            pos_emb = model.position_embedding(positions)
            x = token_emb + pos_emb

            out_dense = ffn(x, top_k=None)
            out_random = ffn(x, top_k=top_k)

            mse = F.mse_loss(out_random, out_dense).item()
            random_mse.append(mse)

    results['random'] = {
        'mse': np.mean(random_mse),
        'std': np.std(random_mse)
    }

    # Restore original router
    if ffn.router.use_mlp:
        ffn.router.W_router_1.data = original_w1
        ffn.router.W_router_2.data = original_w2
    else:
        ffn.router.W_router.data = original_router

    # 3. Oracle Router (ìµœì  ì„ íƒ)
    print("Testing Oracle Router...")
    oracle_mse = []

    with torch.no_grad():
        for text in test_texts:
            tokens = tokenizer(
                text,
                return_tensors='pt',
                padding='max_length',
                max_length=32,
                truncation=True
            )['input_ids'].to(device)

            batch, seq = tokens.shape
            token_emb = model.token_embedding(tokens)
            positions = torch.arange(seq, device=device).unsqueeze(0).expand(batch, -1)
            pos_emb = model.position_embedding(positions)
            x = token_emb + pos_emb
            x_flat = x.view(-1, x.shape[-1])

            # Dense ê³„ì‚°
            z_dense = x_flat @ ffn.W1.T  # [batch*seq, d_ff]
            a_dense = F.gelu(z_dense)
            out_dense = (a_dense @ ffn.W2.T).view(batch, seq, -1)

            # Oracle: ì‹¤ì œ activation í¬ê¸°ë¡œ top-k ì„ íƒ
            activation_scores = a_dense.abs()  # ì‹¤ì œ activation í¬ê¸°
            _, top_indices = torch.topk(activation_scores, top_k, dim=-1)

            mask = torch.zeros_like(z_dense)
            mask.scatter_(-1, top_indices, 1.0)

            z_oracle = z_dense * mask
            a_oracle = F.gelu(z_oracle)
            out_oracle = (a_oracle @ ffn.W2.T).view(batch, seq, -1)

            mse = F.mse_loss(out_oracle, out_dense).item()
            oracle_mse.append(mse)

    results['oracle'] = {
        'mse': np.mean(oracle_mse),
        'std': np.std(oracle_mse)
    }

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'Router Type':<15} {'MSE Loss':<15} {'Std Dev':<15} {'Quality':<15}")
    print("-" * 65)

    for router_type in ['oracle', 'learned', 'random']:
        mse = results[router_type]['mse']
        std = results[router_type]['std']

        if router_type == 'oracle':
            quality = "ğŸŒŸ Upper bound"
        elif router_type == 'learned':
            # Compare to oracle
            gap = (mse - results['oracle']['mse']) / results['oracle']['mse']
            if gap < 0.1:
                quality = "âœ… Excellent"
            elif gap < 0.3:
                quality = "âœ”ï¸  Good"
            else:
                quality = "âš ï¸  Needs training"
        else:  # random
            quality = "âŒ Baseline"

        results[router_type]['quality'] = quality
        print(f"{router_type.capitalize():<15} {mse:<15.6f} {std:<15.6f} {quality:<15}")

    # í•™ìŠµ íš¨ê³¼
    improvement = (results['random']['mse'] - results['learned']['mse']) / results['random']['mse'] * 100
    gap_to_oracle = (results['learned']['mse'] - results['oracle']['mse']) / results['oracle']['mse'] * 100

    print(f"\nğŸ“ˆ Router Learning Effect:")
    print(f"  â€¢ Improvement over random: {improvement:.1f}%")
    print(f"  â€¢ Gap to oracle: {gap_to_oracle:.1f}%")

    if improvement > 30 and gap_to_oracle < 20:
        print(f"  âœ… Router is well-trained!")
    elif improvement > 10:
        print(f"  âœ”ï¸  Router learned something useful")
    else:
        print(f"  âš ï¸  Router needs more training")

    return results


# ============================================================
# 4. Neuron Specialization Analysis
# ============================================================

def analyze_neuron_specialization(
    model: nn.Module,
    tokenizer,
    device: str,
    layer_idx: int = 0
) -> Dict:
    """
    ë‰´ëŸ° íŠ¹í™”ë„ ë¶„ì„: ê° ë‰´ëŸ°ì´ ì–´ë–¤ ì…ë ¥ì— ë°˜ì‘í•˜ëŠ”ì§€
    """
    print("\n" + "="*70)
    print("4ï¸âƒ£  NEURON SPECIALIZATION ANALYSIS")
    print("="*70)

    model.eval()

    # ì¹´í…Œê³ ë¦¬ë³„ í…ŒìŠ¤íŠ¸ ë¬¸ì¥
    test_groups = {
        "Science": [
            "Physics studies matter and energy in the universe",
            "Chemistry explores molecular structures and reactions",
            "Biology examines living organisms and ecosystems",
        ],
        "Technology": [
            "Computers process information using binary code",
            "Software development requires programming skills",
            "Artificial intelligence mimics human cognition",
        ],
        "Arts": [
            "Painting expresses emotions through visual art",
            "Music creates harmony using different instruments",
            "Literature tells stories through written words",
        ],
        "Numbers": [
            "One plus one equals two in mathematics",
            "The year two thousand and twenty four",
            "Three hundred and sixty degrees in a circle",
        ],
    }

    ffn = model.layers[layer_idx].ffn
    d_ff = ffn.d_ff

    print(f"\nAnalyzing Layer {layer_idx} with {len(test_groups)} categories")

    # ê° ì¹´í…Œê³ ë¦¬ë³„ ë‰´ëŸ° í™œì„±í™” íŒ¨í„´
    category_activations = {}

    with torch.no_grad():
        for category, texts in test_groups.items():
            activations = []

            for text in texts:
                tokens = tokenizer(
                    text,
                    return_tensors='pt',
                    padding='max_length',
                    max_length=32,
                    truncation=True
                )['input_ids'].to(device)

                batch, seq = tokens.shape
                token_emb = model.token_embedding(tokens)
                positions = torch.arange(seq, device=device).unsqueeze(0).expand(batch, -1)
                pos_emb = model.position_embedding(positions)
                x = token_emb + pos_emb
                x_flat = x.view(-1, x.shape[-1])

                # FFN activation
                z = x_flat @ ffn.W1.T  # [batch*seq, d_ff]
                a = F.gelu(z)

                # Average activation per neuron
                avg_activation = a.mean(dim=0)  # [d_ff]
                activations.append(avg_activation)

            # Category average
            category_activations[category] = torch.stack(activations).mean(dim=0)

    # ê° ë‰´ëŸ°ì˜ íŠ¹í™”ë„ ê³„ì‚°
    print(f"\n{'Category':<15} {'Specialized':<15} {'Top Neurons':<30}")
    print("-" * 65)

    results = {}
    all_activations = torch.stack(list(category_activations.values()))  # [n_categories, d_ff]

    for cat_idx, (category, activation) in enumerate(category_activations.items()):
        # ì´ ì¹´í…Œê³ ë¦¬ì—ì„œë§Œ ê°•í•˜ê²Œ í™œì„±í™”ë˜ëŠ” ë‰´ëŸ°
        # Specialization = activation_this / activation_others
        other_activations = all_activations[[i for i in range(len(all_activations)) if i != cat_idx]]
        max_other = other_activations.max(dim=0)[0]

        specialization = activation / (max_other + 1e-8)
        specialized_neurons = (specialization > 1.5).sum().item()

        # Top 5 specialized neurons
        top_5_spec, top_5_idx = torch.topk(specialization, 5)

        results[category] = {
            'specialized_count': specialized_neurons,
            'top_neurons': top_5_idx.tolist(),
            'specialization_scores': top_5_spec.tolist()
        }

        print(f"{category:<15} {specialized_neurons:<15} {top_5_idx.tolist()}")

    # ê³µí†µ ë‰´ëŸ° vs íŠ¹í™” ë‰´ëŸ°
    all_top_neurons = set()
    for res in results.values():
        all_top_neurons.update(res['top_neurons'])

    print(f"\nğŸ“Š Specialization Summary:")
    print(f"  â€¢ Total unique specialized neurons: {len(all_top_neurons)}")
    print(f"  â€¢ Average per category: {np.mean([r['specialized_count'] for r in results.values()]):.1f}")

    if len(all_top_neurons) > d_ff * 0.3:
        print(f"  âœ… Good diversity: neurons are specialized for different inputs")
    else:
        print(f"  âš ï¸  Low diversity: many neurons overlap across categories")

    return results


# ============================================================
# 5. Layer-wise Differences
# ============================================================

def analyze_layer_differences(
    model: nn.Module,
    tokenizer,
    device: str,
    num_samples: int = 100,
    top_k: int = 768
) -> Dict:
    """
    ë ˆì´ì–´ë³„ ì°¨ì´ ë¶„ì„
    """
    print("\n" + "="*70)
    print("5ï¸âƒ£  LAYER-WISE DIFFERENCES ANALYSIS")
    print("="*70)

    model.eval()

    test_texts = [
        "The quick brown fox jumps over the lazy dog",
        "Machine learning is transforming artificial intelligence",
    ] * 50
    test_texts = test_texts[:num_samples]

    n_layers = len(model.layers)
    d_ff = model.layers[0].ffn.d_ff

    print(f"\nAnalyzing {n_layers} layers with {num_samples} samples")

    # ê° ë ˆì´ì–´ë³„ í†µê³„
    layer_stats = {}

    with torch.no_grad():
        for text in test_texts:
            tokens = tokenizer(
                text,
                return_tensors='pt',
                padding='max_length',
                max_length=32,
                truncation=True
            )['input_ids'].to(device)

            batch, seq = tokens.shape
            token_emb = model.token_embedding(tokens)
            positions = torch.arange(seq, device=device).unsqueeze(0).expand(batch, -1)
            pos_emb = model.position_embedding(positions)
            x = token_emb + pos_emb

            for layer_idx, layer in enumerate(model.layers):
                if layer_idx not in layer_stats:
                    layer_stats[layer_idx] = {
                        'router_entropy': [],
                        'activation_sparsity': [],
                        'output_norm': []
                    }

                # Attention
                x_norm = layer.norm1(x)
                attn_out, _ = layer.attention(x_norm, x_norm, x_norm)
                x = x + layer.dropout(attn_out)

                # FFN
                x_norm = layer.norm2(x)
                x_flat = x_norm.view(-1, x_norm.shape[-1])

                # Router scores
                router_scores = layer.ffn.router.compute_scores(x_flat)
                router_probs = F.softmax(router_scores, dim=-1)

                # Entropy (ë¶„ì‚°ë„)
                entropy = -(router_probs * (router_probs + 1e-8).log()).sum(dim=-1).mean().item()
                layer_stats[layer_idx]['router_entropy'].append(entropy)

                # Activation sparsity (ì‹¤ì œ 0ì¸ ë¹„ìœ¨)
                z = x_flat @ layer.ffn.W1.T
                a = F.gelu(z)
                sparsity = (a.abs() < 0.01).float().mean().item()
                layer_stats[layer_idx]['activation_sparsity'].append(sparsity)

                # Output norm
                x_ffn = layer.ffn(x_norm, top_k=top_k)
                norm = x_ffn.norm().item()
                layer_stats[layer_idx]['output_norm'].append(norm)

                x = x + layer.dropout(x_ffn)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'Layer':<8} {'Entropy':<12} {'Act Sparsity':<15} {'Output Norm':<15} {'Pattern':<15}")
    print("-" * 75)

    results = {}
    for layer_idx in range(n_layers):
        entropy = np.mean(layer_stats[layer_idx]['router_entropy'])
        sparsity = np.mean(layer_stats[layer_idx]['activation_sparsity'])
        norm = np.mean(layer_stats[layer_idx]['output_norm'])

        # Pattern classification
        if layer_idx < n_layers // 3:
            pattern = "ğŸ”µ Early (syntax)"
        elif layer_idx < 2 * n_layers // 3:
            pattern = "ğŸŸ¢ Middle (semantic)"
        else:
            pattern = "ğŸ”´ Late (abstract)"

        results[layer_idx] = {
            'entropy': entropy,
            'sparsity': sparsity,
            'norm': norm,
            'pattern': pattern
        }

        print(f"Layer {layer_idx:<2} {entropy:<12.4f} {sparsity:<15.4f} {norm:<15.2f} {pattern:<15}")

    # ë ˆì´ì–´ ê°„ ìœ ì‚¬ë„
    print(f"\nğŸ”— Layer Similarity (Router Weight Cosine):")
    for i in range(n_layers - 1):
        router_i = model.layers[i].ffn.router
        router_j = model.layers[i + 1].ffn.router

        # Get weights to compare (use final projection for MLP router)
        if router_i.use_mlp:
            w_i = router_i.W_router_2.data
            w_j = router_j.W_router_2.data
        else:
            w_i = router_i.W_router.data
            w_j = router_j.W_router.data

        # Flatten and compute cosine
        sim = F.cosine_similarity(w_i.flatten().unsqueeze(0), w_j.flatten().unsqueeze(0)).item()
        print(f"  Layer {i} â†” {i+1}: {sim:.4f}")

    return results


# ============================================================
# 6. Dynamic Routing Effects
# ============================================================

def analyze_dynamic_routing(
    model: nn.Module,
    tokenizer,
    device: str,
    top_k: int = 768
) -> Dict:
    """
    ë™ì  ë¼ìš°íŒ… íš¨ê³¼ ë¶„ì„: ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ë³€í™”
    """
    print("\n" + "="*70)
    print("6ï¸âƒ£  DYNAMIC ROUTING EFFECTS ANALYSIS")
    print("="*70)

    model.eval()

    # ê°™ì€ ë‹¨ì–´, ë‹¤ë¥¸ ì»¨í…ìŠ¤íŠ¸
    test_pairs = [
        {
            "word": "bank",
            "context1": "I went to the bank to deposit money",
            "context2": "We sat by the river bank enjoying nature"
        },
        {
            "word": "light",
            "context1": "The light from the sun is bright",
            "context2": "This bag is very light to carry"
        },
        {
            "word": "play",
            "context1": "Children love to play in the park",
            "context2": "We watched a play at the theater"
        },
    ]

    layer_idx = 0
    ffn = model.layers[layer_idx].ffn

    print(f"\nAnalyzing Layer {layer_idx} routing for ambiguous words\n")

    results = {}

    for pair in test_pairs:
        word = pair['word']
        context1 = pair['context1']
        context2 = pair['context2']

        # Tokenize both contexts
        tokens1 = tokenizer(
            context1,
            return_tensors='pt',
            padding='max_length',
            max_length=32,
            truncation=True
        )['input_ids'].to(device)

        tokens2 = tokenizer(
            context2,
            return_tensors='pt',
            padding='max_length',
            max_length=32,
            truncation=True
        )['input_ids'].to(device)

        with torch.no_grad():
            # Context 1
            batch, seq = tokens1.shape
            token_emb1 = model.token_embedding(tokens1)
            positions = torch.arange(seq, device=device).unsqueeze(0).expand(batch, -1)
            pos_emb = model.position_embedding(positions)
            x1 = token_emb1 + pos_emb
            x1_flat = x1.view(-1, x1.shape[-1])

            scores1 = ffn.router.compute_scores(x1_flat)
            _, top_indices1 = torch.topk(scores1, top_k, dim=-1)

            # Context 2
            token_emb2 = model.token_embedding(tokens2)
            x2 = token_emb2 + pos_emb
            x2_flat = x2.view(-1, x2.shape[-1])

            scores2 = ffn.router.compute_scores(x2_flat)
            _, top_indices2 = torch.topk(scores2, top_k, dim=-1)

            # ì„ íƒëœ ë‰´ëŸ° ì§‘í•©
            set1 = set(top_indices1.flatten().tolist())
            set2 = set(top_indices2.flatten().tolist())

            overlap = len(set1 & set2)
            union = len(set1 | set2)
            jaccard = overlap / union if union > 0 else 0

            unique1 = len(set1 - set2)
            unique2 = len(set2 - set1)

        results[word] = {
            'context1': context1,
            'context2': context2,
            'overlap': overlap,
            'jaccard': jaccard,
            'unique1': unique1,
            'unique2': unique2
        }

        print(f"Word: '{word}'")
        print(f"  Context 1: {context1}")
        print(f"  Context 2: {context2}")
        print(f"  Overlap: {overlap} neurons ({jaccard:.2%} Jaccard)")
        print(f"  Unique to context 1: {unique1}")
        print(f"  Unique to context 2: {unique2}")
        print()

    # í‰ê°€
    avg_jaccard = np.mean([r['jaccard'] for r in results.values()])

    print(f"ğŸ“Š Dynamic Routing Evaluation:")
    print(f"  Average Jaccard similarity: {avg_jaccard:.2%}")

    if avg_jaccard < 0.7:
        print(f"  âœ… Strong context sensitivity: routing adapts to context")
    elif avg_jaccard < 0.85:
        print(f"  âœ”ï¸  Moderate context sensitivity")
    else:
        print(f"  âš ï¸  Weak context sensitivity: routing is mostly static")

    return results


# ============================================================
# 7. Computation Efficiency
# ============================================================

def analyze_computation_efficiency(
    model: nn.Module,
    tokenizer,
    device: str,
    num_samples: int = 100
) -> Dict:
    """
    ê³„ì‚° íš¨ìœ¨ì„± ë¶„ì„: ì‹¤ì œ ì†ë„ í–¥ìƒ
    """
    print("\n" + "="*70)
    print("7ï¸âƒ£  COMPUTATION EFFICIENCY ANALYSIS")
    print("="*70)

    model.eval()

    # Test text
    test_text = "The quick brown fox jumps over the lazy dog"
    tokens = tokenizer(
        test_text,
        return_tensors='pt',
        padding='max_length',
        max_length=32,
        truncation=True
    )['input_ids'].to(device)

    d_ff = model.layers[0].ffn.d_ff

    # í…ŒìŠ¤íŠ¸í•  í¬ì†Œë„ ë ˆë²¨
    sparsity_levels = [
        (None, "Dense (100%)"),
        (int(0.50 * d_ff), "50%"),
        (int(0.25 * d_ff), "25%"),
        (int(0.10 * d_ff), "10%"),
    ]

    results = {}

    print(f"\nBenchmarking with {num_samples} iterations per config\n")
    print(f"{'Sparsity':<15} {'Time (ms)':<15} {'Speedup':<12} {'FLOPs %':<12}")
    print("-" * 60)

    baseline_time = None

    for top_k, label in sparsity_levels:
        # Warmup
        with torch.no_grad():
            for _ in range(10):
                _ = model(tokens, top_k=top_k)

        if device == 'cuda':
            torch.cuda.synchronize()

        # Benchmark
        start = time.time()
        with torch.no_grad():
            for _ in range(num_samples):
                _ = model(tokens, top_k=top_k)

        if device == 'cuda':
            torch.cuda.synchronize()

        elapsed = (time.time() - start) * 1000 / num_samples  # ms per sample

        if baseline_time is None:
            baseline_time = elapsed

        speedup = baseline_time / elapsed

        # FLOPs ì¶”ì •
        if top_k is None:
            flops_pct = 100.0
        else:
            flops_pct = top_k / d_ff * 100

        results[label] = {
            'top_k': top_k,
            'time_ms': elapsed,
            'speedup': speedup,
            'flops_pct': flops_pct
        }

        print(f"{label:<15} {elapsed:<15.2f} {speedup:<12.2f}x {flops_pct:<12.1f}%")

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (if CUDA)
    if device == 'cuda':
        print(f"\nğŸ’¾ GPU Memory Usage:")
        for top_k, label in sparsity_levels:
            torch.cuda.reset_peak_memory_stats()

            with torch.no_grad():
                _ = model(tokens, top_k=top_k)

            peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB
            results[label]['memory_mb'] = peak_mem
            print(f"  {label:<15}: {peak_mem:.1f} MB")

    # íš¨ìœ¨ì„± í‰ê°€
    print(f"\nâš¡ Efficiency Summary:")
    best_efficiency = None
    for label, stats in results.items():
        if stats['top_k'] is not None and stats['speedup'] > 1.0:
            efficiency = stats['speedup'] / (stats['flops_pct'] / 100)
            if best_efficiency is None or efficiency > best_efficiency:
                best_efficiency = efficiency
                print(f"  ğŸŒŸ Best: {label} - {stats['speedup']:.2f}x speedup at {stats['flops_pct']:.0f}% FLOPs")

    return results


# ============================================================
# Main
# ============================================================

def find_latest_checkpoint(checkpoint_dir: str) -> str:
    """Find the latest checkpoint in the directory"""
    import glob

    if not os.path.exists(checkpoint_dir):
        raise FileNotFoundError(f"Checkpoint directory not found: {checkpoint_dir}")

    # Look for checkpoint files
    pattern = os.path.join(checkpoint_dir, "checkpoint_*.pt")
    checkpoints = glob.glob(pattern)

    if not checkpoints:
        raise FileNotFoundError(f"No checkpoint files found in {checkpoint_dir}")

    # Sort by modification time (most recent first)
    checkpoints.sort(key=lambda x: os.path.getmtime(x), reverse=True)

    return checkpoints[0]


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint", type=str, default=None,
                       help="Path to checkpoint file (optional if --checkpoint_dir is provided)")
    parser.add_argument("--checkpoint_dir", type=str, default=None,
                       help="Directory containing checkpoints (will use latest)")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--analyses", type=str, default="all",
                       help="Comma-separated list of analyses (1-7) or 'all'")
    parser.add_argument("--num_samples", type=int, default=500,
                       help="Number of samples for analysis (default: 500, recommended: 500-1000)")
    parser.add_argument("--fast", action="store_true",
                       help="Fast mode: fewer samples (100)")
    args = parser.parse_args()

    # Fast mode override
    if args.fast:
        args.num_samples = 100
        print("ğŸš€ Fast mode enabled: using 100 samples")

    # Auto-detect checkpoint directory if not provided
    if args.checkpoint_dir is None and args.checkpoint is None:
        # Try Colab first
        if os.path.exists("/content/drive/MyDrive/sprout_neuron_checkpoints"):
            args.checkpoint_dir = "/content/drive/MyDrive/sprout_neuron_checkpoints"
            print(f"ğŸ” Auto-detected Colab checkpoint dir: {args.checkpoint_dir}")
        # Try local
        elif os.path.exists("./checkpoints/neuron_based"):
            args.checkpoint_dir = "./checkpoints/neuron_based"
            print(f"ğŸ” Auto-detected local checkpoint dir: {args.checkpoint_dir}")

    # Determine checkpoint path
    if args.checkpoint is None:
        if args.checkpoint_dir is None:
            raise ValueError("Either --checkpoint or --checkpoint_dir must be provided")

        # Find latest checkpoint in directory
        args.checkpoint = find_latest_checkpoint(args.checkpoint_dir)
        print(f"ğŸ“‚ Using latest checkpoint: {os.path.basename(args.checkpoint)}")

    print("="*70)
    print("ğŸ”¬ COMPREHENSIVE NEURON-BASED MODEL ANALYSIS")
    print("="*70)
    print(f"Checkpoint: {args.checkpoint}")
    print(f"Device: {args.device}")

    # Load checkpoint
    print("\nğŸ“‚ Loading checkpoint...")
    checkpoint = torch.load(args.checkpoint, map_location=args.device)
    model_args = checkpoint.get('args', {})

    # Create model
    model = NeuronBasedLanguageModel(
        vocab_size=model_args.get('vocab_size', 30522),
        d_model=model_args.get('d_model', 768),
        d_ff=model_args.get('d_ff', 3072),
        n_layers=model_args.get('n_layers', 12),
        n_heads=model_args.get('n_heads', 12),
        max_seq_len=model_args.get('max_seq_len', 128),
    ).to(args.device)

    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print(f"âœ“ Model loaded (Epoch: {checkpoint.get('epoch', '?')}, Step: {checkpoint.get('global_step', '?')})")

    # Tokenizer
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    # Load validation texts (same strategy as training script)
    print("\n" + "="*70)
    print("LOADING VALIDATION DATA")
    print("="*70)
    valid_texts = load_validation_texts(use_cache=True)
    print(f"âœ… Loaded {len(valid_texts)} validation texts")

    # Parse which analyses to run
    if args.analyses == "all":
        analyses_to_run = list(range(1, 8))
    else:
        analyses_to_run = [int(x) for x in args.analyses.split(",")]

    # Run analyses
    all_results = {}

    # ë¶„ì„ ìˆœì„œ: ê°€ë²¼ìš´ ê²ƒë¶€í„° ì‹¤í–‰ (ë¹ ë¥¸ í”¼ë“œë°±)
    # 7(íš¨ìœ¨ì„±) â†’ 6(ë™ì ) â†’ 4(íŠ¹í™”) â†’ 5(ë ˆì´ì–´) â†’ 3(ë¼ìš°í„°) â†’ 2(ì„±ëŠ¥) â†’ 1(ì‚¬ìš©íŒ¨í„´)
    execution_order = [7, 6, 4, 5, 3, 2, 1]

    # ì‹¤í–‰í•  ë¶„ì„ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    ordered_analyses = [a for a in execution_order if a in analyses_to_run]

    print(f"\nğŸ“Š Running {len(ordered_analyses)} analyses in optimized order: {ordered_analyses}")
    print(f"   Samples: {args.num_samples} (from {len(valid_texts)} available validation texts)")
    print()

    for analysis_id in ordered_analyses:
        if analysis_id == 7:
            all_results['efficiency'] = analyze_computation_efficiency(
                model, tokenizer, args.device, num_samples=min(args.num_samples, 100)
            )
        elif analysis_id == 6:
            all_results['dynamic_routing'] = analyze_dynamic_routing(
                model, tokenizer, args.device, top_k=768
            )
        elif analysis_id == 4:
            all_results['specialization'] = analyze_neuron_specialization(
                model, tokenizer, args.device, layer_idx=0
            )
        elif analysis_id == 5:
            all_results['layer_differences'] = analyze_layer_differences(
                model, tokenizer, args.device, num_samples=min(args.num_samples, 200), top_k=768
            )
        elif analysis_id == 3:
            all_results['router_quality'] = analyze_router_quality(
                model, tokenizer, args.device, num_samples=min(args.num_samples, 200), top_k=768
            )
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if args.device == 'cuda':
                torch.cuda.empty_cache()
        elif analysis_id == 2:
            all_results['performance_sparsity'] = analyze_performance_vs_sparsity(
                model, tokenizer, args.device
            )
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if args.device == 'cuda':
                torch.cuda.empty_cache()
        elif analysis_id == 1:
            all_results['usage_patterns'] = analyze_neuron_usage_patterns(
                model, tokenizer, args.device, num_samples=args.num_samples, top_k=768,
                test_texts=valid_texts  # Use real validation data!
            )
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if args.device == 'cuda':
                torch.cuda.empty_cache()

    print("\n" + "="*70)
    print("âœ… COMPREHENSIVE ANALYSIS COMPLETE!")
    print("="*70)
    print(f"\nAnalyzed {len(analyses_to_run)} categories")
    print(f"Results saved in memory (extend script to save to file if needed)")


if __name__ == "__main__":
    main()
