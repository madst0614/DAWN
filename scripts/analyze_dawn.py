"""
DAWN Checkpoint Comprehensive Analysis
Dynamic Neuron Transformer ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ë¶„ì„ í•­ëª©:
1. ë‰´ëŸ° í™œì„±í™” íŒ¨í„´ (ì‚¬ìš© ë¹ˆë„, Gini coefficient, Entropy)
2. íŒ¨í„´ í™œì„±í™” ë¶„ì„ (ë‹¤ì–‘ì„±, ì‚¬ìš©ë¥ )
3. í† í°-ë‰´ëŸ° ì „ë¬¸í™” (íŠ¹ì • í† í°ì´ íŠ¹ì • ë‰´ëŸ° ì„ íƒ?)
4. Layerë³„ ì°¨ì´ (KL divergence, Cosine similarity)
5. ì •í™•ë„-ë¶ˆí™•ì‹¤ì„± ê´€ê³„
6. ì¢…í•© ì‹œê°í™” ë° ë¦¬í¬íŠ¸

Usage:
    python scripts/analyze_dawn.py --checkpoint /path/to/checkpoint_folder
"""

import sys
import os
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import argparse
from tqdm import tqdm
import json
from collections import defaultdict, Counter
from scipy.stats import entropy
import yaml

from models.model import DAWN
from utils.data import load_data, apply_mlm_masking, MLM_CONFIG
from transformers import BertTokenizer


# ============================================================
# ë°ì´í„° ìˆ˜ì§‘
# ============================================================

class ActivationCollector:
    """ë‰´ëŸ° ì„ íƒ íŒ¨í„´ ìˆ˜ì§‘ (v5.0: neuron-only)"""
    def __init__(self, model, n_layers):
        self.model = model
        self.n_layers = n_layers

        # ë‰´ëŸ° ì„ íƒ ê¸°ë¡
        self.neuron_selections = [[] for _ in range(n_layers)]

        # ìœ„ì¹˜ë³„ ë‰´ëŸ° ì„ íƒ
        self.position_neuron_map = defaultdict(lambda: [[] for _ in range(n_layers)])

        # í† í°ë³„ ë‰´ëŸ° ì„ íƒ
        self.token_neuron_map = defaultdict(lambda: [[] for _ in range(n_layers)])

        # ì˜ˆì¸¡ ì •í™•ë„ë³„ íŒ¨í„´
        self.correct_selections = [[] for _ in range(n_layers)]
        self.incorrect_selections = [[] for _ in range(n_layers)]

    def collect(self, input_ids, labels, logits, all_selected):
        """í•œ ë°°ì¹˜ì˜ ì„ íƒ íŒ¨í„´ ìˆ˜ì§‘ (ìµœì í™”)"""
        B, S = input_ids.shape

        # ì˜ˆì¸¡ ì •í™•ë„
        predictions = logits.argmax(dim=-1)  # [B, S]
        correct_mask = (predictions == labels) & (labels != -100)  # [B, S]

        # CPUë¡œ í•œ ë²ˆì— ì´ë™ (ë§¤ ë£¨í”„ë§ˆë‹¤ í•˜ì§€ ì•Šê³ )
        input_ids_cpu = input_ids.cpu()

        for layer_idx, selected_idx in enumerate(all_selected):
            # selected_idx: [B, S, k]

            # CPUë¡œ í•œ ë²ˆì— ì´ë™
            selected_cpu = selected_idx.cpu()

            # 1. ì „ì²´ ë‰´ëŸ° ì„ íƒ ê¸°ë¡
            self.neuron_selections[layer_idx].append(selected_cpu)

            # 2. í† í°ë³„ + ìœ„ì¹˜ë³„ ë‰´ëŸ° ì„ íƒ (vectorized)
            # Flatten: [B, S, k] â†’ [B*S, k]
            selected_flat = selected_cpu.reshape(-1, selected_cpu.shape[-1])  # [B*S, k]
            tokens_flat = input_ids_cpu.reshape(-1)  # [B*S]

            # í† í°ë³„: ê° unique tokenì— ëŒ€í•´ ë‰´ëŸ° ìˆ˜ì§‘
            for token_id in tokens_flat.unique().tolist():
                mask = tokens_flat == token_id
                neurons = selected_flat[mask].reshape(-1).tolist()
                self.token_neuron_map[token_id][layer_idx].extend(neurons)

            # ìœ„ì¹˜ë³„: ê° ìœ„ì¹˜ì— ëŒ€í•´ ë‰´ëŸ° ìˆ˜ì§‘
            for s in range(S):
                # ëª¨ë“  ë°°ì¹˜ì˜ së²ˆì§¸ ìœ„ì¹˜
                neurons = selected_cpu[:, s, :].reshape(-1).tolist()
                self.position_neuron_map[s][layer_idx].extend(neurons)

            # 3. ì •í™•ë„ë³„ ë‰´ëŸ° ì„ íƒ
            correct_neurons = selected_cpu[correct_mask.cpu()]
            incorrect_neurons = selected_cpu[~correct_mask.cpu()]

            if len(correct_neurons) > 0:
                self.correct_selections[layer_idx].append(correct_neurons)
            if len(incorrect_neurons) > 0:
                self.incorrect_selections[layer_idx].append(incorrect_neurons)

    def finalize(self):
        """ìˆ˜ì§‘ ì™„ë£Œ í›„ í…ì„œ ë³‘í•©"""
        for layer_idx in range(self.n_layers):
            if self.neuron_selections[layer_idx]:
                self.neuron_selections[layer_idx] = torch.cat(
                    self.neuron_selections[layer_idx], dim=0
                )

            if self.correct_selections[layer_idx]:
                self.correct_selections[layer_idx] = torch.cat(
                    self.correct_selections[layer_idx], dim=0
                )

            if self.incorrect_selections[layer_idx]:
                self.incorrect_selections[layer_idx] = torch.cat(
                    self.incorrect_selections[layer_idx], dim=0
                )


# ============================================================
# 1. ë‰´ëŸ° ì‚¬ìš© ë¶„ì„
# ============================================================

def analyze_neuron_usage(collector, n_neurons, n_layers):
    """ë‰´ëŸ° ì‚¬ìš© ë¹ˆë„ ë° ë¶„í¬ ë¶„ì„"""
    print("\n" + "="*70)
    print("1. NEURON USAGE ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        selections = collector.neuron_selections[layer_idx]

        if len(selections) == 0:
            continue

        # ë‰´ëŸ°ë³„ ì„ íƒ ë¹ˆë„
        neuron_counts = torch.bincount(
            selections.flatten(),
            minlength=n_neurons
        ).numpy()

        total_selections = neuron_counts.sum()
        neuron_freq = neuron_counts / total_selections

        # Gini coefficient (ë¶ˆê· í˜• ì¸¡ì •)
        sorted_freq = np.sort(neuron_freq)
        n = len(sorted_freq)
        cumsum = np.cumsum(sorted_freq)
        gini = (2 * np.sum((np.arange(1, n+1)) * sorted_freq) - (n + 1) * cumsum[-1]) / (n * cumsum[-1])

        # ì‚¬ìš©ë¥ 
        used_neurons = (neuron_counts > 0).sum()
        usage_ratio = used_neurons / n_neurons

        # Top-k ì§‘ì¤‘ë„
        top_10_ratio = np.sort(neuron_freq)[-10:].sum()
        top_50_ratio = np.sort(neuron_freq)[-50:].sum()

        print(f"\nLayer {layer_idx}:")
        print(f"  Used neurons: {used_neurons}/{n_neurons} ({usage_ratio:.2%})")
        print(f"  Gini coefficient: {gini:.4f} (0=equal, 1=unequal)")
        print(f"  Entropy: {entropy(neuron_freq + 1e-10):.4f}")
        print(f"  Top-10 neurons: {top_10_ratio:.2%}")
        print(f"  Top-50 neurons: {top_50_ratio:.2%}")

        # âš ï¸ ê²½ê³  ì‹œìŠ¤í…œ
        warnings = []
        if usage_ratio < 0.2:
            warnings.append(f"âš ï¸  SPARSE: Only {usage_ratio:.1%} neurons used - potential bottleneck!")
        if gini > 0.8:
            warnings.append(f"âš ï¸  UNEQUAL: Gini={gini:.2f} - heavily concentrated usage!")
        if top_10_ratio > 0.5:
            warnings.append(f"âš ï¸  DOMINATED: Top-10 neurons = {top_10_ratio:.1%} of usage!")

        for warning in warnings:
            print(f"  {warning}")

        results[f'layer_{layer_idx}'] = {
            'neuron_counts': neuron_counts.tolist(),
            'neuron_freq': neuron_freq.tolist(),
            'gini_coefficient': float(gini),
            'used_neurons': int(used_neurons),
            'total_neurons': int(n_neurons),
            'usage_ratio': float(usage_ratio),
            'top_10_ratio': float(top_10_ratio),
            'top_50_ratio': float(top_50_ratio),
            'entropy': float(entropy(neuron_freq + 1e-10)),
        }

    return results


# ============================================================
# 2. í† í°-ë‰´ëŸ° ì „ë¬¸í™” ë¶„ì„
# ============================================================

def analyze_token_neuron_specialization(collector, tokenizer, n_neurons, n_layers, top_k_tokens=50):
    """í† í°ë³„ ë‰´ëŸ° ì„ íƒ íŒ¨í„´ ë¶„ì„"""
    print("\n" + "="*70)
    print("2. TOKEN-NEURON SPECIALIZATION")
    print("="*70)

    results = {}

    # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í† í°
    all_tokens = list(collector.token_neuron_map.keys())
    token_counts = {
        token_id: sum(len(neurons) for neurons in collector.token_neuron_map[token_id])
        for token_id in all_tokens
    }
    top_tokens = sorted(token_counts.keys(), key=lambda x: token_counts[x], reverse=True)[:top_k_tokens]

    for layer_idx in range(n_layers):
        print(f"\nLayer {layer_idx} - Top 10 specialized tokens:")
        token_neuron_patterns = {}

        for token_id in top_tokens:
            neurons = collector.token_neuron_map[token_id][layer_idx]

            if len(neurons) == 0:
                continue

            neuron_counts = Counter(neurons)
            total = sum(neuron_counts.values())

            top_3 = neuron_counts.most_common(3)
            concentration = sum(count for _, count in top_3) / total if total > 0 else 0
            unique_neurons = len(neuron_counts)

            token_str = tokenizer.decode([token_id]).strip()

            token_neuron_patterns[token_str] = {
                'token_id': token_id,
                'total_occurrences': total,
                'unique_neurons': unique_neurons,
                'concentration': float(concentration),
                'top_3_neurons': [(int(n), int(c)) for n, c in top_3],
            }

        # ì§‘ì¤‘ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_tokens = sorted(
            token_neuron_patterns.items(),
            key=lambda x: x[1]['concentration'],
            reverse=True
        )[:10]

        for token_str, data in sorted_tokens:
            print(f"  '{token_str}': concentration={data['concentration']:.2%}, "
                  f"unique={data['unique_neurons']}, top_3={data['top_3_neurons'][:2]}")

        results[f'layer_{layer_idx}'] = token_neuron_patterns

    return results


# ============================================================
# 3. Layerë³„ ì°¨ì´ ë¶„ì„
# ============================================================

def analyze_layer_differences(neuron_usage_results):
    """Layerë³„ ë‰´ëŸ° ì‚¬ìš© íŒ¨í„´ ì°¨ì´"""
    print("\n" + "="*70)
    print("3. LAYER DIFFERENCES")
    print("="*70)

    results = {}
    layers = sorted([k for k in neuron_usage_results.keys() if k.startswith('layer_')])

    for i, layer_i in enumerate(layers):
        for j, layer_j in enumerate(layers):
            if i >= j:
                continue

            freq_i = np.array(neuron_usage_results[layer_i]['neuron_freq'])
            freq_j = np.array(neuron_usage_results[layer_j]['neuron_freq'])

            kl_div = entropy(freq_i + 1e-10, freq_j + 1e-10)
            cos_sim = np.dot(freq_i, freq_j) / (np.linalg.norm(freq_i) * np.linalg.norm(freq_j))

            print(f"\n{layer_i} vs {layer_j}:")
            print(f"  KL Divergence: {kl_div:.4f} (higher = more different)")
            print(f"  Cosine Similarity: {cos_sim:.4f} (1=identical)")

            results[f'{layer_i}_vs_{layer_j}'] = {
                'kl_divergence': float(kl_div),
                'cosine_similarity': float(cos_sim),
            }

    return results


# ============================================================
# 4. ì •í™•ë„-ë¶ˆí™•ì‹¤ì„± ë¶„ì„
# ============================================================

def analyze_uncertainty_accuracy(collector, n_layers):
    """ì •ë‹µ/ì˜¤ë‹µ ì‹œ ë‰´ëŸ° ì„ íƒ íŒ¨í„´ ë¹„êµ"""
    print("\n" + "="*70)
    print("4. ACCURACY-UNCERTAINTY RELATIONSHIP")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        correct_sel = collector.correct_selections[layer_idx]
        incorrect_sel = collector.incorrect_selections[layer_idx]

        if len(correct_sel) == 0 or len(incorrect_sel) == 0:
            continue

        correct_unique = len(torch.unique(correct_sel))
        incorrect_unique = len(torch.unique(incorrect_sel))

        # ì¤‘ë³µ ë‰´ëŸ° (ì •ë‹µê³¼ ì˜¤ë‹µ ëª¨ë‘ ì‚¬ìš©)
        correct_neurons_set = set(torch.unique(correct_sel).tolist())
        incorrect_neurons_set = set(torch.unique(incorrect_sel).tolist())
        overlap = correct_neurons_set & incorrect_neurons_set
        overlap_ratio = len(overlap) / len(correct_neurons_set | incorrect_neurons_set)

        # ë‰´ëŸ° ë‹¤ì–‘ì„± (í‰ê·  ìœ ë‹ˆí¬ ë‰´ëŸ° ìˆ˜)
        correct_diversity = correct_unique / len(correct_sel) if len(correct_sel) > 0 else 0
        incorrect_diversity = incorrect_unique / len(incorrect_sel) if len(incorrect_sel) > 0 else 0

        print(f"\nLayer {layer_idx}:")
        print(f"  Correct: {len(correct_sel):,} samples, {correct_unique} unique neurons")
        print(f"  Incorrect: {len(incorrect_sel):,} samples, {incorrect_unique} unique neurons")
        print(f"  Overlap: {len(overlap)} neurons ({overlap_ratio:.2%})")
        print(f"  Diversity: Correct={correct_diversity:.4f}, Incorrect={incorrect_diversity:.4f}")

        # âš ï¸ ë¶ˆí™•ì‹¤ì„± ì‹ í˜¸ ì²´í¬
        if overlap_ratio > 0.9:
            print(f"  âš ï¸  WEAK SIGNAL: {overlap_ratio:.1%} overlap - can't distinguish correct/incorrect!")
        elif abs(correct_unique - incorrect_unique) / max(correct_unique, incorrect_unique) < 0.1:
            print(f"  âš ï¸  SIMILAR PATTERNS: Correct and incorrect use similar neurons!")

        results[f'layer_{layer_idx}'] = {
            'correct_samples': len(correct_sel),
            'incorrect_samples': len(incorrect_sel),
            'correct_unique_neurons': int(correct_unique),
            'incorrect_unique_neurons': int(incorrect_unique),
            'overlap_neurons': len(overlap),
            'overlap_ratio': float(overlap_ratio),
            'correct_diversity': float(correct_diversity),
            'incorrect_diversity': float(incorrect_diversity),
        }

    return results


# ============================================================
# 5. ì‹œê°í™”
# ============================================================

def visualize_results(neuron_usage_results, output_dir):
    """ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
    output_dir = Path(output_dir)
    n_layers = len([k for k in neuron_usage_results.keys() if k.startswith('layer_')])

    print("\n" + "="*70)
    print("5. GENERATING VISUALIZATIONS")
    print("="*70)

    # 1. ë‰´ëŸ° ì‚¬ìš© ë¶„í¬
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for layer_idx in range(min(n_layers, 4)):
        layer_key = f'layer_{layer_idx}'
        neuron_freq = neuron_usage_results[layer_key]['neuron_freq']

        ax = axes[layer_idx]
        ax.hist(neuron_freq, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
        ax.set_xlabel('Neuron Selection Frequency', fontsize=10)
        ax.set_ylabel('Count', fontsize=10)
        ax.set_title(f'Layer {layer_idx}\nGini: {neuron_usage_results[layer_key]["gini_coefficient"]:.3f}, '
                    f'Usage: {neuron_usage_results[layer_key]["usage_ratio"]:.2%}', fontsize=11)
        ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / 'neuron_usage_distribution.png', dpi=150, bbox_inches='tight')
    plt.show()  # Colabì—ì„œ ë°”ë¡œ í‘œì‹œ
    plt.close()
    print("  âœ“ neuron_usage_distribution.png")

    # 2. Layerë³„ í†µê³„
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    layers = list(range(n_layers))
    gini_coeffs = [neuron_usage_results[f'layer_{i}']['gini_coefficient'] for i in layers]
    usage_ratios = [neuron_usage_results[f'layer_{i}']['usage_ratio'] for i in layers]
    entropies = [neuron_usage_results[f'layer_{i}']['entropy'] for i in layers]

    axes[0].bar(layers, gini_coeffs, color='skyblue', edgecolor='black', width=0.6)
    axes[0].set_xlabel('Layer', fontsize=11)
    axes[0].set_ylabel('Gini Coefficient', fontsize=11)
    axes[0].set_title('Neuron Usage Inequality', fontsize=12, fontweight='bold')
    axes[0].grid(alpha=0.3, axis='y')

    axes[1].bar(layers, usage_ratios, color='lightcoral', edgecolor='black', width=0.6)
    axes[1].set_xlabel('Layer', fontsize=11)
    axes[1].set_ylabel('Usage Ratio', fontsize=11)
    axes[1].set_title('Fraction of Used Neurons', fontsize=12, fontweight='bold')
    axes[1].grid(alpha=0.3, axis='y')
    axes[1].set_ylim([0, 1])

    axes[2].bar(layers, entropies, color='lightgreen', edgecolor='black', width=0.6)
    axes[2].set_xlabel('Layer', fontsize=11)
    axes[2].set_ylabel('Entropy', fontsize=11)
    axes[2].set_title('Neuron Selection Diversity', fontsize=12, fontweight='bold')
    axes[2].grid(alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(output_dir / 'layer_statistics.png', dpi=150, bbox_inches='tight')
    plt.show()  # Colabì—ì„œ ë°”ë¡œ í‘œì‹œ
    plt.close()
    print("  âœ“ layer_statistics.png")

    # 3. Top-50 ë‰´ëŸ° íˆíŠ¸ë§µ
    fig, ax = plt.subplots(figsize=(12, 6))

    heatmap_data = []
    for layer_idx in range(n_layers):
        layer_key = f'layer_{layer_idx}'
        neuron_freq = np.array(neuron_usage_results[layer_key]['neuron_freq'])
        top_50_indices = np.argsort(neuron_freq)[-50:]
        top_50_freq = neuron_freq[top_50_indices]
        heatmap_data.append(top_50_freq)

    heatmap_data = np.array(heatmap_data)

    sns.heatmap(heatmap_data, cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Frequency'})
    ax.set_xlabel('Top-50 Neurons (sorted by frequency)', fontsize=11)
    ax.set_ylabel('Layer', fontsize=11)
    ax.set_title('Top-50 Most Active Neurons per Layer', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_dir / 'neuron_heatmap.png', dpi=150, bbox_inches='tight')
    plt.show()  # Colabì—ì„œ ë°”ë¡œ í‘œì‹œ
    plt.close()
    print("  âœ“ neuron_heatmap.png")


# ============================================================
# 6. ë¦¬í¬íŠ¸ ìƒì„±
# ============================================================

def generate_report(all_results, output_dir, checkpoint_path):
    """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
    output_dir = Path(output_dir)
    report_path = output_dir / 'analysis_report.txt'

    print("\n" + "="*70)
    print("6. GENERATING REPORT")
    print("="*70)

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("DAWN - Dynamic Neuron Transformer Analysis Report\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Checkpoint: {checkpoint_path}\n\n")

        # 1. ë‰´ëŸ° ì‚¬ìš©
        f.write("1. NEURON USAGE ANALYSIS\n")
        f.write("-" * 80 + "\n")

        neuron_results = all_results['neuron_usage']
        for layer_key in sorted(neuron_results.keys()):
            layer_data = neuron_results[layer_key]
            f.write(f"\n{layer_key.upper()}:\n")
            f.write(f"  Used: {layer_data['used_neurons']}/{layer_data['total_neurons']} "
                   f"({layer_data['usage_ratio']:.2%})\n")
            f.write(f"  Gini: {layer_data['gini_coefficient']:.4f}, "
                   f"Entropy: {layer_data['entropy']:.4f}\n")
            f.write(f"  Top-10: {layer_data['top_10_ratio']:.2%}, "
                   f"Top-50: {layer_data['top_50_ratio']:.2%}\n")

        # 2. Layer ì°¨ì´
        f.write("\n\n2. LAYER DIFFERENCES\n")
        f.write("-" * 80 + "\n")

        layer_diff = all_results['layer_differences']
        for pair_key in sorted(layer_diff.keys()):
            pair_data = layer_diff[pair_key]
            f.write(f"\n{pair_key}: KL={pair_data['kl_divergence']:.4f}, "
                   f"Cosine={pair_data['cosine_similarity']:.4f}\n")

        # 3. í† í°-ë‰´ëŸ° ì „ë¬¸í™” (ìƒ˜í”Œ)
        f.write("\n\n3. TOKEN-NEURON SPECIALIZATION (Top 5 per layer)\n")
        f.write("-" * 80 + "\n")

        token_spec = all_results['token_neuron_specialization']
        for layer_key in sorted(token_spec.keys()):
            f.write(f"\n{layer_key.upper()}:\n")
            layer_data = token_spec[layer_key]

            sorted_tokens = sorted(
                layer_data.items(),
                key=lambda x: x[1]['concentration'],
                reverse=True
            )[:5]

            for token_str, data in sorted_tokens:
                f.write(f"  '{token_str}': {data['concentration']:.2%} concentration, "
                       f"{data['unique_neurons']} unique, top_3={data['top_3_neurons']}\n")

        # 4. ì •í™•ë„-ë¶ˆí™•ì‹¤ì„±
        f.write("\n\n4. ACCURACY-UNCERTAINTY\n")
        f.write("-" * 80 + "\n")

        uncertainty = all_results['uncertainty_accuracy']
        for layer_key in sorted(uncertainty.keys()):
            layer_data = uncertainty[layer_key]
            f.write(f"\n{layer_key}:\n")
            f.write(f"  Correct: {layer_data['correct_unique_neurons']} neurons\n")
            f.write(f"  Incorrect: {layer_data['incorrect_unique_neurons']} neurons\n")
            f.write(f"  Overlap: {layer_data['overlap_neurons']} ({layer_data['overlap_ratio']:.2%})\n")
            f.write(f"  Diversity: Correct={layer_data['correct_diversity']:.4f}, "
                   f"Incorrect={layer_data['incorrect_diversity']:.4f}\n")

        # 5. ê¶Œì¥ì‚¬í•­
        f.write("\n\n5. RECOMMENDATIONS\n")
        f.write("-" * 80 + "\n\n")

        recommendations = []

        # ë‰´ëŸ° í¬ì†Œì„± ì²´í¬
        neuron_results = all_results['neuron_usage']
        for layer_key in sorted(neuron_results.keys()):
            layer_data = neuron_results[layer_key]
            if layer_data['usage_ratio'] < 0.2:
                recommendations.append(
                    f"âš ï¸  {layer_key}: Only {layer_data['usage_ratio']:.1%} neurons used\n"
                    f"   â†’ Consider increasing n_neurons (currently {layer_data['total_neurons']})\n"
                    f"   â†’ Or reducing k (top-k selection parameter)"
                )
            if layer_data['gini_coefficient'] > 0.8:
                recommendations.append(
                    f"âš ï¸  {layer_key}: High inequality (Gini={layer_data['gini_coefficient']:.2f})\n"
                    f"   â†’ Few neurons dominate - may need better initialization\n"
                    f"   â†’ Or add regularization to encourage uniform usage"
                )

        # ë¶ˆí™•ì‹¤ì„± ì‹ í˜¸ ì²´í¬
        for layer_key in sorted(uncertainty.keys()):
            layer_data = uncertainty[layer_key]
            if layer_data['overlap_ratio'] > 0.9:
                recommendations.append(
                    f"âš ï¸  {layer_key}: Weak uncertainty signal (overlap={layer_data['overlap_ratio']:.1%})\n"
                    f"   â†’ Model can't distinguish correct/incorrect predictions\n"
                    f"   â†’ May need more training or larger model capacity"
                )

        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                f.write(f"{i}. {rec}\n\n")
        else:
            f.write("âœ“ No critical issues detected!\n")

        f.write("\n" + "=" * 80 + "\n")

    print(f"  âœ“ analysis_report.txt")


# ============================================================
# Main
# ============================================================

# ============================================================
# ì¶”ê°€ ë¶„ì„: íŒ¨í„´, ìœ„ì¹˜
# ============================================================

def analyze_pattern_usage(collector, n_patterns, n_layers):
    """FFN íŒ¨í„´ ì‚¬ìš© ë¶„ì„ â­"""
    print("\n" + "="*70)
    print("PATTERN (FFN) USAGE ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.pattern_selections[layer_idx]) == 0:
            continue

        pattern_weights = collector.pattern_selections[layer_idx]  # [N, S, n_patterns]

        # Top-k íŒ¨í„´ ì„ íƒ (ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜)
        top_patterns = pattern_weights.argmax(dim=-1).numpy()  # [N, S]
        top_patterns = top_patterns.flatten()  # [N * S]
        pattern_counts = np.bincount(top_patterns, minlength=n_patterns)
        pattern_freq = pattern_counts / pattern_counts.sum()

        # Gini coefficient
        sorted_freq = np.sort(pattern_freq)
        n = len(sorted_freq)
        cumsum = np.cumsum(sorted_freq)
        gini = (2 * np.sum((np.arange(1, n+1)) * sorted_freq) - (n + 1) * cumsum[-1]) / (n * cumsum[-1])

        # ì‚¬ìš©ë¥ 
        used_patterns = (pattern_counts > 0).sum()
        usage_ratio = used_patterns / n_patterns

        # Top-k ì§‘ì¤‘ë„
        top_10_ratio = np.sort(pattern_freq)[-10:].sum()

        print(f"\nLayer {layer_idx}:")
        print(f"  Used patterns: {used_patterns}/{n_patterns} ({usage_ratio:.2%})")
        print(f"  Gini: {gini:.4f}, Entropy: {entropy(pattern_freq + 1e-10):.4f}")
        print(f"  Top-10 patterns: {top_10_ratio:.2%}")

        # ê²½ê³ 
        if usage_ratio < 0.2:
            print(f"  âš ï¸  SPARSE: Only {usage_ratio:.1%} patterns used!")
        if gini > 0.8:
            print(f"  âš ï¸  UNEQUAL: Pattern Gini={gini:.2f}!")

        results[f'layer_{layer_idx}'] = {
            'used_patterns': int(used_patterns),
            'total_patterns': int(n_patterns),
            'usage_ratio': float(usage_ratio),
            'gini_coefficient': float(gini),
            'entropy': float(entropy(pattern_freq + 1e-10)),
            'top_10_ratio': float(top_10_ratio),
        }

    return results


def analyze_pattern_collapse_detail(collector, n_patterns, n_layers):
    """íŒ¨í„´ collapse ìƒì„¸ ë¶„ì„ - ì–´ë–¤ íŒ¨í„´ë“¤ì´ ì§€ë°°í•˜ëŠ”ê°€?"""
    print("\n" + "="*70)
    print("PATTERN COLLAPSE DETAIL ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.pattern_selections[layer_idx]) == 0:
            continue

        pattern_weights = collector.pattern_selections[layer_idx]  # [N, S, n_patterns]
        top_patterns = pattern_weights.argmax(dim=-1).numpy().flatten()  # [N * S]

        # íŒ¨í„´ë³„ ì¹´ìš´íŠ¸
        pattern_counts = np.bincount(top_patterns, minlength=n_patterns)
        total_selections = pattern_counts.sum()

        # Top-10 íŒ¨í„´ ì°¾ê¸°
        top_indices = np.argsort(pattern_counts)[::-1][:10]
        top_counts = pattern_counts[top_indices]
        top_ratios = top_counts / total_selections

        print(f"\nLayer {layer_idx}:")
        print(f"  Total selections: {total_selections}")
        print(f"  Top-10 patterns:")
        for rank, (idx, count, ratio) in enumerate(zip(top_indices, top_counts, top_ratios), 1):
            print(f"    #{rank}: Pattern {idx:3d} - {count:7d} times ({ratio:6.2%})")

        # í‰ê·  gate ê°’ ë¶„ì„ (ìƒìœ„ 5ê°œ vs í•˜ìœ„ 5ê°œ)
        avg_weights = pattern_weights.mean(dim=(0, 1)).numpy()  # [n_patterns]
        top5_avg = avg_weights[top_indices[:5]].mean()
        bottom5_indices = np.argsort(pattern_counts)[:5]
        bottom5_avg = avg_weights[bottom5_indices].mean()

        print(f"  Average gate values:")
        print(f"    Top-5 patterns: {top5_avg:.6f}")
        print(f"    Bottom-5 patterns: {bottom5_avg:.6f}")
        print(f"    Ratio: {top5_avg / (bottom5_avg + 1e-10):.2f}x")

        # Collapse ê²½ê³ 
        top1_ratio = top_ratios[0]
        top5_ratio = top_ratios[:5].sum()
        if top1_ratio > 0.5:
            print(f"  ğŸ”´ SEVERE COLLAPSE: Top-1 pattern dominates {top1_ratio:.1%}!")
        elif top5_ratio > 0.8:
            print(f"  âš ï¸  COLLAPSE: Top-5 patterns dominate {top5_ratio:.1%}!")

        results[f'layer_{layer_idx}'] = {
            'top_10_patterns': top_indices.tolist(),
            'top_10_counts': top_counts.tolist(),
            'top_10_ratios': top_ratios.tolist(),
            'top5_avg_gate': float(top5_avg),
            'bottom5_avg_gate': float(bottom5_avg),
        }

    return results


def analyze_neuron_pattern_correlation(collector, n_layers, sample_size=1000):
    """ë‰´ëŸ° Set â†’ íŒ¨í„´ ë§¤í•‘ ì¼ê´€ì„± ë¶„ì„"""
    print("\n" + "="*70)
    print("NEURON-PATTERN CORRELATION ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.neuron_selections[layer_idx]) == 0:
            continue
        if len(collector.pattern_selections[layer_idx]) == 0:
            continue

        neuron_sel = collector.neuron_selections[layer_idx]  # [N, S, k]
        pattern_weights = collector.pattern_selections[layer_idx]  # [N, S, n_patterns]

        N, S, k = neuron_sel.shape
        total_samples = N * S

        # ìƒ˜í”Œë§ (ë„ˆë¬´ í¬ë©´)
        if total_samples > sample_size:
            indices = np.random.choice(total_samples, sample_size, replace=False)
        else:
            indices = np.arange(total_samples)

        # Flatten
        neuron_sel_flat = neuron_sel.reshape(-1, k).numpy()[indices]  # [sample, k]
        pattern_sel_flat = pattern_weights.argmax(dim=-1).reshape(-1).numpy()[indices]  # [sample]

        # ë‰´ëŸ° Set â†’ íŒ¨í„´ ë§¤í•‘ ë¹ˆë„
        neuron_pattern_map = defaultdict(lambda: defaultdict(int))

        for neuron_set, pattern in zip(neuron_sel_flat, pattern_sel_flat):
            neuron_key = tuple(sorted(neuron_set.tolist()))
            neuron_pattern_map[neuron_key][pattern] += 1

        # ì¼ê´€ì„± ì¸¡ì •: ê°™ì€ ë‰´ëŸ° Setì´ í•­ìƒ ê°™ì€ íŒ¨í„´ ì„ íƒ?
        consistency_scores = []
        for neuron_key, pattern_counts in neuron_pattern_map.items():
            total = sum(pattern_counts.values())
            if total > 1:  # 2ë²ˆ ì´ìƒ ë‚˜íƒ€ë‚œ Setë§Œ
                max_count = max(pattern_counts.values())
                consistency = max_count / total
                consistency_scores.append(consistency)

        if consistency_scores:
            avg_consistency = np.mean(consistency_scores)
            median_consistency = np.median(consistency_scores)

            print(f"\nLayer {layer_idx}:")
            print(f"  Unique neuron sets: {len(neuron_pattern_map)}")
            print(f"  Sets appearing >1 time: {len(consistency_scores)}")
            print(f"  Avg consistency: {avg_consistency:.4f}")
            print(f"  Median consistency: {median_consistency:.4f}")

            if avg_consistency > 0.9:
                print(f"  âœ… HIGH: Same neuron sets â†’ same patterns ({avg_consistency:.1%})")
            elif avg_consistency < 0.5:
                print(f"  âš ï¸  LOW: Neuron-pattern mapping is inconsistent")

            results[f'layer_{layer_idx}'] = {
                'unique_neuron_sets': len(neuron_pattern_map),
                'repeated_sets': len(consistency_scores),
                'avg_consistency': float(avg_consistency),
                'median_consistency': float(median_consistency),
            }

    return results


def analyze_selection_confidence(collector, n_layers):
    """ì„ íƒ confidence ë¶„ì„ (softmax score ë¶„í¬)"""
    print("\n" + "="*70)
    print("SELECTION CONFIDENCE ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.pattern_selections[layer_idx]) == 0:
            continue

        pattern_weights = collector.pattern_selections[layer_idx]  # [N, S, n_patterns]

        # Top-k ì ìˆ˜ ë¶„ì„
        top_scores, _ = pattern_weights.topk(k=3, dim=-1)  # [N, S, 3]
        top1_scores = top_scores[:, :, 0].numpy().flatten()
        top2_scores = top_scores[:, :, 1].numpy().flatten()
        top3_scores = top_scores[:, :, 2].numpy().flatten()

        # Gap ë¶„ì„
        gap_1_2 = top1_scores - top2_scores
        gap_2_3 = top2_scores - top3_scores

        print(f"\nLayer {layer_idx}:")
        print(f"  Top-1 score: {top1_scores.mean():.6f} Â± {top1_scores.std():.6f}")
        print(f"  Top-2 score: {top2_scores.mean():.6f} Â± {top2_scores.std():.6f}")
        print(f"  Top-3 score: {top3_scores.mean():.6f} Â± {top3_scores.std():.6f}")
        print(f"  Gap (1-2): {gap_1_2.mean():.6f} Â± {gap_1_2.std():.6f}")
        print(f"  Gap (2-3): {gap_2_3.mean():.6f} Â± {gap_2_3.std():.6f}")

        # Confidence í•´ì„
        if gap_1_2.mean() < 0.01:
            print(f"  âš ï¸  LOW CONFIDENCE: Top-1/2 gap very small ({gap_1_2.mean():.6f})")
        elif gap_1_2.mean() > 0.1:
            print(f"  âœ… HIGH CONFIDENCE: Clear winner (gap={gap_1_2.mean():.4f})")

        results[f'layer_{layer_idx}'] = {
            'top1_mean': float(top1_scores.mean()),
            'top1_std': float(top1_scores.std()),
            'gap_1_2_mean': float(gap_1_2.mean()),
            'gap_1_2_std': float(gap_1_2.std()),
        }

    return results


def analyze_position_patterns(collector, n_layers, max_positions=128):
    """ì‹œí€€ìŠ¤ ìœ„ì¹˜ë³„ ë‰´ëŸ° íŒ¨í„´ ë¶„ì„ â­"""
    print("\n" + "="*70)
    print("POSITION-BASED NEURON PATTERNS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        position_stats = {}

        # ì‹œì‘ (0-15), ì¤‘ê°„ (48-63), ë (112-127)
        ranges = {
            'start': (0, 16),
            'middle': (48, 64),
            'end': (112, 128)
        }

        for range_name, (start_pos, end_pos) in ranges.items():
            all_neurons = []
            for pos in range(start_pos, min(end_pos, max_positions)):
                if pos in collector.position_neuron_map:
                    neurons = collector.position_neuron_map[pos][layer_idx]
                    all_neurons.extend(neurons)

            if all_neurons:
                unique_neurons = len(set(all_neurons))
                position_stats[range_name] = unique_neurons
            else:
                position_stats[range_name] = 0

        # ìœ„ì¹˜ë³„ ë‹¤ì–‘ì„± ì°¨ì´
        if position_stats['start'] > 0 and position_stats['end'] > 0:
            diversity_change = (position_stats['end'] - position_stats['start']) / position_stats['start']

            print(f"\nLayer {layer_idx}:")
            print(f"  Start (0-15): {position_stats['start']} unique neurons")
            print(f"  Middle (48-63): {position_stats['middle']} unique neurons")
            print(f"  End (112-127): {position_stats['end']} unique neurons")
            print(f"  Change: {diversity_change:+.1%}")

            if abs(diversity_change) > 0.5:
                print(f"  âš ï¸  LARGE CHANGE: Position strongly affects neuron selection!")

        results[f'layer_{layer_idx}'] = position_stats

    return results


# ============================================================
# v5.0: Basis FFN Analysis
# ============================================================

def analyze_basis_usage(model, collector, n_layers, n_basis):
    """Analyze basis usage patterns in BasisFFN

    Checks:
    1. Which basis blocks are used
    2. Basis activation frequency
    3. Basis collapse detection
    """
    print("\n" + "="*70)
    print("ğŸ¯ BASIS USAGE ANALYSIS (v5.0)")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        layer = model.layers[layer_idx].basis_ffn

        # Get neuron selections from collector
        neuron_selections = collector.neuron_selections[layer_idx]  # [N, S, k]

        # Flatten
        neuron_idx_flat = neuron_selections.reshape(-1).long()  # [N*S*k]

        # Get coefficients for selected neurons
        coef_A = layer.neuron_coef_A[neuron_idx_flat]  # [N*S*k, n_basis]
        coef_B = layer.neuron_coef_B[neuron_idx_flat]  # [N*S*k, n_basis]

        # Compute basis importance (absolute average)
        basis_importance_A = coef_A.abs().mean(dim=0).detach().cpu().numpy()  # [n_basis]
        basis_importance_B = coef_B.abs().mean(dim=0).detach().cpu().numpy()  # [n_basis]

        # Combined importance
        basis_importance = (basis_importance_A + basis_importance_B) / 2

        # Statistics
        threshold = 0.01
        used_basis = (basis_importance > threshold).sum()

        # Gini coefficient (inequality measure)
        sorted_imp = np.sort(basis_importance)
        n = len(sorted_imp)
        index = np.arange(1, n + 1)
        gini = (2 * np.sum(index * sorted_imp)) / (n * np.sum(sorted_imp)) - (n + 1) / n

        # Entropy (diversity measure)
        basis_probs = basis_importance / (basis_importance.sum() + 1e-10)
        basis_entropy = -np.sum(basis_probs * np.log(basis_probs + 1e-10))
        max_entropy = np.log(n_basis)
        normalized_entropy = basis_entropy / max_entropy

        print(f"\nLayer {layer_idx}:")
        print(f"  Active basis (>{threshold}): {used_basis}/{n_basis} ({used_basis/n_basis*100:.1f}%)")
        print(f"  Gini coefficient: {gini:.4f} (0=equal, 1=concentrated)")
        print(f"  Entropy: {basis_entropy:.4f}/{max_entropy:.4f} ({normalized_entropy*100:.1f}%)")

        # Top basis
        top_idx = np.argsort(basis_importance)[::-1][:5]
        print(f"  Top-5 basis importance: {basis_importance[top_idx]}")

        # Warnings
        if used_basis < n_basis * 0.5:
            print(f"  ğŸ”´ WARNING: Only {used_basis}/{n_basis} basis active - COLLAPSE!")
        elif used_basis < n_basis * 0.7:
            print(f"  ğŸŸ¡ CAUTION: {used_basis}/{n_basis} basis active - underutilized")
        else:
            print(f"  âœ… GOOD: {used_basis}/{n_basis} basis active")

        if gini > 0.7:
            print(f"  ğŸ”´ WARNING: High Gini ({gini:.3f}) - basis usage concentrated!")

        if normalized_entropy < 0.6:
            print(f"  ğŸ”´ WARNING: Low entropy ({normalized_entropy:.3f}) - low diversity!")

        results[f'layer_{layer_idx}'] = {
            'active_basis': int(used_basis),
            'total_basis': int(n_basis),
            'usage_ratio': float(used_basis / n_basis),
            'gini': float(gini),
            'entropy': float(basis_entropy),
            'normalized_entropy': float(normalized_entropy),
            'importance_A': basis_importance_A.tolist(),
            'importance_B': basis_importance_B.tolist(),
            'importance_combined': basis_importance.tolist(),
        }

    return results


def analyze_neuron_basis_composition(model, n_layers, n_basis, n_neurons):
    """Analyze how neurons compose basis blocks

    Checks:
    1. How many basis each neuron uses
    2. Sparsity of neuron-basis connections
    3. Redundancy in compositions
    """
    print("\n" + "="*70)
    print("ğŸ”— NEURON-BASIS COMPOSITION ANALYSIS (v5.0)")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        layer = model.layers[layer_idx].basis_ffn

        # Coefficients: [n_neurons, n_basis]
        coef_A = layer.neuron_coef_A.data.cpu().numpy()
        coef_B = layer.neuron_coef_B.data.cpu().numpy()

        # Count active connections (threshold = 0.1)
        threshold = 0.1
        active_A = (np.abs(coef_A) > threshold).sum(axis=1)  # per neuron
        active_B = (np.abs(coef_B) > threshold).sum(axis=1)

        # Statistics
        avg_active_A = active_A.mean()
        avg_active_B = active_B.mean()
        std_active_A = active_A.std()
        std_active_B = active_B.std()

        # Sparsity
        sparsity_A = 1 - (active_A.sum() / (n_neurons * n_basis))
        sparsity_B = 1 - (active_B.sum() / (n_neurons * n_basis))

        # Check for redundancy (neurons with same basis)
        from scipy.spatial.distance import pdist, squareform

        # Normalize coefficients
        coef_A_norm = coef_A / (np.linalg.norm(coef_A, axis=1, keepdims=True) + 1e-8)
        coef_B_norm = coef_B / (np.linalg.norm(coef_B, axis=1, keepdims=True) + 1e-8)

        # Compute pairwise similarity (sample if too large)
        sample_size = min(200, n_neurons)
        sample_idx = np.random.choice(n_neurons, sample_size, replace=False)

        sim_A = 1 - pdist(coef_A_norm[sample_idx], 'cosine')
        sim_B = 1 - pdist(coef_B_norm[sample_idx], 'cosine')

        # Count highly similar pairs (>0.8)
        similar_pairs_A = (sim_A > 0.8).sum()
        similar_pairs_B = (sim_B > 0.8).sum()
        total_pairs = len(sim_A)

        print(f"\nLayer {layer_idx}:")
        print(f"  Avg active basis per neuron:")
        print(f"    Coef A: {avg_active_A:.2f} Â± {std_active_A:.2f} / {n_basis}")
        print(f"    Coef B: {avg_active_B:.2f} Â± {std_active_B:.2f} / {n_basis}")
        print(f"  Sparsity: A={sparsity_A*100:.1f}%, B={sparsity_B*100:.1f}%")
        print(f"  Similar neuron pairs (>0.8 cosine):")
        print(f"    A: {similar_pairs_A}/{total_pairs} ({similar_pairs_A/total_pairs*100:.1f}%)")
        print(f"    B: {similar_pairs_B}/{total_pairs} ({similar_pairs_B/total_pairs*100:.1f}%)")

        # Warnings
        if avg_active_A < 3 or avg_active_B < 3:
            print(f"  ğŸ”´ WARNING: Neurons use <3 basis on average - too sparse!")
        elif avg_active_A < 5 or avg_active_B < 5:
            print(f"  ğŸŸ¡ CAUTION: Low basis usage per neuron")
        else:
            print(f"  âœ… GOOD: Neurons use multiple basis")

        if similar_pairs_A / total_pairs > 0.3 or similar_pairs_B / total_pairs > 0.3:
            print(f"  ğŸ”´ WARNING: Many similar neurons - redundancy!")

        results[f'layer_{layer_idx}'] = {
            'avg_active_basis_A': float(avg_active_A),
            'avg_active_basis_B': float(avg_active_B),
            'sparsity_A': float(sparsity_A),
            'sparsity_B': float(sparsity_B),
            'similar_pairs_ratio_A': float(similar_pairs_A / total_pairs),
            'similar_pairs_ratio_B': float(similar_pairs_B / total_pairs),
        }

    return results


def analyze_basis_orthogonality(model, n_layers):
    """Analyze diversity of basis blocks

    Checks:
    1. Orthogonality between basis vectors
    2. Similarity distribution
    3. Effective rank
    """
    print("\n" + "="*70)
    print("ğŸ“ BASIS ORTHOGONALITY ANALYSIS (v5.0)")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        layer = model.layers[layer_idx].basis_ffn

        # Basis A: [n_basis, d_model, basis_rank]
        basis_A = layer.basis_A.data
        n_basis, d_model, basis_rank = basis_A.shape

        # Flatten to [n_basis, d_model*basis_rank]
        basis_A_flat = basis_A.view(n_basis, -1)

        # Normalize
        basis_A_norm = F.normalize(basis_A_flat, dim=1)

        # Similarity matrix [n_basis, n_basis]
        similarity = torch.mm(basis_A_norm, basis_A_norm.T)

        # Off-diagonal elements
        mask = ~torch.eye(n_basis, dtype=torch.bool, device=similarity.device)
        off_diag = similarity[mask].detach().cpu().numpy()

        # Statistics
        mean_sim = off_diag.mean()
        max_sim = off_diag.max()
        std_sim = off_diag.std()

        # Count highly similar pairs (>0.7)
        similar_pairs = (off_diag > 0.7).sum()
        total_pairs = len(off_diag)

        # Effective rank (via singular values)
        U, S, V = torch.svd(basis_A_flat)
        S_normalized = S / S.sum()
        effective_rank = torch.exp(-(S_normalized * torch.log(S_normalized + 1e-10)).sum()).item()
        rank_ratio = effective_rank / n_basis

        print(f"\nLayer {layer_idx} - Basis A:")
        print(f"  Mean similarity: {mean_sim:.4f} (lower is better)")
        print(f"  Max similarity: {max_sim:.4f}")
        print(f"  Std similarity: {std_sim:.4f}")
        print(f"  Similar pairs (>0.7): {similar_pairs}/{total_pairs} ({similar_pairs/total_pairs*100:.1f}%)")
        print(f"  Effective rank: {effective_rank:.2f}/{n_basis} ({rank_ratio*100:.1f}%)")

        # Same for Basis B
        basis_B = layer.basis_B.data
        basis_B_flat = basis_B.view(n_basis, -1)
        basis_B_norm = F.normalize(basis_B_flat, dim=1)
        similarity_B = torch.mm(basis_B_norm, basis_B_norm.T)
        off_diag_B = similarity_B[mask].detach().cpu().numpy()

        mean_sim_B = off_diag_B.mean()
        max_sim_B = off_diag_B.max()
        similar_pairs_B = (off_diag_B > 0.7).sum()

        print(f"\nLayer {layer_idx} - Basis B:")
        print(f"  Mean similarity: {mean_sim_B:.4f}")
        print(f"  Max similarity: {max_sim_B:.4f}")
        print(f"  Similar pairs (>0.7): {similar_pairs_B}/{total_pairs} ({similar_pairs_B/total_pairs*100:.1f}%)")

        # Warnings
        if mean_sim > 0.5 or mean_sim_B > 0.5:
            print(f"  ğŸ”´ WARNING: High mean similarity - basis not diverse!")
        elif mean_sim > 0.3 or mean_sim_B > 0.3:
            print(f"  ğŸŸ¡ CAUTION: Moderate similarity")
        else:
            print(f"  âœ… GOOD: Low similarity - diverse basis")

        if rank_ratio < 0.6:
            print(f"  ğŸ”´ WARNING: Low effective rank ({rank_ratio:.2f}) - basis collapse!")
        elif rank_ratio < 0.8:
            print(f"  ğŸŸ¡ CAUTION: Moderate effective rank")
        else:
            print(f"  âœ… GOOD: High effective rank - all basis used")

        results[f'layer_{layer_idx}'] = {
            'mean_similarity_A': float(mean_sim),
            'max_similarity_A': float(max_sim),
            'similar_pairs_ratio_A': float(similar_pairs / total_pairs),
            'effective_rank_A': float(effective_rank),
            'rank_ratio_A': float(rank_ratio),
            'mean_similarity_B': float(mean_sim_B),
            'max_similarity_B': float(max_sim_B),
            'similar_pairs_ratio_B': float(similar_pairs_B / total_pairs),
        }

    return results


def analyze_neuron_coactivation(collector, n_neurons, n_layers):
    """ë‰´ëŸ° co-activation íŒ¨í„´ ë¶„ì„ - ì–´ë–¤ ë‰´ëŸ°ë“¤ì´ í•¨ê»˜ ì„ íƒë˜ë‚˜? (ë³‘ë ¬ ìµœì í™”)"""
    print("\n" + "="*70)
    print("NEURON CO-ACTIVATION ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.neuron_selections[layer_idx]) == 0:
            continue

        selected_neurons = collector.neuron_selections[layer_idx]  # [N, S, k]
        N, S, k = selected_neurons.shape

        # ğŸš€ Vectorized co-activation counting using one-hot encoding
        # Create one-hot representation: [N, S, n_neurons]
        device = selected_neurons.device
        one_hot = torch.zeros(N, S, n_neurons, dtype=torch.float32, device=device)

        # Scatter ones at selected neuron positions
        one_hot.scatter_(2, selected_neurons, 1.0)

        # Activation counts: sum over batch and sequence
        neuron_activation_count = one_hot.sum(dim=(0, 1))  # [n_neurons]

        # Co-activation matrix: outer product then sum
        # [N, S, n_neurons, 1] @ [N, S, 1, n_neurons] -> [N, S, n_neurons, n_neurons]
        # Then sum over N, S -> [n_neurons, n_neurons]
        coactivation = torch.einsum('nsi,nsj->ij', one_hot, one_hot)  # Faster than matmul

        # Remove self-activation (diagonal)
        coactivation.fill_diagonal_(0)

        # Move to CPU for further processing
        coactivation = coactivation.cpu()
        neuron_activation_count = neuron_activation_count.cpu()

        # Normalize to get co-activation probability P(j | i)
        # Use broadcasting to avoid loop
        coactivation_prob = coactivation / (neuron_activation_count.unsqueeze(1) + 1e-10)

        # Find strong co-activation patterns (vectorized)
        threshold = 0.8
        # Create mask for upper triangle (avoid duplicates)
        mask = torch.triu(torch.ones_like(coactivation_prob, dtype=torch.bool), diagonal=1)
        # Mutual high probability: both P(j|i) and P(i|j) > threshold
        mutual_mask = mask & (coactivation_prob > threshold) & (coactivation_prob.T > threshold)

        # Get indices where condition is true
        strong_indices = torch.nonzero(mutual_mask, as_tuple=False)

        # Extract probabilities
        strong_pairs = []
        for idx in strong_indices:
            i, j = idx[0].item(), idx[1].item()
            prob_ij = coactivation_prob[i, j].item()
            prob_ji = coactivation_prob[j, i].item()
            strong_pairs.append((i, j, prob_ij, prob_ji))

        # Statistics
        active_neurons = (neuron_activation_count > 0).sum().item()
        active_mask = neuron_activation_count > 0
        avg_coactivation = coactivation_prob[active_mask, :][:, active_mask].mean().item() if active_mask.any() else 0.0

        # Save only summary stats (matrices are too large for JSON)
        results[f'layer_{layer_idx}'] = {
            'activation_counts': neuron_activation_count.numpy().tolist(),
            'strong_pairs': strong_pairs[:20],  # Top 20 strong pairs
            'active_neurons': int(active_neurons),
            'avg_coactivation_prob': float(avg_coactivation),
            # Store matrix shape info instead of full matrix
            'coactivation_matrix_shape': list(coactivation.shape),
            'coactivation_prob_shape': list(coactivation_prob.shape),
        }

        print(f"\nLayer {layer_idx}:")
        print(f"  Active neurons: {active_neurons}/{n_neurons}")
        print(f"  Strong co-activation pairs (>{threshold}): {len(strong_pairs)}")
        print(f"  Avg co-activation probability: {avg_coactivation:.4f}")

        # ê²½ê³ 
        if len(strong_pairs) > 50:
            print(f"  âš ï¸  STRONG COUPLING: {len(strong_pairs)} neuron pairs almost always activate together!")
            print(f"     â†’ May indicate redundant neurons or over-specialized combinations")

        # Show top 5 strong pairs
        if strong_pairs:
            print(f"  Top 5 co-activation pairs:")
            for idx, (i, j, prob_ij, prob_ji) in enumerate(strong_pairs[:5], 1):
                print(f"    #{idx}: Neurons ({i}, {j}) - P(j|i)={prob_ij:.3f}, P(i|j)={prob_ji:.3f}")

    return results


def analyze_neuron_diversity(model, n_layers):
    """ë‰´ëŸ° ê°„ ìœ ì‚¬ë„ ë° effective rank ë¶„ì„ (clustering í¬í•¨)"""
    print("\n" + "="*70)
    print("NEURON DIVERSITY ANALYSIS")
    print("="*70)

    from scipy.cluster.hierarchy import linkage

    results = {}

    for layer_idx in range(n_layers):
        if hasattr(model, '_orig_mod'):
            router = model._orig_mod.layers[layer_idx].neuron_router
        else:
            router = model.layers[layer_idx].neuron_router

        # Get neurons (handle low-rank decomposition)
        if hasattr(router, 'neuron_codes'):
            # v3.2: Low-rank neurons
            neurons = torch.matmul(router.neuron_codes.data, router.neuron_basis.data)
        else:
            # v3.1 and earlier: Full-rank neurons
            neurons = router.neurons.data

        neurons_cpu = neurons.cpu()

        # ì •ê·œí™”
        neurons_norm = F.normalize(neurons, p=2, dim=1)

        # ë‰´ëŸ° ê°„ ìœ ì‚¬ë„ (ì½”ì‚¬ì¸)
        similarity = torch.matmul(neurons_norm, neurons_norm.T)  # [n_neurons, n_neurons]

        # ìê¸° ìì‹  ì œì™¸í•˜ê³  ìœ ì‚¬ë„ ê³„ì‚°
        mask = ~torch.eye(similarity.shape[0], dtype=torch.bool, device=similarity.device)
        off_diag_sim = similarity[mask]

        # Find highly similar pairs (vectorized)
        threshold = 0.9
        similarity_cpu = similarity.cpu()
        n_neurons = similarity.shape[0]

        # Create upper triangular mask (avoid duplicates and diagonal)
        triu_mask = torch.triu(torch.ones(n_neurons, n_neurons, dtype=torch.bool), diagonal=1)

        # Find pairs with similarity > threshold
        high_sim_mask = triu_mask & (torch.abs(similarity_cpu) > threshold)
        pair_indices = torch.nonzero(high_sim_mask, as_tuple=False)

        # Extract pairs with their similarity values
        similar_pairs = [
            (i.item(), j.item(), similarity_cpu[i, j].item())
            for i, j in pair_indices
        ]

        # Hierarchical clustering
        try:
            linkage_matrix = linkage(neurons_cpu.numpy(), method='ward')
        except Exception as e:
            print(f"  âš ï¸  Clustering failed: {e}")
            linkage_matrix = None

        # Effective rank (SVD ê¸°ë°˜)
        U, S, V = torch.svd(neurons)
        # Normalized singular values
        S_normalized = S / S.sum()
        # Entropy-based effective rank
        entropy_val = -(S_normalized * torch.log(S_normalized + 1e-10)).sum()
        effective_rank = torch.exp(entropy_val).item()

        # í†µê³„
        mean_sim = off_diag_sim.mean().item()
        max_sim = off_diag_sim.max().item()
        std_sim = off_diag_sim.std().item()

        # Rank ratio
        rank_ratio = effective_rank / neurons.shape[0]

        results[f'layer_{layer_idx}'] = {
            'mean_similarity': mean_sim,
            'max_similarity': max_sim,
            'std_similarity': std_sim,
            'effective_rank': effective_rank,
            'total_neurons': neurons.shape[0],
            'rank_ratio': rank_ratio,
            'similar_pairs': similar_pairs[:20],  # Top 20 similar pairs
            'linkage': linkage_matrix.tolist() if linkage_matrix is not None else None
        }

        print(f"\nLayer {layer_idx}:")
        print(f"  Mean similarity: {mean_sim:.4f}")
        print(f"  Max similarity: {max_sim:.4f}")
        print(f"  Std similarity: {std_sim:.4f}")
        print(f"  Effective rank: {effective_rank:.1f} / {neurons.shape[0]}")
        print(f"  Rank ratio: {rank_ratio:.2%}")
        print(f"  Highly similar pairs (>{threshold}): {len(similar_pairs)}")

        # ê²½ê³ 
        if mean_sim > 0.5:
            print(f"  âš ï¸  HIGH SIMILARITY: Neurons are redundant!")
        if rank_ratio < 0.5:
            print(f"  âš ï¸  LOW RANK: Limited neuron diversity!")
        if len(similar_pairs) > 50:
            print(f"  âš ï¸  REDUNDANCY: {len(similar_pairs)} highly similar neuron pairs!")

    return results




def visualize_neuron_roles(diversity_results, coactivation_results, model, output_dir):
    """ë‰´ëŸ° ì—­í•  ì¢…í•© ì‹œê°í™” (similarity, co-activation, clustering)"""
    print("\n=== Visualizing Neuron Roles ===")

    from scipy.cluster.hierarchy import dendrogram
    import numpy as np

    output_dir = Path(output_dir)
    roles_dir = output_dir / 'neuron_roles'
    roles_dir.mkdir(exist_ok=True)

    n_layers = len([k for k in diversity_results.keys() if k.startswith('layer_')])

    for layer_idx in range(n_layers):
        layer_key = f'layer_{layer_idx}'

        if layer_key not in diversity_results:
            continue

        div_data = diversity_results[layer_key]
        coact_data = coactivation_results.get(layer_key, {})

        # Get neurons
        if hasattr(model, '_orig_mod'):
            router = model._orig_mod.layers[layer_idx].neuron_router
        else:
            router = model.layers[layer_idx].neuron_router

        if hasattr(router, 'neuron_codes'):
            neurons = torch.matmul(router.neuron_codes.data, router.neuron_basis.data)
        else:
            neurons = router.neurons.data

        neurons_cpu = neurons.cpu()
        neurons_norm = F.normalize(neurons, p=2, dim=1)
        similarity = torch.matmul(neurons_norm, neurons_norm.T).cpu().numpy()

        # Create comprehensive figure
        fig = plt.figure(figsize=(20, 12))

        # 1. Similarity Matrix
        ax1 = plt.subplot(2, 3, 1)
        im1 = ax1.imshow(similarity, cmap='RdBu', vmin=-1, vmax=1)
        ax1.set_title(f'Layer {layer_idx}: Neuron Similarity Matrix', fontsize=12, fontweight='bold')
        ax1.set_xlabel('Neuron ID')
        ax1.set_ylabel('Neuron ID')
        plt.colorbar(im1, ax=ax1, label='Cosine Similarity')

        # Add stats text
        stats_text = f"Mean: {div_data['mean_similarity']:.3f}\n"
        stats_text += f"Effective Rank: {div_data['effective_rank']:.1f}/{div_data['total_neurons']}\n"
        stats_text += f"Rank Ratio: {div_data['rank_ratio']:.2%}"
        ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes,
                verticalalignment='top', fontsize=9,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))

        # 2. Similarity Distribution
        ax2 = plt.subplot(2, 3, 2)
        sim_off_diag = similarity[~np.eye(similarity.shape[0], dtype=bool)]
        ax2.hist(sim_off_diag, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
        ax2.set_xlabel('Cosine Similarity')
        ax2.set_ylabel('Count')
        ax2.set_title('Similarity Distribution (off-diagonal)', fontsize=12, fontweight='bold')
        ax2.axvline(div_data['mean_similarity'], color='red', linestyle='--',
                   label=f"Mean: {div_data['mean_similarity']:.3f}")
        ax2.legend()
        ax2.grid(alpha=0.3)

        # 3. Dendrogram (if available)
        ax3 = plt.subplot(2, 3, 3)
        if div_data['linkage'] is not None:
            linkage_matrix = np.array(div_data['linkage'])
            dendrogram(linkage_matrix, ax=ax3, no_labels=True, color_threshold=0)
            ax3.set_title('Hierarchical Clustering', fontsize=12, fontweight='bold')
            ax3.set_ylabel('Distance')
        else:
            ax3.text(0.5, 0.5, 'Clustering unavailable', ha='center', va='center')
            ax3.set_title('Hierarchical Clustering', fontsize=12, fontweight='bold')

        # 4. Co-activation Summary (if available)
        ax4 = plt.subplot(2, 3, 4)
        if coact_data and 'strong_pairs' in coact_data:
            # Since we don't store full matrix, visualize strong pairs as network
            strong_pairs = coact_data.get('strong_pairs', [])

            if strong_pairs:
                # Create sparse representation of strong pairs
                n_neurons = div_data['total_neurons']
                sparse_coact = np.zeros((n_neurons, n_neurons))
                for i, j, prob_ij, prob_ji in strong_pairs:
                    sparse_coact[i, j] = prob_ij
                    sparse_coact[j, i] = prob_ji

                im4 = ax4.imshow(sparse_coact, cmap='YlOrRd', vmin=0, vmax=1)
                ax4.set_title(f'Strong Co-activation Pairs (>{len(strong_pairs)} pairs)', fontsize=12, fontweight='bold')
                ax4.set_xlabel('Neuron j')
                ax4.set_ylabel('Neuron i')
                plt.colorbar(im4, ax=ax4, label='Probability')
            else:
                ax4.text(0.5, 0.5, 'No strong co-activation pairs', ha='center', va='center')
                ax4.set_title('Strong Co-activation Pairs', fontsize=12, fontweight='bold')
        else:
            ax4.text(0.5, 0.5, 'No co-activation data', ha='center', va='center')
            ax4.set_title('Co-activation Probability', fontsize=12, fontweight='bold')

        # 5. Activation Counts (if available)
        ax5 = plt.subplot(2, 3, 5)
        if coact_data and 'activation_counts' in coact_data:
            activation_counts = np.array(coact_data['activation_counts'])
            ax5.bar(range(len(activation_counts)), activation_counts, color='coral', alpha=0.7)
            ax5.set_xlabel('Neuron ID')
            ax5.set_ylabel('Activation Count')
            ax5.set_title('Neuron Activation Frequency', fontsize=12, fontweight='bold')
            ax5.axhline(activation_counts.mean(), color='blue', linestyle='--',
                       label=f"Mean: {activation_counts.mean():.1f}")
            ax5.legend()
            ax5.grid(alpha=0.3, axis='y')
        else:
            ax5.text(0.5, 0.5, 'No activation data', ha='center', va='center')
            ax5.set_title('Neuron Activation Frequency', fontsize=12, fontweight='bold')

        # 6. Singular Value Distribution (Rank Analysis)
        ax6 = plt.subplot(2, 3, 6)
        U, S, V = torch.svd(neurons)
        S_normalized = (S / S.sum()).cpu().numpy()  # Move to CPU before numpy conversion
        ax6.plot(S_normalized, marker='o', markersize=3, alpha=0.7)
        ax6.set_xlabel('Singular Value Index')
        ax6.set_ylabel('Normalized Magnitude')
        ax6.set_title('Singular Value Spectrum', fontsize=12, fontweight='bold')
        ax6.set_yscale('log')
        ax6.grid(alpha=0.3)
        ax6.axhline(1.0/len(S_normalized), color='red', linestyle='--',
                   label=f'Uniform (1/{len(S_normalized)})')
        ax6.legend()

        plt.tight_layout()
        plt.savefig(roles_dir / f'neuron_roles_layer_{layer_idx}.png', dpi=150, bbox_inches='tight')
        plt.close()

        print(f"  Saved: neuron_roles_layer_{layer_idx}.png")

    # Summary across layers
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    layers = list(range(n_layers))
    mean_sims = [diversity_results[f'layer_{i}']['mean_similarity'] for i in layers]
    rank_ratios = [diversity_results[f'layer_{i}']['rank_ratio'] for i in layers]
    num_similar_pairs = [len(diversity_results[f'layer_{i}']['similar_pairs']) for i in layers]

    # Mean similarity by layer
    axes[0, 0].plot(layers, mean_sims, marker='o', linewidth=2, markersize=8)
    axes[0, 0].set_xlabel('Layer')
    axes[0, 0].set_ylabel('Mean Cosine Similarity')
    axes[0, 0].set_title('Neuron Similarity Across Layers', fontweight='bold')
    axes[0, 0].grid(alpha=0.3)
    axes[0, 0].axhline(0.5, color='red', linestyle='--', alpha=0.5, label='High redundancy threshold')
    axes[0, 0].legend()

    # Rank ratio by layer
    axes[0, 1].plot(layers, rank_ratios, marker='s', linewidth=2, markersize=8, color='green')
    axes[0, 1].set_xlabel('Layer')
    axes[0, 1].set_ylabel('Effective Rank Ratio')
    axes[0, 1].set_title('Neuron Diversity Across Layers', fontweight='bold')
    axes[0, 1].grid(alpha=0.3)
    axes[0, 1].axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Low diversity threshold')
    axes[0, 1].legend()
    axes[0, 1].set_ylim([0, 1])

    # Highly similar pairs
    axes[1, 0].bar(layers, num_similar_pairs, color='orange', alpha=0.7, edgecolor='black')
    axes[1, 0].set_xlabel('Layer')
    axes[1, 0].set_ylabel('Number of Pairs')
    axes[1, 0].set_title('Highly Similar Neuron Pairs (>0.9)', fontweight='bold')
    axes[1, 0].grid(alpha=0.3, axis='y')

    # Co-activation stats (if available)
    if all(f'layer_{i}' in coactivation_results for i in layers):
        strong_coact_pairs = [len(coactivation_results[f'layer_{i}'].get('strong_pairs', [])) for i in layers]
        axes[1, 1].bar(layers, strong_coact_pairs, color='purple', alpha=0.7, edgecolor='black')
        axes[1, 1].set_xlabel('Layer')
        axes[1, 1].set_ylabel('Number of Pairs')
        axes[1, 1].set_title('Strong Co-activation Pairs (>0.8)', fontweight='bold')
        axes[1, 1].grid(alpha=0.3, axis='y')
    else:
        axes[1, 1].text(0.5, 0.5, 'No co-activation data', ha='center', va='center',
                       transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('Strong Co-activation Pairs', fontweight='bold')

    plt.tight_layout()
    plt.savefig(roles_dir / 'neuron_roles_summary.png', dpi=150, bbox_inches='tight')
    plt.close()

    print(f"  Saved: neuron_roles_summary.png")


# ============================================================
# LAYER BOTTLENECK DIAGNOSIS
# ============================================================

def analyze_layer_bottleneck(model, dataloader, device, num_batches=50, tokenizer=None):
    """Layerë³„ gradient ë° activation ë¶„ì„"""
    print("\n" + "="*70)
    print("LAYER BOTTLENECK DIAGNOSIS")
    print("="*70)

    model.train()  # Enable gradients
    stats = {
        'gradients': defaultdict(list),
        'activations': defaultdict(list),
    }

    n_layers = len(model.layers) if hasattr(model, 'layers') else len(model._orig_mod.layers)

    for batch_idx, batch in enumerate(tqdm(dataloader, total=num_batches, desc="Analyzing bottleneck")):
        if batch_idx >= num_batches:
            break

        input_ids = batch['input_ids'].to(device)

        # Apply MLM masking if tokenizer available
        if tokenizer is not None:
            input_ids, labels = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)
        else:
            labels = input_ids.clone()

        model.zero_grad()

        # Forward with losses
        logits, losses = model(input_ids, return_losses=True)

        # Compute loss
        B, S, V = logits.shape
        loss = F.cross_entropy(
            logits.view(B * S, V),
            labels.view(B * S),
            ignore_index=-100
        )

        # Add aux losses
        aux_loss = sum(losses['pattern_load']) + sum(losses['neuron_ortho'])
        total_loss = loss + 0.1 * aux_loss

        # Backward
        total_loss.backward()

        # Collect gradients for each layer
        for layer_idx in range(n_layers):
            if hasattr(model, '_orig_mod'):
                layer = model._orig_mod.layers[layer_idx]
            else:
                layer = model.layers[layer_idx]

            # Router gradients
            if layer.neuron_router.neurons.grad is not None:
                router_grad = layer.neuron_router.neurons.grad.norm().item()
            else:
                router_grad = 0.0

            # Pattern gradients
            if layer.neuron_interaction.pattern_queries.grad is not None:
                pattern_grad = layer.neuron_interaction.pattern_queries.grad.norm().item()
            else:
                pattern_grad = 0.0

            stats['gradients'][layer_idx].append({
                'router': router_grad,
                'pattern': pattern_grad
            })

    model.eval()  # Back to eval

    # Analyze results
    results = {}
    print(f"\nGradient Flow Analysis:")
    print("-" * 70)

    for layer_idx in range(n_layers):
        router_grads = [s['router'] for s in stats['gradients'][layer_idx]]
        pattern_grads = [s['pattern'] for s in stats['gradients'][layer_idx]]

        avg_router = np.mean(router_grads)
        avg_pattern = np.mean(pattern_grads)

        print(f"\nLayer {layer_idx}:")
        print(f"  Router grad:  {avg_router:.6f}")
        print(f"  Pattern grad: {avg_pattern:.6f}")

        if avg_router < 0.001:
            print(f"  ğŸ”´ WEAK GRADIENT: Router gradient very small!")
        if avg_pattern < 0.001:
            print(f"  ğŸ”´ WEAK GRADIENT: Pattern gradient very small!")

        results[f'layer_{layer_idx}'] = {
            'router_grad_mean': float(avg_router),
            'router_grad_std': float(np.std(router_grads)),
            'pattern_grad_mean': float(avg_pattern),
            'pattern_grad_std': float(np.std(pattern_grads)),
        }

    # Compare across layers
    print(f"\n{'='*70}")
    print("Gradient Flow Comparison:")
    print("-" * 70)

    router_means = [results[f'layer_{i}']['router_grad_mean'] for i in range(n_layers)]
    max_grad = max(router_means) if max(router_means) > 0 else 1.0

    for layer_idx in range(n_layers):
        ratio = router_means[layer_idx] / max_grad
        bar = "â–ˆ" * int(ratio * 50)
        marker = " ğŸ”´" if ratio < 0.3 else ""
        print(f"  L{layer_idx}: {ratio:.3f} {bar}{marker}")

    return results


def analyze_information_flow(model, dataloader, device, num_batches=50):
    """Layerê°„ ì •ë³´ íë¦„ ë¶„ì„ (similarity, change, rank)"""
    print("\n" + "="*70)
    print("INFORMATION FLOW ANALYSIS")
    print("="*70)

    model.eval()

    n_layers = len(model.layers) if hasattr(model, 'layers') else len(model._orig_mod.layers)
    d_model = model.d_model if hasattr(model, 'd_model') else model._orig_mod.d_model

    # Collect representations from each layer
    representations = [[] for _ in range(n_layers + 1)]  # +1 for input

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, total=num_batches, desc="Collecting representations")):
            if batch_idx >= num_batches:
                break

            input_ids = batch['input_ids'].to(device)
            B, S = input_ids.shape

            # Get model reference
            if hasattr(model, '_orig_mod'):
                m = model._orig_mod
            else:
                m = model

            # Embedding
            pos = torch.arange(S, device=device).unsqueeze(0)
            x = m.token_emb(input_ids) + m.pos_emb(pos)
            x = m.dropout(x)

            representations[0].append(x.cpu())

            # Causal mask
            mask = torch.tril(torch.ones(S, S, device=device)).unsqueeze(0).unsqueeze(0)

            # Through layers
            for layer_idx, layer in enumerate(m.layers):
                x, _ = layer(x, mask)
                representations[layer_idx + 1].append(x.cpu())

    # Stack representations
    for i in range(n_layers + 1):
        representations[i] = torch.cat(representations[i], dim=0)  # [N, S, D]

    # Analysis
    results = {}

    # 1. Cosine similarity to input
    print(f"\nCosine Similarity to Input:")
    print("-" * 70)

    x0_flat = representations[0].view(-1, d_model)

    for layer_idx in range(1, n_layers + 1):
        x_flat = representations[layer_idx].view(-1, d_model)

        # Sample if too large
        if x0_flat.shape[0] > 10000:
            idx = torch.randperm(x0_flat.shape[0])[:10000]
            x0_sample = x0_flat[idx]
            x_sample = x_flat[idx]
        else:
            x0_sample = x0_flat
            x_sample = x_flat

        cos_sim = F.cosine_similarity(x0_sample, x_sample, dim=-1).mean().item()

        bar = "â–ˆ" * int(cos_sim * 50)
        marker = " ğŸ”´" if cos_sim < 0.5 else ""
        print(f"  After L{layer_idx-1}: {cos_sim:.3f} {bar}{marker}")

        results[f'layer_{layer_idx-1}_input_sim'] = float(cos_sim)

    # 2. Layer-to-layer change
    print(f"\nLayer-to-Layer Change (L2 distance):")
    print("-" * 70)

    for layer_idx in range(n_layers):
        x_prev = representations[layer_idx].view(-1, d_model)
        x_next = representations[layer_idx + 1].view(-1, d_model)

        # Sample if too large
        if x_prev.shape[0] > 10000:
            idx = torch.randperm(x_prev.shape[0])[:10000]
            x_prev = x_prev[idx]
            x_next = x_next[idx]

        change = ((x_next - x_prev) ** 2).sum(dim=-1).sqrt().mean().item()

        bar = "â–ˆ" * int(min(change / 5.0, 1.0) * 50)
        marker = " ğŸ”´" if change > 8.0 else " âš ï¸" if change > 5.0 else ""
        print(f"  L{layer_idx}: {change:.3f} {bar}{marker}")

        results[f'layer_{layer_idx}_change'] = float(change)

    # 3. Effective rank
    print(f"\nEffective Rank (representation diversity):")
    print("-" * 70)

    for layer_idx in range(n_layers + 1):
        x_flat = representations[layer_idx].view(-1, d_model)

        # Sample for efficiency
        if x_flat.shape[0] > 5000:
            idx = torch.randperm(x_flat.shape[0])[:5000]
            x_flat = x_flat[idx]

        # Compute covariance
        x_centered = x_flat - x_flat.mean(dim=0, keepdim=True)
        cov = torch.mm(x_centered.T, x_centered) / x_centered.shape[0]

        # Eigenvalues
        eigenvalues = torch.linalg.eigvalsh(cov)
        eigenvalues = eigenvalues[eigenvalues > 1e-8]

        if len(eigenvalues) == 0:
            eff_rank = 0.0
            rank_ratio = 0.0
        else:
            # Normalize
            eigenvalues = eigenvalues / eigenvalues.sum()

            # Effective rank
            entropy_val = -(eigenvalues * torch.log(eigenvalues + 1e-10)).sum()
            eff_rank = torch.exp(entropy_val).item()
            rank_ratio = eff_rank / d_model

        bar = "â–ˆ" * int(rank_ratio * 50)
        marker = " ğŸ”´" if rank_ratio < 0.3 else " âš ï¸" if rank_ratio < 0.5 else ""
        name = "Input" if layer_idx == 0 else f"L{layer_idx-1}"
        print(f"  {name:>5s}: {rank_ratio:.3f} ({eff_rank:.1f}/{d_model}) {bar}{marker}")

        key = 'input_rank' if layer_idx == 0 else f'layer_{layer_idx-1}_rank'
        results[key] = {
            'effective_rank': float(eff_rank),
            'rank_ratio': float(rank_ratio)
        }

    return results


def analyze_task_pressure(model, dataloader, device, num_batches=30, tokenizer=None):
    """ê° ë ˆì´ì–´ì˜ task pressure ì¸¡ì • (ablation study)"""
    print("\n" + "="*70)
    print("TASK PRESSURE ANALYSIS (Ablation Study)")
    print("="*70)

    model.eval()

    n_layers = len(model.layers) if hasattr(model, 'layers') else len(model._orig_mod.layers)
    layer_contributions = defaultdict(list)

    # Get model reference
    if hasattr(model, '_orig_mod'):
        m = model._orig_mod
    else:
        m = model

    # Baseline: all layers
    print("\nComputing baseline (all layers)...")
    baseline_losses = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= num_batches:
                break

            input_ids = batch['input_ids'].to(device)

            # Apply MLM masking if tokenizer available
            if tokenizer is not None:
                input_ids, labels = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)
            else:
                labels = input_ids.clone()

            logits = model(input_ids)
            B, S, V = logits.shape
            loss = F.cross_entropy(
                logits.view(B * S, V),
                labels.view(B * S),
                ignore_index=-100
            ).item()
            baseline_losses.append(loss)

    baseline_loss = np.mean(baseline_losses)
    print(f"Baseline loss: {baseline_loss:.4f}")

    # Ablate each layer
    print("\nAblating each layer (identity function)...")

    for layer_idx in tqdm(range(n_layers), desc="Layer ablation"):
        # Save original forward
        original_forward = m.layers[layer_idx].forward

        # Define identity forward
        def identity_forward(x, mask=None, **kwargs):
            # Return x unchanged, with dummy outputs
            return x, None

        # Replace with identity
        m.layers[layer_idx].forward = identity_forward

        # Measure loss
        ablated_losses = []
        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx >= num_batches:
                    break

                input_ids = batch['input_ids'].to(device)

                # Apply MLM masking if tokenizer available
                if tokenizer is not None:
                    input_ids, labels = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)
                else:
                    labels = input_ids.clone()

                logits = model(input_ids)
                B, S, V = logits.shape
                loss = F.cross_entropy(
                    logits.view(B * S, V),
                    labels.view(B * S),
                    ignore_index=-100
                ).item()
                ablated_losses.append(loss)

        # Restore original
        m.layers[layer_idx].forward = original_forward

        ablated_loss = np.mean(ablated_losses)
        contribution = ablated_loss - baseline_loss
        layer_contributions[layer_idx] = contribution

    # Results
    print(f"\n{'='*70}")
    print("Layer Importance (loss increase when removed):")
    print("-" * 70)

    max_contrib = max(layer_contributions.values()) if layer_contributions else 1.0

    results = {}
    for layer_idx in range(n_layers):
        contrib = layer_contributions[layer_idx]
        importance = contrib / max_contrib if max_contrib > 0 else 0

        bar = "â–ˆ" * int(importance * 50)
        marker = " ğŸ”¥" if importance > 0.8 else " âš ï¸" if importance > 0.5 else ""

        print(f"  L{layer_idx}: {contrib:+.4f} {bar}{marker}")

        results[f'layer_{layer_idx}'] = {
            'loss_increase': float(contrib),
            'importance': float(importance)
        }

    return results


def analyze_gradient_flow_detailed(model, dataloader, device, num_batches=30, tokenizer=None):
    """íŒŒë¼ë¯¸í„° ìˆ˜ë¡œ ì •ê·œí™”ëœ gradient ë¶„ì„"""
    print("\n" + "="*70)
    print("DETAILED GRADIENT FLOW ANALYSIS (Normalized by Param Count)")
    print("="*70)

    model.train()

    n_layers = len(model.layers) if hasattr(model, 'layers') else len(model._orig_mod.layers)

    # Collect gradients
    router_grads = [[] for _ in range(n_layers)]
    pattern_grads = [[] for _ in range(n_layers)]
    ffn_grads = [[] for _ in range(n_layers)]

    for batch_idx, batch in enumerate(tqdm(dataloader, total=num_batches, desc="Collecting detailed gradients")):
        if batch_idx >= num_batches:
            break

        input_ids = batch['input_ids'].to(device)

        if tokenizer is not None:
            input_ids, labels = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)
        else:
            labels = input_ids.clone()

        model.zero_grad()

        # Forward
        logits, losses = model(input_ids, return_losses=True)

        # Loss
        B, S, V = logits.shape
        ce_loss = F.cross_entropy(
            logits.view(B * S, V),
            labels.view(B * S),
            ignore_index=-100
        )

        # With regularization
        pattern_load_loss = sum(losses['pattern_load'])
        neuron_ortho_loss = sum(losses['neuron_ortho'])
        total_loss = ce_loss + 0.1 * pattern_load_loss + 0.01 * neuron_ortho_loss

        total_loss.backward()

        # Collect per-layer gradients
        for layer_idx in range(n_layers):
            if hasattr(model, '_orig_mod'):
                layer = model._orig_mod.layers[layer_idx]
            else:
                layer = model.layers[layer_idx]

            # Router gradients (all params)
            router_grad_list = [
                p.grad.flatten()
                for p in layer.neuron_router.parameters()
                if p.grad is not None
            ]
            if router_grad_list:
                router_grad = torch.cat(router_grad_list).norm().item()
            else:
                router_grad = 0.0

            # Pattern query gradients specifically
            if layer.neuron_interaction.pattern_queries.grad is not None:
                pattern_grad = layer.neuron_interaction.pattern_queries.grad.norm().item()
            else:
                pattern_grad = 0.0

            # All FFN gradients
            ffn_grad_list = [
                p.grad.flatten()
                for p in layer.neuron_interaction.parameters()
                if p.grad is not None
            ]
            if ffn_grad_list:
                ffn_grad = torch.cat(ffn_grad_list).norm().item()
            else:
                ffn_grad = 0.0

            router_grads[layer_idx].append(router_grad)
            pattern_grads[layer_idx].append(pattern_grad)
            ffn_grads[layer_idx].append(ffn_grad)

    model.eval()

    # Analysis
    results = {}

    print("\n" + "-" * 70)
    print("Component Gradients (averaged across batches):")
    print("-" * 70)

    for layer_idx in range(n_layers):
        if hasattr(model, '_orig_mod'):
            layer = model._orig_mod.layers[layer_idx]
        else:
            layer = model.layers[layer_idx]

        # Parameter counts
        router_params = sum(p.numel() for p in layer.neuron_router.parameters())
        pattern_query_params = layer.neuron_interaction.pattern_queries.numel()
        ffn_params = sum(p.numel() for p in layer.neuron_interaction.parameters())

        # Average gradients
        avg_router_grad = np.mean(router_grads[layer_idx])
        avg_pattern_grad = np.mean(pattern_grads[layer_idx])
        avg_ffn_grad = np.mean(ffn_grads[layer_idx])

        # Normalized by param count (gradient per parameter)
        norm_router = avg_router_grad / router_params if router_params > 0 else 0
        norm_pattern = avg_pattern_grad / pattern_query_params if pattern_query_params > 0 else 0
        norm_ffn = avg_ffn_grad / ffn_params if ffn_params > 0 else 0

        print(f"\nLayer {layer_idx}:")
        print(f"  Router:")
        print(f"    Grad norm: {avg_router_grad:.6f}")
        print(f"    Params: {router_params:,}")
        print(f"    Grad/param: {norm_router:.9f}")
        print(f"  Pattern queries:")
        print(f"    Grad norm: {avg_pattern_grad:.6f}")
        print(f"    Params: {pattern_query_params:,}")
        print(f"    Grad/param: {norm_pattern:.9f}")
        print(f"  Full FFN:")
        print(f"    Grad norm: {avg_ffn_grad:.6f}")
        print(f"    Params: {ffn_params:,}")
        print(f"    Grad/param: {norm_ffn:.9f}")

        # Ratio analysis
        if norm_router > 0:
            pattern_to_router_ratio = norm_pattern / norm_router
            print(f"  Pattern/Router ratio: {pattern_to_router_ratio:.4f}")

            if pattern_to_router_ratio < 0.01:
                print(f"    ğŸ”´ Pattern gradient is <1% of Router!")
            elif pattern_to_router_ratio < 0.1:
                print(f"    âš ï¸  Pattern gradient is <10% of Router")

        results[f'layer_{layer_idx}'] = {
            'router_grad': float(avg_router_grad),
            'router_params': int(router_params),
            'router_grad_per_param': float(norm_router),
            'pattern_grad': float(avg_pattern_grad),
            'pattern_params': int(pattern_query_params),
            'pattern_grad_per_param': float(norm_pattern),
            'ffn_grad': float(avg_ffn_grad),
            'ffn_params': int(ffn_params),
            'ffn_grad_per_param': float(norm_ffn),
        }

    return results


def analyze_pattern_necessity(model, dataloader, device, tokenizer=None, num_batches=50):
    """íŒ¨í„´ì´ ì‹¤ì œë¡œ í•„ìš”í•œì§€ ablation í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("PATTERN NECESSITY ANALYSIS (Ablation Test)")
    print("="*70)

    model.eval()

    if hasattr(model, '_orig_mod'):
        m = model._orig_mod
    else:
        m = model

    # Save original pattern queries
    original_queries = [
        layer.neuron_interaction.pattern_queries.data.clone()
        for layer in m.layers
    ]

    def evaluate_loss():
        """Evaluate model loss"""
        total_loss = 0.0
        total_tokens = 0

        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx >= num_batches:
                    break

                input_ids = batch['input_ids'].to(device)

                if tokenizer is not None:
                    input_ids, labels = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)
                else:
                    labels = input_ids.clone()

                logits = model(input_ids)
                B, S, V = logits.shape

                loss = F.cross_entropy(
                    logits.view(B * S, V),
                    labels.view(B * S),
                    ignore_index=-100
                )

                total_loss += loss.item() * B * S
                total_tokens += B * S

        return total_loss / total_tokens if total_tokens > 0 else 0.0

    # 1. Normal loss
    print("\n1. Normal model (all patterns active)...")
    normal_loss = evaluate_loss()
    print(f"   Loss: {normal_loss:.4f}")

    # 2. Uniform patterns (all same)
    print("\n2. Uniform patterns (all identical)...")
    for layer in m.layers:
        layer.neuron_interaction.pattern_queries.data.fill_(0.01)  # Small uniform value

    uniform_loss = evaluate_loss()
    diff_uniform = uniform_loss - normal_loss
    print(f"   Loss: {uniform_loss:.4f} (Î” = {diff_uniform:+.4f})")

    # 3. Single pattern only
    print("\n3. Single pattern per layer...")
    for layer_idx, layer in enumerate(m.layers):
        layer.neuron_interaction.pattern_queries.data.zero_()
        # Keep only first pattern
        layer.neuron_interaction.pattern_queries.data[0] = original_queries[layer_idx][0]

    single_loss = evaluate_loss()
    diff_single = single_loss - normal_loss
    print(f"   Loss: {single_loss:.4f} (Î” = {diff_single:+.4f})")

    # 4. Random patterns
    print("\n4. Random patterns (reinitialize)...")
    for layer in m.layers:
        layer.neuron_interaction.pattern_queries.data.normal_(0, 0.02)

    random_loss = evaluate_loss()
    diff_random = random_loss - normal_loss
    print(f"   Loss: {random_loss:.4f} (Î” = {diff_random:+.4f})")

    # Restore original
    for layer_idx, layer in enumerate(m.layers):
        layer.neuron_interaction.pattern_queries.data.copy_(original_queries[layer_idx])

    # Analysis
    print(f"\n{'='*70}")
    print("Pattern Necessity Summary:")
    print("-" * 70)

    if abs(diff_uniform) < 0.01:
        print("ğŸ”´ CRITICAL: Uniform patterns â‰ˆ normal loss!")
        print("   â†’ Patterns are NOT being used effectively")
        print("   â†’ Model can work WITHOUT pattern selection")
    elif abs(diff_uniform) < 0.05:
        print("âš ï¸  WARNING: Uniform patterns only slightly worse")
        print("   â†’ Pattern selection has minimal impact")
    else:
        print("âœ… Patterns are necessary (uniform degrades loss)")

    if abs(diff_single) < 0.01:
        print("\nğŸ”´ CRITICAL: Single pattern â‰ˆ normal loss!")
        print("   â†’ Only 1 pattern needed per layer")
        print("   â†’ Pattern collapse is EXPECTED behavior")
    elif abs(diff_single) < 0.05:
        print("\nâš ï¸  WARNING: Single pattern almost sufficient")
        print("   â†’ Pattern diversity may not be needed")
    else:
        print("\nâœ… Multiple patterns needed (single pattern degrades)")

    if abs(diff_random) < 0.5:
        print("\nâš ï¸  WARNING: Random patterns not much worse")
        print("   â†’ Trained patterns barely better than random")
        print("   â†’ Patterns may not be learning")
    else:
        print("\nâœ… Trained patterns are learned (random is much worse)")

    results = {
        'normal_loss': float(normal_loss),
        'uniform_loss': float(uniform_loss),
        'uniform_diff': float(diff_uniform),
        'single_loss': float(single_loss),
        'single_diff': float(diff_single),
        'random_loss': float(random_loss),
        'random_diff': float(diff_random),
    }

    return results


def analyze_regularization_impact(model, dataloader, device, tokenizer=None, num_batches=50):
    """Regularizationì´ ì‹¤ì œë¡œ lossì— ì–¼ë§ˆë‚˜ ì˜í–¥ ì£¼ëŠ”ì§€ ë¶„ì„"""
    print("\n" + "="*70)
    print("REGULARIZATION IMPACT ANALYSIS")
    print("="*70)

    model.eval()

    # Collect losses
    ce_losses = []
    pattern_load_losses = []
    neuron_ortho_losses = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, total=num_batches, desc="Analyzing regularization")):
            if batch_idx >= num_batches:
                break

            input_ids = batch['input_ids'].to(device)

            if tokenizer is not None:
                input_ids, labels = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)
            else:
                labels = input_ids.clone()

            logits, losses = model(input_ids, return_losses=True)

            B, S, V = logits.shape
            ce_loss = F.cross_entropy(
                logits.view(B * S, V),
                labels.view(B * S),
                ignore_index=-100
            ).item()

            pattern_load_loss = sum(l.item() for l in losses['pattern_load'])
            neuron_ortho_loss = sum(l.item() for l in losses['neuron_ortho'])

            ce_losses.append(ce_loss)
            pattern_load_losses.append(pattern_load_loss)
            neuron_ortho_losses.append(neuron_ortho_loss)

    # Average
    avg_ce = np.mean(ce_losses)
    avg_load = np.mean(pattern_load_losses)
    avg_ortho = np.mean(neuron_ortho_losses)

    # With current weights
    current_weight_load = 0.1
    current_weight_ortho = 0.01

    weighted_load = current_weight_load * avg_load
    weighted_ortho = current_weight_ortho * avg_ortho
    total_loss = avg_ce + weighted_load + weighted_ortho

    print(f"\nLoss Components (averaged over {num_batches} batches):")
    print("-" * 70)
    print(f"  CE Loss:              {avg_ce:.4f}")
    print(f"  Pattern Load Loss:    {avg_load:.4f} (raw)")
    print(f"    â†’ Weighted (0.1):   {weighted_load:.4f} ({weighted_load/total_loss*100:.1f}% of total)")
    print(f"  Neuron Ortho Loss:    {avg_ortho:.4f} (raw)")
    print(f"    â†’ Weighted (0.01):  {weighted_ortho:.4f} ({weighted_ortho/total_loss*100:.1f}% of total)")
    print(f"  Total Loss:           {total_loss:.4f}")

    # Impact analysis
    print(f"\n{'='*70}")
    print("Regularization Impact:")
    print("-" * 70)

    load_ratio = weighted_load / avg_ce
    ortho_ratio = weighted_ortho / avg_ce

    print(f"  Load balancing / CE:  {load_ratio:.4f} ({load_ratio*100:.2f}%)")
    print(f"  Orthogonality / CE:   {ortho_ratio:.4f} ({ortho_ratio*100:.2f}%)")

    if load_ratio < 0.01:
        print(f"\n  ğŸ”´ Load balancing is <1% of CE loss!")
        print(f"     â†’ Too weak to enforce pattern diversity")
        print(f"     â†’ Consider increasing weight: 0.1 â†’ {0.1 * 10:.1f}")
    elif load_ratio < 0.05:
        print(f"\n  âš ï¸  Load balancing is <5% of CE loss")
        print(f"     â†’ May be too weak")
        print(f"     â†’ Consider increasing weight: 0.1 â†’ {0.1 * 2:.1f}")
    else:
        print(f"\n  âœ… Load balancing has significant impact")

    if ortho_ratio < 0.001:
        print(f"\n  ğŸ”´ Orthogonality is <0.1% of CE loss!")
        print(f"     â†’ Too weak to enforce neuron diversity")
        print(f"     â†’ Consider increasing weight: 0.01 â†’ {0.01 * 10:.2f}")
    elif ortho_ratio < 0.01:
        print(f"\n  âš ï¸  Orthogonality is <1% of CE loss")
        print(f"     â†’ May be too weak")
        print(f"     â†’ Consider increasing weight: 0.01 â†’ {0.01 * 2:.2f}")
    else:
        print(f"\n  âœ… Orthogonality has significant impact")

    # Recommendations
    print(f"\n{'='*70}")
    print("Recommendations:")
    print("-" * 70)

    if load_ratio < 0.05:
        # Calculate needed weight
        target_ratio = 0.05
        needed_weight = (target_ratio * avg_ce) / avg_load if avg_load > 0 else 0.1
        print(f"  â†’ Increase load balancing weight to {needed_weight:.2f}")
        print(f"    (to reach {target_ratio*100:.0f}% of CE loss)")

    if ortho_ratio < 0.01:
        target_ratio = 0.01
        needed_weight = (target_ratio * avg_ce) / avg_ortho if avg_ortho > 0 else 0.01
        print(f"  â†’ Increase orthogonality weight to {needed_weight:.3f}")
        print(f"    (to reach {target_ratio*100:.0f}% of CE loss)")

    results = {
        'ce_loss': float(avg_ce),
        'pattern_load_loss_raw': float(avg_load),
        'pattern_load_loss_weighted': float(weighted_load),
        'neuron_ortho_loss_raw': float(avg_ortho),
        'neuron_ortho_loss_weighted': float(weighted_ortho),
        'total_loss': float(total_loss),
        'load_to_ce_ratio': float(load_ratio),
        'ortho_to_ce_ratio': float(ortho_ratio),
    }

    return results


def analyze_pattern_diversity(model, n_layers):
    """íŒ¨í„´ ê°„ ì‹¤ì œ ì°¨ì´ ë¶„ì„ - íŒ¨í„´ë“¤ì´ ì§„ì§œ ë‹¤ë¥¸ê°€?"""
    print("\n" + "="*70)
    print("PATTERN DIVERSITY ANALYSIS (Are Patterns Actually Different?)")
    print("="*70)

    if hasattr(model, '_orig_mod'):
        m = model._orig_mod
    else:
        m = model

    results = {}

    for layer_idx in range(n_layers):
        layer = m.layers[layer_idx]
        patterns = layer.neuron_interaction.pattern_queries.data  # [n_patterns, d_model]

        # Cosine similarity matrix
        patterns_norm = F.normalize(patterns, dim=1)
        similarity = torch.mm(patterns_norm, patterns_norm.T)  # [n_patterns, n_patterns]

        # ëŒ€ê°ì„  ì œì™¸
        n_patterns = patterns.shape[0]
        mask = ~torch.eye(n_patterns, dtype=torch.bool, device=similarity.device)
        off_diag = similarity[mask]

        avg_sim = off_diag.mean().item()
        max_sim = off_diag.max().item()
        min_sim = off_diag.min().item()
        std_sim = off_diag.std().item()

        # Effective diversity (1 - avg similarity)
        eff_diversity = 1 - avg_sim

        # Count highly similar pairs
        high_sim_threshold = 0.9
        high_sim_pairs = (off_diag > high_sim_threshold).sum().item()

        print(f"\nLayer {layer_idx}:")
        print(f"  Pattern similarity (off-diagonal):")
        print(f"    Mean: {avg_sim:.4f}")
        print(f"    Max:  {max_sim:.4f}")
        print(f"    Min:  {min_sim:.4f}")
        print(f"    Std:  {std_sim:.4f}")
        print(f"  Effective diversity: {eff_diversity:.4f}")
        print(f"  Highly similar pairs (>0.9): {high_sim_pairs}/{len(off_diag)}")

        # Warnings
        if avg_sim > 0.9:
            print(f"  ğŸ”´ CRITICAL: Patterns are almost identical (avg sim {avg_sim:.3f})")
            print(f"     â†’ No diversity from initialization")
        elif avg_sim > 0.7:
            print(f"  âš ï¸  WARNING: Patterns are very similar (avg sim {avg_sim:.3f})")
        else:
            print(f"  âœ… Patterns have reasonable diversity")

        results[f'layer_{layer_idx}'] = {
            'mean_similarity': float(avg_sim),
            'max_similarity': float(max_sim),
            'min_similarity': float(min_sim),
            'std_similarity': float(std_sim),
            'effective_diversity': float(eff_diversity),
            'high_sim_pairs': int(high_sim_pairs),
            'total_pairs': int(len(off_diag)),
        }

    return results


def analyze_neuron_pattern_mapping_quality(model, dataloader, device, n_layers, tokenizer=None, num_batches=50):
    """ë‰´ëŸ° â†’ íŒ¨í„´ ë§¤í•‘ì˜ ì¼ê´€ì„± ë¶„ì„"""
    print("\n" + "="*70)
    print("NEURON-PATTERN MAPPING QUALITY")
    print("="*70)

    model.eval()

    # Collect neuron-pattern mappings
    neuron_to_patterns = [defaultdict(lambda: defaultdict(int)) for _ in range(n_layers)]

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, total=num_batches, desc="Collecting neuron-pattern mappings")):
            if batch_idx >= num_batches:
                break

            input_ids = batch['input_ids'].to(device)

            if tokenizer is not None:
                input_ids, _ = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)

            _, selected_neurons = model(input_ids, return_activations=True)

            for layer_idx in range(n_layers):
                neurons = selected_neurons[layer_idx]  # [B, S, k]
                patterns = pattern_weights[layer_idx]  # [B, S, n_patterns]

                B, S, k = neurons.shape

                for b in range(B):
                    for s in range(S):
                        # ë‰´ëŸ° ì¡°í•©ì„ tupleë¡œ
                        neuron_set = tuple(sorted(neurons[b, s].cpu().tolist()))

                        # ì„ íƒëœ íŒ¨í„´ (top-1)
                        pattern_id = patterns[b, s].argmax().item()

                        neuron_to_patterns[layer_idx][neuron_set][pattern_id] += 1

    # Analysis
    results = {}

    print("\n" + "-" * 70)
    print("Neuron Set â†’ Pattern Mapping Consistency:")
    print("-" * 70)

    for layer_idx in range(n_layers):
        mappings = neuron_to_patterns[layer_idx]

        if not mappings:
            continue

        # ê° ë‰´ëŸ° ì¡°í•©ì´ ëª‡ ê°œì˜ ë‹¤ë¥¸ íŒ¨í„´ì„ ì„ íƒí•˜ë‚˜?
        pattern_counts = []
        consistencies = []

        for neuron_set, pattern_dist in mappings.items():
            num_patterns = len(pattern_dist)
            pattern_counts.append(num_patterns)

            # Consistency: ê°€ì¥ ë§ì´ ì„ íƒëœ íŒ¨í„´ì˜ ë¹„ìœ¨
            total = sum(pattern_dist.values())
            max_count = max(pattern_dist.values())
            consistency = max_count / total if total > 0 else 0
            consistencies.append(consistency)

        avg_patterns = np.mean(pattern_counts)
        max_patterns = np.max(pattern_counts)

        # 1:1 ë§¤í•‘ ë¹„ìœ¨
        one_to_one = sum(1 for c in pattern_counts if c == 1)
        one_to_one_ratio = one_to_one / len(mappings) if len(mappings) > 0 else 0

        avg_consistency = np.mean(consistencies)

        print(f"\nLayer {layer_idx}:")
        print(f"  Unique neuron sets: {len(mappings):,}")
        print(f"  Avg patterns per neuron set: {avg_patterns:.2f}")
        print(f"  Max patterns per neuron set: {max_patterns}")
        print(f"  One-to-one mappings: {one_to_one:,}/{len(mappings):,} ({one_to_one_ratio*100:.1f}%)")
        print(f"  Avg consistency: {avg_consistency:.4f}")

        # Interpretation
        if one_to_one_ratio > 0.99:
            print(f"  ğŸ”´ DETERMINISTIC: >99% one-to-one mapping!")
            print(f"     â†’ Patterns completely determined by neurons")
            print(f"     â†’ No context-dependent selection")
        elif one_to_one_ratio > 0.9:
            print(f"  âš ï¸  HIGHLY DETERMINISTIC: {one_to_one_ratio*100:.1f}% one-to-one")
        elif avg_consistency > 0.9:
            print(f"  âš ï¸  MOSTLY CONSISTENT: Same neurons â†’ mostly same pattern")
        else:
            print(f"  âœ… Context-dependent: Same neurons can select different patterns")

        results[f'layer_{layer_idx}'] = {
            'unique_neuron_sets': int(len(mappings)),
            'avg_patterns_per_set': float(avg_patterns),
            'max_patterns_per_set': int(max_patterns),
            'one_to_one_mappings': int(one_to_one),
            'one_to_one_ratio': float(one_to_one_ratio),
            'avg_consistency': float(avg_consistency),
        }

    return results


def analyze_pattern_ffn_impact(model, dataloader, device, n_layers, tokenizer=None, num_batches=30):
    """íŒ¨í„´ì´ FFN ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì‹¤ì œ ì˜í–¥ ì¸¡ì •"""
    print("\n" + "="*70)
    print("PATTERN FFN IMPACT ANALYSIS")
    print("="*70)

    model.eval()

    if hasattr(model, '_orig_mod'):
        m = model._orig_mod
    else:
        m = model

    # Save original pattern queries
    original_queries = [
        layer.neuron_interaction.pattern_queries.data.clone()
        for layer in m.layers
    ]

    def measure_output_diff(mode='uniform'):
        """Measure output difference with modified patterns"""
        total_diff = 0.0
        total_norm = 0.0
        layer_diffs = [0.0 for _ in range(n_layers)]
        layer_norms = [0.0 for _ in range(n_layers)]

        with torch.no_grad():
            for batch_idx, batch in enumerate(dataloader):
                if batch_idx >= num_batches:
                    break

                input_ids = batch['input_ids'].to(device)

                if tokenizer is not None:
                    input_ids, _ = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)

                # Normal forward
                logits_normal = model(input_ids)

                # Modified patterns
                if mode == 'uniform':
                    for layer in m.layers:
                        layer.neuron_interaction.pattern_queries.data.fill_(0.01)
                elif mode == 'random':
                    for layer in m.layers:
                        layer.neuron_interaction.pattern_queries.data.normal_(0, 0.02)

                logits_modified = model(input_ids)

                # Restore
                for layer, orig in zip(m.layers, original_queries):
                    layer.neuron_interaction.pattern_queries.data.copy_(orig)

                # Measure difference
                diff = (logits_normal - logits_modified).abs().mean().item()
                norm = logits_normal.abs().mean().item()

                total_diff += diff
                total_norm += norm

        avg_diff = total_diff / num_batches
        avg_norm = total_norm / num_batches
        relative_diff = avg_diff / avg_norm if avg_norm > 0 else 0

        return avg_diff, relative_diff

    # Test different modifications
    print("\nLogits change when patterns are modified:")
    print("-" * 70)

    uniform_diff, uniform_rel = measure_output_diff('uniform')
    print(f"\nUniform patterns:")
    print(f"  Absolute diff: {uniform_diff:.6f}")
    print(f"  Relative diff: {uniform_rel*100:.2f}%")

    if uniform_rel < 0.01:
        print(f"  ğŸ”´ CRITICAL: <1% change - patterns have NO effect!")
    elif uniform_rel < 0.05:
        print(f"  âš ï¸  WARNING: <5% change - patterns have minimal effect")
    else:
        print(f"  âœ… Patterns have significant effect")

    random_diff, random_rel = measure_output_diff('random')
    print(f"\nRandom patterns:")
    print(f"  Absolute diff: {random_diff:.6f}")
    print(f"  Relative diff: {random_rel*100:.2f}%")

    # Layer-wise analysis
    print(f"\n{'='*70}")
    print("Layer-wise Pattern Impact:")
    print("-" * 70)

    layer_impacts = []

    for ablate_layer_idx in range(n_layers):
        # Uniform only this layer
        for layer_idx, layer in enumerate(m.layers):
            if layer_idx == ablate_layer_idx:
                layer.neuron_interaction.pattern_queries.data.fill_(0.01)

        layer_diff, layer_rel = measure_output_diff('uniform')

        # Restore
        for layer, orig in zip(m.layers, original_queries):
            layer.neuron_interaction.pattern_queries.data.copy_(orig)

        layer_impacts.append(layer_rel)

        print(f"  L{ablate_layer_idx}: {layer_rel*100:.2f}% change")

    # Identify most important layer
    max_impact_layer = np.argmax(layer_impacts)
    print(f"\n  Most impactful layer: L{max_impact_layer} ({layer_impacts[max_impact_layer]*100:.2f}%)")

    results = {
        'uniform_absolute_diff': float(uniform_diff),
        'uniform_relative_diff': float(uniform_rel),
        'random_absolute_diff': float(random_diff),
        'random_relative_diff': float(random_rel),
        'layer_impacts': {
            f'layer_{i}': float(impact)
            for i, impact in enumerate(layer_impacts)
        },
        'most_impactful_layer': int(max_impact_layer),
    }

    return results


def diagnose_bottleneck(all_results, n_layers):
    """í†µí•© ì§„ë‹¨: bottleneck ìœ„ì¹˜ íŒŒì•…"""
    print("\n" + "="*70)
    print("BOTTLENECK DIAGNOSIS SUMMARY")
    print("="*70)

    issues = defaultdict(list)

    # Check each layer
    for layer_idx in range(n_layers):
        layer_key = f'layer_{layer_idx}'

        # 1. Gradient flow
        if 'bottleneck' in all_results:
            grad_mean = all_results['bottleneck'][layer_key]['router_grad_mean']
            if grad_mean < 0.001:
                issues[layer_idx].append(f"Weak gradient ({grad_mean:.6f})")

        # 2. Neuron usage
        if 'neuron_usage' in all_results:
            usage_ratio = all_results['neuron_usage'][layer_key]['usage_ratio']
            if usage_ratio < 0.7:
                issues[layer_idx].append(f"Low neuron usage ({usage_ratio:.1%})")

        # 3. Pattern usage
        if 'pattern_usage' in all_results:
            pattern_gini = all_results['pattern_usage'][layer_key]['gini_coefficient']
            if pattern_gini > 0.7:
                issues[layer_idx].append(f"Pattern collapse (Gini={pattern_gini:.2f})")

        # 4. Representation rank
        if 'information_flow' in all_results:
            rank_key = f'layer_{layer_idx}_rank'
            if rank_key in all_results['information_flow']:
                rank_ratio = all_results['information_flow'][rank_key]['rank_ratio']
                if rank_ratio < 0.4:
                    issues[layer_idx].append(f"Low diversity (rank={rank_ratio:.1%})")

        # 5. Task pressure
        if 'task_pressure' in all_results:
            importance = all_results['task_pressure'][layer_key]['importance']
            if importance > 0.8:
                issues[layer_idx].append(f"High task pressure (importance={importance:.2f})")

    # Report
    print("\nğŸ” Bottleneck Indicators:")
    print("-" * 70)

    bottleneck_layers = []
    for layer_idx in range(n_layers):
        if issues[layer_idx]:
            print(f"\n  Layer {layer_idx}: ğŸ”´ ISSUES FOUND")
            for issue in issues[layer_idx]:
                print(f"    - {issue}")
            bottleneck_layers.append(layer_idx)
        else:
            print(f"\n  Layer {layer_idx}: âœ… OK")

    # Summary
    print(f"\n{'='*70}")
    if bottleneck_layers:
        print(f"âš ï¸  Potential bottlenecks detected in layers: {bottleneck_layers}")
        print(f"\nRecommendations:")
        for layer_idx in bottleneck_layers:
            print(f"\n  Layer {layer_idx}:")
            if any("gradient" in i for i in issues[layer_idx]):
                print(f"    â†’ Increase regularization weights")
                print(f"    â†’ Check learning rate")
            if any("usage" in i for i in issues[layer_idx]):
                print(f"    â†’ Increase capacity (more neurons/patterns)")
                print(f"    â†’ Strengthen diversity regularization")
            if any("collapse" in i for i in issues[layer_idx]):
                print(f"    â†’ Increase load balancing weight (0.1 â†’ 0.2)")
                print(f"    â†’ Add pattern dropout")
            if any("diversity" in i for i in issues[layer_idx]):
                print(f"    â†’ Increase orthogonality weight (0.01 â†’ 0.02)")
                print(f"    â†’ Consider hard orthogonalization")
            if any("pressure" in i for i in issues[layer_idx]):
                print(f"    â†’ This layer is critical - consider increasing its capacity")
    else:
        print(f"âœ… No critical bottlenecks detected!")

    return {
        'bottleneck_layers': bottleneck_layers,
        'issues': dict(issues)
    }


def main():
    parser = argparse.ArgumentParser(description='Analyze DAWN checkpoint')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to checkpoint folder or .pt file')
    parser.add_argument('--num_batches', type=int, default=100,
                       help='Number of batches to analyze (default: 100, use --quick for faster analysis)')
    parser.add_argument('--skip_bottleneck', action='store_true',
                       help='Skip bottleneck analysis (faster)')
    parser.add_argument('--quick', action='store_true',
                       help='Quick analysis mode (10 batches, skip heavy analyses)')
    args = parser.parse_args()

    # Quick mode overrides
    if args.quick:
        args.num_batches = 10
        args.skip_bottleneck = True
        print("âš¡ QUICK MODE: Using 10 batches, skipping bottleneck analyses")

    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì²˜ë¦¬
    checkpoint_path = Path(args.checkpoint)

    if checkpoint_path.is_dir():
        # í´ë”ì¸ ê²½ìš° best_model.pt ì°¾ê¸°
        best_model_path = checkpoint_path / 'best_model.pt'
        config_path = checkpoint_path / 'config.json'
        output_dir = checkpoint_path / 'analysis'
    else:
        # íŒŒì¼ì¸ ê²½ìš°
        best_model_path = checkpoint_path
        config_path = checkpoint_path.parent / 'config.json'
        output_dir = checkpoint_path.parent / 'analysis'

    if not best_model_path.exists():
        print(f"âŒ Checkpoint not found: {best_model_path}")
        return

    output_dir.mkdir(exist_ok=True)

    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Config ë¡œë“œ
    print(f"\nLoading config: {config_path}")
    with open(config_path, 'r') as f:
        cfg = json.load(f)

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    print(f"Loading checkpoint: {best_model_path}")
    checkpoint = torch.load(best_model_path, map_location=device)

    # ë²„ì „ ê°ì§€
    checkpoint_version = checkpoint.get('model_version', 'unknown')
    current_version = DAWN.__version__
    print(f"\nğŸ“Œ Checkpoint version: {checkpoint_version}")
    print(f"ğŸ“Œ Current model version: {current_version}")

    if checkpoint_version == 'unknown':
        print("   âš ï¸  Old checkpoint (pre-versioning)")
    elif checkpoint_version != current_version:
        print(f"   âš ï¸  Version mismatch - will attempt backward compatible loading")

    # ëª¨ë¸ ìƒì„±
    print("\nCreating model...")
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    vocab_size = tokenizer.vocab_size

    # Backward compatibility for config parameters
    neuron_k = cfg['model'].get('neuron_k', cfg['model'].get('k', 8))

    # v6.0: Model creation (backward compatible)
    model = DAWN(
        vocab_size=vocab_size,
        hidden_dim=cfg['model']['d_model'],
        num_layers=cfg['model']['n_layers'],
        n_heads=cfg['model']['n_heads'],
        n_neurons=cfg['model']['n_neurons'],
        neuron_rank=cfg['model'].get('neuron_rank', 16),  # v6.0: not used, backward compat
        neuron_k=neuron_k,
        n_basis=cfg['model'].get('n_basis', 8),
        basis_rank=cfg['model'].get('basis_rank', 64),
        mod_rank=cfg['model'].get('mod_rank', None),  # v6.0: not used
        d_ff=cfg['model'].get('d_ff', None),
        max_seq_len=cfg['model']['max_seq_len'],
        dropout=cfg['model']['dropout']
    )

    # Load state dict (handle torch.compile() prefix)
    state_dict = checkpoint['model_state_dict']

    # Remove _orig_mod. prefix if present (from torch.compile)
    if any(k.startswith('_orig_mod.') for k in state_dict.keys()):
        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}

    # Load with strict=False to handle backward compatibility (connection weights)
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

    # Check if missing keys are only connection weights (expected for old checkpoints)
    connection_keys = [k for k in missing_keys if 'connection.weight' in k]
    other_missing = [k for k in missing_keys if 'connection.weight' not in k]

    if connection_keys:
        print(f"\nâš ï¸  Loading checkpoint without inter-layer connections ({len(connection_keys)} layers)")
        print("   Connection weights initialized to zero (equivalent to pre-connection model)")

    if other_missing:
        print(f"\nâš ï¸  Warning: Missing keys (not connection-related): {other_missing}")

    if unexpected_keys:
        print(f"\nâš ï¸  Warning: Unexpected keys in checkpoint: {unexpected_keys[:5]}...")

    model = model.to(device)
    model.eval()

    n_layers = cfg['model']['n_layers']
    n_neurons = cfg['model']['n_neurons']

    print(f"\nModel: {n_layers} layers, {n_neurons} neurons/layer (v5.0)")
    print(f"Validation loss: {checkpoint.get('val_loss', 'N/A')}")
    print(f"Epoch: {checkpoint.get('epoch', 'N/A')}")

    # ë°ì´í„° ë¡œë“œ
    print("\nLoading validation data...")
    _, val_loader, _ = load_data(
        cfg['data'],
        max_length=cfg['model']['max_seq_len'],
        batch_size=32
    )

    # ìˆ˜ì§‘
    print(f"\nCollecting neuron patterns from {args.num_batches} batches...")
    collector = ActivationCollector(model, n_layers)

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader, total=args.num_batches)):
            if batch_idx >= args.num_batches:
                break

            input_ids = batch['input_ids'].to(device)
            input_ids, labels = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)

            logits, all_selected = model(input_ids, return_activations=True)
            collector.collect(input_ids, labels, logits, all_selected)

    collector.finalize()

    # ë¶„ì„
    neuron_usage_results = analyze_neuron_usage(collector, n_neurons, n_layers)
    token_spec_results = analyze_token_neuron_specialization(
        collector, tokenizer, n_neurons, n_layers, top_k_tokens=100
    )
    layer_diff_results = analyze_layer_differences(neuron_usage_results)
    uncertainty_results = analyze_uncertainty_accuracy(collector, n_layers)

    # v5.0: Basis FFN Analysis ğŸ¯
    print("\n" + "="*70)
    print("ğŸ¯ v6.0 SPECIFIC ANALYSES")
    print("="*70)

    n_basis = cfg['model'].get('n_basis', 8)

    # v6.0: Use model's built-in basis analysis
    if hasattr(model, 'analyze_basis_usage'):
        print("\n  Using v6.0 built-in basis analysis...")
        basis_usage_results = model.analyze_basis_usage()
    else:
        # Fallback to old analysis for v5.x models
        basis_usage_results = analyze_basis_usage(model, collector, n_layers, n_basis)

    basis_composition_results = analyze_neuron_basis_composition(model, n_layers, n_basis, n_neurons)
    basis_orthogonality_results = analyze_basis_orthogonality(model, n_layers)

    # â­ Pattern analysis (v5.0: disabled - no patterns in v5.0)
    # pattern_usage_results = analyze_pattern_usage(collector, n_patterns, n_layers)
    # pattern_collapse_results = analyze_pattern_collapse_detail(collector, n_patterns, n_layers)
    # neuron_pattern_corr_results = analyze_neuron_pattern_correlation(collector, n_layers)
    # confidence_results = analyze_selection_confidence(collector, n_layers)
    # position_pattern_results = analyze_position_patterns(collector, n_layers)

    # ğŸ§¬ Neuron diversity ë¶„ì„ (v5.0: disabled for now)
    # diversity_results = analyze_neuron_diversity(model, n_layers)

    # ğŸ¤ Co-activation ë¶„ì„
    coactivation_results = analyze_neuron_coactivation(collector, n_neurons, n_layers)

    print("\nâœ… Analysis complete! (v5.0: pattern analyses disabled)")

    # ğŸ”¬ Bottleneck ë¶„ì„ (v5.0: disabled for simplicity)
    if False and not args.skip_bottleneck:
        print("\n" + "="*70)
        print("ğŸš€ RUNNING UNIFIED BOTTLENECK ANALYSIS (OPTIMIZED)")
        print("="*70)
        print(f"Processing {args.num_batches} batches for all analyses in parallel...")

        # âš¡ ëª¨ë¸ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë¶„ì„ ë¨¼ì € (ì¦‰ì‹œ ì‹¤í–‰)
        pattern_diversity_results = analyze_pattern_diversity(model, n_layers)

        # âš¡ ëª¨ë“  ë¶„ì„ì— ë™ì¼í•œ ë°°ì¹˜ ìˆ˜ ì‚¬ìš©
        num_batches = args.num_batches

        bottleneck_results = analyze_layer_bottleneck(model, val_loader, device, num_batches=num_batches, tokenizer=tokenizer)
        information_flow_results = analyze_information_flow(model, val_loader, device, num_batches=num_batches)
        task_pressure_results = analyze_task_pressure(model, val_loader, device, num_batches=num_batches//3, tokenizer=tokenizer)

        # ğŸ” Deep dive analyses
        gradient_detailed_results = analyze_gradient_flow_detailed(model, val_loader, device, num_batches=num_batches//3, tokenizer=tokenizer)
        pattern_necessity_results = analyze_pattern_necessity(model, val_loader, device, tokenizer=tokenizer, num_batches=num_batches//2)
        regularization_impact_results = analyze_regularization_impact(model, val_loader, device, tokenizer=tokenizer, num_batches=num_batches)

        # ğŸ¯ Pattern-specific deep dives
        neuron_pattern_mapping_results = analyze_neuron_pattern_mapping_quality(model, val_loader, device, n_layers, tokenizer=tokenizer, num_batches=num_batches)
        pattern_ffn_impact_results = analyze_pattern_ffn_impact(model, val_loader, device, n_layers, tokenizer=tokenizer, num_batches=num_batches//3)

        print(f"âœ… All bottleneck analyses completed!")
    else:
        print("\nâ© Skipping bottleneck analysis (use --skip_bottleneck to skip)")
        bottleneck_results = {}
        information_flow_results = {}
        task_pressure_results = {}
        gradient_detailed_results = {}
        pattern_necessity_results = {}
        regularization_impact_results = {}
        pattern_diversity_results = {}
        neuron_pattern_mapping_results = {}
        pattern_ffn_impact_results = {}

    # v5.0: Simplified results (neuron-only, no patterns)
    all_results = {
        'neuron_usage': neuron_usage_results,
        'token_neuron_specialization': token_spec_results,
        'layer_differences': layer_diff_results,
        'uncertainty_accuracy': uncertainty_results,
        'neuron_coactivation': coactivation_results,
        # v5.0 Basis FFN analyses
        'basis_usage': basis_usage_results,
        'basis_composition': basis_composition_results,
        'basis_orthogonality': basis_orthogonality_results,
    }

    # ì €ì¥
    print(f"\nSaving results to: {output_dir}")

    # Convert numpy arrays to lists for JSON serialization
    def convert_to_serializable(obj):
        """Recursively convert numpy arrays to lists"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(item) for item in obj]
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        else:
            return obj

    all_results_serializable = convert_to_serializable(all_results)

    with open(output_dir / 'analysis_results.json', 'w') as f:
        json.dump(all_results_serializable, f, indent=2)
    print("  âœ“ analysis_results.json")

    # ì‹œê°í™”
    visualize_results(neuron_usage_results, output_dir)

    # ë¦¬í¬íŠ¸
    generate_report(all_results, output_dir, best_model_path)

    print("\n" + "="*70)
    print("âœ… ANALYSIS COMPLETE!")
    print("="*70)
    print(f"\nResults saved to: {output_dir}/")
    print("  - analysis_results.json")
    print("  - analysis_report.txt")
    print("  - neuron_usage_distribution.png")
    print("  - layer_statistics.png")
    print("  - neuron_heatmap.png")
    print("  - neuron_roles/ (similarity, co-activation, clustering)")
    if not args.skip_bottleneck:
        print("\nğŸ”¬ Bottleneck Diagnosis:")
        print("  - Gradient flow analysis")
        print("  - Information flow (similarity, change, rank)")
        print("  - Task pressure (layer ablation)")
        print("  - Integrated bottleneck diagnosis")
        print("\nğŸ” Deep Dive Analyses:")
        print("  - Detailed gradient flow (normalized by param count)")
        print("  - Pattern necessity (ablation tests)")
        print("  - Regularization impact (loss component analysis)")

    print("\n" + "="*70)


if __name__ == "__main__":
    main()
