"""
DAWN (Dynamic Architecture With Neurons) ì¢…í•© ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

í•™ìŠµëœ DAWN ëª¨ë¸ì˜ ìƒì„¸ ë¶„ì„:
- ë ˆì´ì–´ë³„ íŠ¹ì„± ë¶„ì„
- ì„±ëŠ¥ breakdown
- ì˜ˆì¸¡ ë¶„í¬ ë¶„ì„

Usage:
    python scripts/analyze_dawn.py --checkpoint path/to/checkpoint.pt
    python scripts/analyze_dawn.py --checkpoint path/to/checkpoint.pt --output results.json
"""

import sys
import os
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import argparse
from tqdm import tqdm
from collections import Counter, defaultdict
from datetime import datetime

from models.model import HierarchicalLanguageModel
from utils.training import CheckpointManager


# ============================================================
# Data Loading
# ============================================================

def load_data_from_config(config_path, batch_size=64):
    """Configì—ì„œ ë°ì´í„° ë¡œë“œ"""
    import yaml
    import pickle
    from transformers import AutoTokenizer
    from torch.utils.data import DataLoader, Dataset

    # Load config
    with open(config_path, 'r') as f:
        cfg = yaml.safe_load(f)

    data_cfg = cfg['data']
    model_cfg = cfg['model']

    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

    # Load validation texts
    val_path = os.path.join(data_cfg['base_dir'], data_cfg['val_file'])
    if not os.path.exists(val_path):
        raise FileNotFoundError(f"Validation data not found: {val_path}")

    with open(val_path, 'rb') as f:
        val_texts = pickle.load(f)

    # Simple dataset
    class TextDataset(Dataset):
        def __init__(self, texts, tokenizer, max_length):
            self.texts = texts
            self.tokenizer = tokenizer
            self.max_length = max_length

        def __len__(self):
            return len(self.texts)

        def __getitem__(self, idx):
            encoding = self.tokenizer(
                self.texts[idx],
                truncation=True,
                max_length=self.max_length,
                padding='max_length',
                return_tensors='pt'
            )
            return {
                'input_ids': encoding['input_ids'].squeeze(0),
                'attention_mask': encoding['attention_mask'].squeeze(0)
            }

    val_dataset = TextDataset(val_texts, tokenizer, model_cfg['max_seq_len'])
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)

    return val_loader, tokenizer, cfg


# ============================================================
# Model Loading
# ============================================================

def load_checkpoint(checkpoint_path, device='cuda'):
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    checkpoint_dir = Path(checkpoint_path).parent

    # Config ë¡œë“œ (config.json íŒŒì¼ì—ì„œ)
    config_path = checkpoint_dir / 'config.json'
    if config_path.exists():
        with open(config_path, 'r') as f:
            config = json.load(f)
        print(f"Loaded config from: {config_path}")
    else:
        # ê¸°ë³¸ê°’ ì‚¬ìš©
        print("Config file not found, using defaults")
        config = {
            'model': {
                'd_model': 512,
                'n_heads': 8,
                'n_layers': 6,
                'max_seq_len': 128,
                'n_input': 128,
                'n_process': 256,
                'dropout': 0.1
            }
        }

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    else:
        state_dict = checkpoint

    # vocab_sizeë¥¼ state_dictì—ì„œ ì¶”ë¡  (token_embedding.weight shape)
    vocab_size = 30522  # Default for bert-base-uncased
    if 'token_embedding.weight' in state_dict:
        vocab_size = state_dict['token_embedding.weight'].shape[0]
        print(f"Inferred vocab_size from state_dict: {vocab_size}")

    # configê°€ ìƒˆ í˜•ì‹ì¸ì§€ í™•ì¸
    if 'model' in config:
        model_cfg = config['model']
    else:
        # êµ¬ í˜•ì‹ í˜¸í™˜
        model_cfg = config

    # ëª¨ë¸ ìƒì„±
    model = HierarchicalLanguageModel(
        vocab_size=vocab_size,
        d_model=model_cfg.get('d_model', 512),
        n_heads=model_cfg.get('n_heads', 8),
        n_layers=model_cfg.get('n_layers', 6),
        max_seq_len=model_cfg.get('max_seq_len', 128),
        n_input=model_cfg.get('n_input', 128),
        n_process=model_cfg.get('n_process', 256),
        dropout=model_cfg.get('dropout', 0.1)
    )

    model.load_state_dict(state_dict)

    model = model.to(device)
    model.eval()

    # ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì¶œë ¥
    if 'epoch' in checkpoint:
        print(f"Checkpoint epoch: {checkpoint['epoch']}")
    if 'loss' in checkpoint:
        print(f"Checkpoint loss: {checkpoint['loss']:.4f}")
    if 'metrics' in checkpoint:
        metrics = checkpoint['metrics']
        if 'val_acc' in metrics:
            print(f"Checkpoint val_acc: {metrics['val_acc']:.4f}")

    return model, config


# ============================================================
# Analysis Functions
# ============================================================

def analyze_routing_patterns(model, val_loader, device):
    """Router íŒ¨í„´ ë¶„ì„ (simplified for new architecture)"""
    print("\nğŸ“Š Analyzing Routing Patterns...")

    # New architecture doesn't expose routing stats directly
    # Return basic info
    results = {
        'n_layers': len(model.layers),
        'layers': {}
    }

    for layer_idx in range(len(model.layers)):
        results['layers'][layer_idx] = {
            'info': 'Routing analysis not available in simplified architecture'
        }

    return results


def analyze_neuron_usage(model, val_loader, device):
    """ë‰´ëŸ° ì‚¬ìš© ë¶„í¬ ë¶„ì„ (simplified for new architecture)"""
    print("\nğŸ§  Analyzing Neuron Usage...")

    # New architecture doesn't track usage stats
    results = {'layers': {}}

    for layer_idx in range(len(model.layers)):
        results['layers'][layer_idx] = {
            'info': 'Usage analysis not available in simplified architecture'
        }

    return results


def analyze_neuron_specialization(model, val_loader, tokenizer, device, max_batches=50):
    """ë‰´ëŸ° íŠ¹í™” ë¶„ì„ (simplified for new architecture)"""
    print("\nğŸ’ Analyzing Neuron Specialization...")

    # New architecture doesn't expose neuron routing for analysis
    results = {
        'info': 'Specialization analysis not available in simplified architecture',
        'total_analyzed': 0,
        'avg_diversity': 0,
        'std_diversity': 0,
        'specialized_count': 0,
        'specialized_neurons': []
    }

    return results


def analyze_layer_differences(model, val_loader, device):
    """ë ˆì´ì–´ë³„ íŠ¹ì„± ë¶„ì„"""
    print("\nğŸ“ˆ Analyzing Layer Differences...")

    layer_outputs = defaultdict(list)

    for batch in tqdm(val_loader, desc="Layer analysis"):
        input_ids = batch['input_ids'].to(device)

        with torch.no_grad():
            # Embedding
            B, S = input_ids.shape
            token_emb = model.token_embedding(input_ids)
            positions = torch.arange(S, device=device).unsqueeze(0).expand(B, -1)
            pos_emb = model.position_embedding(positions)
            x = model.dropout(token_emb + pos_emb)

            # ê° ë ˆì´ì–´ í†µê³¼
            for layer_idx, layer in enumerate(model.layers):
                x = layer(x)

                # ì¶œë ¥ í†µê³„
                layer_outputs[layer_idx].append({
                    'norm': x.norm().item(),
                    'mean': x.mean().item(),
                    'std': x.std().item()
                })

    # í‰ê·  ê³„ì‚°
    results = {'layers': {}}

    for layer_idx in range(len(model.layers)):
        outputs = layer_outputs[layer_idx]
        results['layers'][layer_idx] = {
            'avg_norm': np.mean([o['norm'] for o in outputs]),
            'avg_mean': np.mean([o['mean'] for o in outputs]),
            'avg_std': np.mean([o['std'] for o in outputs])
        }

    return results


def analyze_performance(model, val_loader, device):
    """ì„±ëŠ¥ ì„¸ë¶€ ë¶„ì„"""
    print("\nğŸ¯ Analyzing Performance...")

    all_losses = []
    all_corrects = []

    for batch in tqdm(val_loader, desc="Performance analysis"):
        input_ids = batch['input_ids'].to(device)
        labels = input_ids.clone()

        with torch.no_grad():
            outputs = model(input_ids, labels=labels)
            logits = outputs['logits']

            # Per-token loss
            loss_fct = nn.CrossEntropyLoss(reduction='none', ignore_index=-100)
            per_token_loss = loss_fct(
                logits.view(-1, logits.size(-1)),
                labels.view(-1)
            )

            # Accuracy
            preds = logits.argmax(dim=-1)
            correct = (preds == labels).float()

            all_losses.extend(per_token_loss.cpu().tolist())
            all_corrects.extend(correct.view(-1).cpu().tolist())

    losses = np.array(all_losses)
    corrects = np.array(all_corrects)

    # Percentile ë¶„ì„
    easy_threshold = np.percentile(losses, 25)
    hard_threshold = np.percentile(losses, 75)

    easy_mask = losses < easy_threshold
    hard_mask = losses > hard_threshold

    results = {
        'overall_loss': float(np.mean(losses)),
        'overall_acc': float(np.mean(corrects)),
        'easy_samples_acc': float(np.mean(corrects[easy_mask])) if easy_mask.sum() > 0 else 0,
        'hard_samples_acc': float(np.mean(corrects[hard_mask])) if hard_mask.sum() > 0 else 0,
        'loss_percentiles': {
            'p25': float(easy_threshold),
            'p50': float(np.percentile(losses, 50)),
            'p75': float(hard_threshold)
        }
    }

    return results


def analyze_aux_loss_components(model, val_loader, device):
    """Aux loss êµ¬ì„± ìš”ì†Œ ë¶„ì„ (simplified for new architecture)"""
    print("\nâš–ï¸  Analyzing Loss Components...")

    total_main_loss = 0
    n_batches = 0

    for batch in tqdm(val_loader, desc="Loss analysis"):
        input_ids = batch['input_ids'].to(device)
        labels = input_ids.clone()

        with torch.no_grad():
            outputs = model(input_ids, labels=labels)
            main_loss = outputs['loss']

            total_main_loss += main_loss.item()
            n_batches += 1

    results = {
        'avg_main_loss': total_main_loss / n_batches,
        'avg_aux_loss': 0.0,
        'aux_to_main_ratio': 0.0
    }

    return results


# ============================================================
# Main Analysis
# ============================================================

def comprehensive_analysis(model, val_loader, tokenizer, device):
    """DAWN ëª¨ë¸ ì¢…í•© ë¶„ì„"""
    print("=" * 60)
    print("DAWN Comprehensive Analysis")
    print("=" * 60)

    results = {
        'timestamp': datetime.now().isoformat(),
        'model_config': model.get_model_stats()
    }

    # 1. Routing Patterns (simplified)
    routing_results = analyze_routing_patterns(model, val_loader, device)
    results['routing'] = routing_results

    print("\nğŸ“Š ROUTING STATISTICS")
    print("-" * 40)
    print(f"  Number of layers: {routing_results['n_layers']}")
    print("  (Detailed routing analysis not available in simplified architecture)")

    # 2. Neuron Usage (simplified)
    usage_results = analyze_neuron_usage(model, val_loader, device)
    results['usage'] = usage_results

    print("\nğŸ§  NEURON USAGE")
    print("-" * 40)
    print("  (Usage analysis not available in simplified architecture)")

    # 3. Specialization (simplified)
    spec_results = analyze_neuron_specialization(model, val_loader, tokenizer, device)
    results['specialization'] = spec_results

    print("\nğŸ’ NEURON SPECIALIZATION")
    print("-" * 40)
    print("  (Specialization analysis not available in simplified architecture)")

    # 4. Layer Differences
    layer_results = analyze_layer_differences(model, val_loader, device)
    results['layers'] = layer_results

    print("\nğŸ“ˆ LAYER-WISE ANALYSIS")
    print("-" * 40)
    for layer_idx, stats in layer_results['layers'].items():
        print(f"  Layer {layer_idx}: norm={stats['avg_norm']:.2f}, "
              f"std={stats['avg_std']:.4f}")

    # 5. Performance
    perf_results = analyze_performance(model, val_loader, device)
    results['performance'] = perf_results

    print("\nğŸ¯ PERFORMANCE BREAKDOWN")
    print("-" * 40)
    print(f"  Overall accuracy: {perf_results['overall_acc']*100:.2f}%")
    print(f"  Easy samples (top 25%): {perf_results['easy_samples_acc']*100:.2f}%")
    print(f"  Hard samples (bottom 25%): {perf_results['hard_samples_acc']*100:.2f}%")

    # 6. Aux Loss
    aux_results = analyze_aux_loss_components(model, val_loader, device)
    results['aux_loss'] = aux_results

    print("\nâš–ï¸  AUX LOSS ANALYSIS")
    print("-" * 40)
    print(f"  Main loss: {aux_results['avg_main_loss']:.4f}")
    print(f"  Aux loss: {aux_results['avg_aux_loss']:.6f}")
    print(f"  Aux/Main ratio: {aux_results['aux_to_main_ratio']:.4f}")

    print("\n" + "=" * 60)

    return results


def main():
    parser = argparse.ArgumentParser(description='DAWN Model Analysis')

    parser.add_argument('--checkpoint', type=str, required=True,
                        help='Path to model checkpoint')
    parser.add_argument('--config', type=str, default='configs/train_config.yaml',
                        help='Path to config file for data loading')
    parser.add_argument('--output', type=str, default=None,
                        help='Output JSON file path')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='Batch size for analysis')

    args = parser.parse_args()

    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Load model
    print(f"\nLoading checkpoint: {args.checkpoint}")
    model, config = load_checkpoint(args.checkpoint, device)
    print(f"Model loaded successfully!")

    # Load data from config
    config_path = Path(PROJECT_ROOT) / args.config
    print(f"\nLoading validation data from config: {config_path}")
    val_loader, tokenizer, _ = load_data_from_config(
        config_path=config_path,
        batch_size=args.batch_size
    )
    print(f"Loaded {len(val_loader)} batches")

    # Run analysis
    results = comprehensive_analysis(model, val_loader, tokenizer, device)

    # Save results
    if args.output:
        output_path = args.output
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"dawn_analysis_{timestamp}.json"

    # Convert numpy arrays to lists for JSON serialization
    def convert_to_serializable(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, dict):
            return {k: convert_to_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_to_serializable(v) for v in obj]
        return obj

    results = convert_to_serializable(results)

    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nâœ… Analysis complete! Results saved to: {output_path}")


if __name__ == '__main__':
    main()
