"""
DAWN Checkpoint Comprehensive Analysis
Dynamic Neuron Transformer ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ë¶„ì„ í•­ëª©:
1. ë‰´ëŸ° í™œì„±í™” íŒ¨í„´ (ì‚¬ìš© ë¹ˆë„, Gini coefficient, Entropy)
2. íŒ¨í„´ í™œì„±í™” ë¶„ì„ (ë‹¤ì–‘ì„±, ì‚¬ìš©ë¥ )
3. í† í°-ë‰´ëŸ° ì „ë¬¸í™” (íŠ¹ì • í† í°ì´ íŠ¹ì • ë‰´ëŸ° ì„ íƒ?)
4. Layerë³„ ì°¨ì´ (KL divergence, Cosine similarity)
5. ì •í™•ë„-ë¶ˆí™•ì‹¤ì„± ê´€ê³„
6. ì¢…í•© ì‹œê°í™” ë° ë¦¬í¬íŠ¸

Usage:
    python scripts/analyze_dawn.py --checkpoint /path/to/checkpoint_folder
"""

import sys
import os
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import argparse
from tqdm import tqdm
import json
from collections import defaultdict, Counter
from scipy.stats import entropy
import yaml

from models.model import DAWN
from utils.data import load_data, apply_mlm_masking, MLM_CONFIG
from transformers import BertTokenizer


# ============================================================
# ë°ì´í„° ìˆ˜ì§‘
# ============================================================

class ActivationCollector:
    """ë‰´ëŸ°/íŒ¨í„´ ì„ íƒ íŒ¨í„´ ìˆ˜ì§‘ (í™•ì¥íŒ)"""
    def __init__(self, model, n_layers):
        self.model = model
        self.n_layers = n_layers

        # ë‰´ëŸ° ì„ íƒ ê¸°ë¡
        self.neuron_selections = [[] for _ in range(n_layers)]

        # íŒ¨í„´ ì„ íƒ ê¸°ë¡ â­ NEW
        self.pattern_selections = [[] for _ in range(n_layers)]

        # ìœ„ì¹˜ë³„ ë‰´ëŸ° ì„ íƒ â­ NEW
        self.position_neuron_map = defaultdict(lambda: [[] for _ in range(n_layers)])

        # í† í°ë³„ ë‰´ëŸ° ì„ íƒ
        self.token_neuron_map = defaultdict(lambda: [[] for _ in range(n_layers)])

        # ì˜ˆì¸¡ ì •í™•ë„ë³„ íŒ¨í„´
        self.correct_selections = [[] for _ in range(n_layers)]
        self.incorrect_selections = [[] for _ in range(n_layers)]

    def collect(self, input_ids, labels, logits, all_selected, all_patterns=None):
        """í•œ ë°°ì¹˜ì˜ ì„ íƒ íŒ¨í„´ ìˆ˜ì§‘ (ìµœì í™”)"""
        B, S = input_ids.shape

        # ì˜ˆì¸¡ ì •í™•ë„
        predictions = logits.argmax(dim=-1)  # [B, S]
        correct_mask = (predictions == labels) & (labels != -100)  # [B, S]

        # CPUë¡œ í•œ ë²ˆì— ì´ë™ (ë§¤ ë£¨í”„ë§ˆë‹¤ í•˜ì§€ ì•Šê³ )
        input_ids_cpu = input_ids.cpu()

        for layer_idx, selected_idx in enumerate(all_selected):
            # selected_idx: [B, S, k]

            # CPUë¡œ í•œ ë²ˆì— ì´ë™
            selected_cpu = selected_idx.cpu()

            # 1. ì „ì²´ ë‰´ëŸ° ì„ íƒ ê¸°ë¡
            self.neuron_selections[layer_idx].append(selected_cpu)

            # 2. íŒ¨í„´ ì„ íƒ ê¸°ë¡
            if all_patterns is not None:
                pattern_weights = all_patterns[layer_idx]  # [B, S, n_patterns]
                self.pattern_selections[layer_idx].append(pattern_weights.cpu())

            # 3. í† í°ë³„ + ìœ„ì¹˜ë³„ ë‰´ëŸ° ì„ íƒ (vectorized)
            # Flatten: [B, S, k] â†’ [B*S, k]
            selected_flat = selected_cpu.reshape(-1, selected_cpu.shape[-1])  # [B*S, k]
            tokens_flat = input_ids_cpu.reshape(-1)  # [B*S]

            # í† í°ë³„: ê° unique tokenì— ëŒ€í•´ ë‰´ëŸ° ìˆ˜ì§‘
            for token_id in tokens_flat.unique().tolist():
                mask = tokens_flat == token_id
                neurons = selected_flat[mask].reshape(-1).tolist()
                self.token_neuron_map[token_id][layer_idx].extend(neurons)

            # ìœ„ì¹˜ë³„: ê° ìœ„ì¹˜ì— ëŒ€í•´ ë‰´ëŸ° ìˆ˜ì§‘
            for s in range(S):
                # ëª¨ë“  ë°°ì¹˜ì˜ së²ˆì§¸ ìœ„ì¹˜
                neurons = selected_cpu[:, s, :].reshape(-1).tolist()
                self.position_neuron_map[s][layer_idx].extend(neurons)

            # 4. ì •í™•ë„ë³„ ë‰´ëŸ° ì„ íƒ
            correct_neurons = selected_cpu[correct_mask.cpu()]
            incorrect_neurons = selected_cpu[~correct_mask.cpu()]

            if len(correct_neurons) > 0:
                self.correct_selections[layer_idx].append(correct_neurons)
            if len(incorrect_neurons) > 0:
                self.incorrect_selections[layer_idx].append(incorrect_neurons)

    def finalize(self):
        """ìˆ˜ì§‘ ì™„ë£Œ í›„ í…ì„œ ë³‘í•©"""
        for layer_idx in range(self.n_layers):
            if self.neuron_selections[layer_idx]:
                self.neuron_selections[layer_idx] = torch.cat(
                    self.neuron_selections[layer_idx], dim=0
                )

            # íŒ¨í„´ ì •ë³´ ë³‘í•© â­ NEW
            if self.pattern_selections[layer_idx]:
                self.pattern_selections[layer_idx] = torch.cat(
                    self.pattern_selections[layer_idx], dim=0
                )

            if self.correct_selections[layer_idx]:
                self.correct_selections[layer_idx] = torch.cat(
                    self.correct_selections[layer_idx], dim=0
                )

            if self.incorrect_selections[layer_idx]:
                self.incorrect_selections[layer_idx] = torch.cat(
                    self.incorrect_selections[layer_idx], dim=0
                )


# ============================================================
# 1. ë‰´ëŸ° ì‚¬ìš© ë¶„ì„
# ============================================================

def analyze_neuron_usage(collector, n_neurons, n_layers):
    """ë‰´ëŸ° ì‚¬ìš© ë¹ˆë„ ë° ë¶„í¬ ë¶„ì„"""
    print("\n" + "="*70)
    print("1. NEURON USAGE ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        selections = collector.neuron_selections[layer_idx]

        if len(selections) == 0:
            continue

        # ë‰´ëŸ°ë³„ ì„ íƒ ë¹ˆë„
        neuron_counts = torch.bincount(
            selections.flatten(),
            minlength=n_neurons
        ).numpy()

        total_selections = neuron_counts.sum()
        neuron_freq = neuron_counts / total_selections

        # Gini coefficient (ë¶ˆê· í˜• ì¸¡ì •)
        sorted_freq = np.sort(neuron_freq)
        n = len(sorted_freq)
        cumsum = np.cumsum(sorted_freq)
        gini = (2 * np.sum((np.arange(1, n+1)) * sorted_freq) - (n + 1) * cumsum[-1]) / (n * cumsum[-1])

        # ì‚¬ìš©ë¥ 
        used_neurons = (neuron_counts > 0).sum()
        usage_ratio = used_neurons / n_neurons

        # Top-k ì§‘ì¤‘ë„
        top_10_ratio = np.sort(neuron_freq)[-10:].sum()
        top_50_ratio = np.sort(neuron_freq)[-50:].sum()

        print(f"\nLayer {layer_idx}:")
        print(f"  Used neurons: {used_neurons}/{n_neurons} ({usage_ratio:.2%})")
        print(f"  Gini coefficient: {gini:.4f} (0=equal, 1=unequal)")
        print(f"  Entropy: {entropy(neuron_freq + 1e-10):.4f}")
        print(f"  Top-10 neurons: {top_10_ratio:.2%}")
        print(f"  Top-50 neurons: {top_50_ratio:.2%}")

        # âš ï¸ ê²½ê³  ì‹œìŠ¤í…œ
        warnings = []
        if usage_ratio < 0.2:
            warnings.append(f"âš ï¸  SPARSE: Only {usage_ratio:.1%} neurons used - potential bottleneck!")
        if gini > 0.8:
            warnings.append(f"âš ï¸  UNEQUAL: Gini={gini:.2f} - heavily concentrated usage!")
        if top_10_ratio > 0.5:
            warnings.append(f"âš ï¸  DOMINATED: Top-10 neurons = {top_10_ratio:.1%} of usage!")

        for warning in warnings:
            print(f"  {warning}")

        results[f'layer_{layer_idx}'] = {
            'neuron_counts': neuron_counts.tolist(),
            'neuron_freq': neuron_freq.tolist(),
            'gini_coefficient': float(gini),
            'used_neurons': int(used_neurons),
            'total_neurons': int(n_neurons),
            'usage_ratio': float(usage_ratio),
            'top_10_ratio': float(top_10_ratio),
            'top_50_ratio': float(top_50_ratio),
            'entropy': float(entropy(neuron_freq + 1e-10)),
        }

    return results


# ============================================================
# 2. í† í°-ë‰´ëŸ° ì „ë¬¸í™” ë¶„ì„
# ============================================================

def analyze_token_neuron_specialization(collector, tokenizer, n_neurons, n_layers, top_k_tokens=50):
    """í† í°ë³„ ë‰´ëŸ° ì„ íƒ íŒ¨í„´ ë¶„ì„"""
    print("\n" + "="*70)
    print("2. TOKEN-NEURON SPECIALIZATION")
    print("="*70)

    results = {}

    # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í† í°
    all_tokens = list(collector.token_neuron_map.keys())
    token_counts = {
        token_id: sum(len(neurons) for neurons in collector.token_neuron_map[token_id])
        for token_id in all_tokens
    }
    top_tokens = sorted(token_counts.keys(), key=lambda x: token_counts[x], reverse=True)[:top_k_tokens]

    for layer_idx in range(n_layers):
        print(f"\nLayer {layer_idx} - Top 10 specialized tokens:")
        token_neuron_patterns = {}

        for token_id in top_tokens:
            neurons = collector.token_neuron_map[token_id][layer_idx]

            if len(neurons) == 0:
                continue

            neuron_counts = Counter(neurons)
            total = sum(neuron_counts.values())

            top_3 = neuron_counts.most_common(3)
            concentration = sum(count for _, count in top_3) / total if total > 0 else 0
            unique_neurons = len(neuron_counts)

            token_str = tokenizer.decode([token_id]).strip()

            token_neuron_patterns[token_str] = {
                'token_id': token_id,
                'total_occurrences': total,
                'unique_neurons': unique_neurons,
                'concentration': float(concentration),
                'top_3_neurons': [(int(n), int(c)) for n, c in top_3],
            }

        # ì§‘ì¤‘ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_tokens = sorted(
            token_neuron_patterns.items(),
            key=lambda x: x[1]['concentration'],
            reverse=True
        )[:10]

        for token_str, data in sorted_tokens:
            print(f"  '{token_str}': concentration={data['concentration']:.2%}, "
                  f"unique={data['unique_neurons']}, top_3={data['top_3_neurons'][:2]}")

        results[f'layer_{layer_idx}'] = token_neuron_patterns

    return results


# ============================================================
# 3. Layerë³„ ì°¨ì´ ë¶„ì„
# ============================================================

def analyze_layer_differences(neuron_usage_results):
    """Layerë³„ ë‰´ëŸ° ì‚¬ìš© íŒ¨í„´ ì°¨ì´"""
    print("\n" + "="*70)
    print("3. LAYER DIFFERENCES")
    print("="*70)

    results = {}
    layers = sorted([k for k in neuron_usage_results.keys() if k.startswith('layer_')])

    for i, layer_i in enumerate(layers):
        for j, layer_j in enumerate(layers):
            if i >= j:
                continue

            freq_i = np.array(neuron_usage_results[layer_i]['neuron_freq'])
            freq_j = np.array(neuron_usage_results[layer_j]['neuron_freq'])

            kl_div = entropy(freq_i + 1e-10, freq_j + 1e-10)
            cos_sim = np.dot(freq_i, freq_j) / (np.linalg.norm(freq_i) * np.linalg.norm(freq_j))

            print(f"\n{layer_i} vs {layer_j}:")
            print(f"  KL Divergence: {kl_div:.4f} (higher = more different)")
            print(f"  Cosine Similarity: {cos_sim:.4f} (1=identical)")

            results[f'{layer_i}_vs_{layer_j}'] = {
                'kl_divergence': float(kl_div),
                'cosine_similarity': float(cos_sim),
            }

    return results


# ============================================================
# 4. ì •í™•ë„-ë¶ˆí™•ì‹¤ì„± ë¶„ì„
# ============================================================

def analyze_uncertainty_accuracy(collector, n_layers):
    """ì •ë‹µ/ì˜¤ë‹µ ì‹œ ë‰´ëŸ° ì„ íƒ íŒ¨í„´ ë¹„êµ"""
    print("\n" + "="*70)
    print("4. ACCURACY-UNCERTAINTY RELATIONSHIP")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        correct_sel = collector.correct_selections[layer_idx]
        incorrect_sel = collector.incorrect_selections[layer_idx]

        if len(correct_sel) == 0 or len(incorrect_sel) == 0:
            continue

        correct_unique = len(torch.unique(correct_sel))
        incorrect_unique = len(torch.unique(incorrect_sel))

        # ì¤‘ë³µ ë‰´ëŸ° (ì •ë‹µê³¼ ì˜¤ë‹µ ëª¨ë‘ ì‚¬ìš©)
        correct_neurons_set = set(torch.unique(correct_sel).tolist())
        incorrect_neurons_set = set(torch.unique(incorrect_sel).tolist())
        overlap = correct_neurons_set & incorrect_neurons_set
        overlap_ratio = len(overlap) / len(correct_neurons_set | incorrect_neurons_set)

        # ë‰´ëŸ° ë‹¤ì–‘ì„± (í‰ê·  ìœ ë‹ˆí¬ ë‰´ëŸ° ìˆ˜)
        correct_diversity = correct_unique / len(correct_sel) if len(correct_sel) > 0 else 0
        incorrect_diversity = incorrect_unique / len(incorrect_sel) if len(incorrect_sel) > 0 else 0

        print(f"\nLayer {layer_idx}:")
        print(f"  Correct: {len(correct_sel):,} samples, {correct_unique} unique neurons")
        print(f"  Incorrect: {len(incorrect_sel):,} samples, {incorrect_unique} unique neurons")
        print(f"  Overlap: {len(overlap)} neurons ({overlap_ratio:.2%})")
        print(f"  Diversity: Correct={correct_diversity:.4f}, Incorrect={incorrect_diversity:.4f}")

        # âš ï¸ ë¶ˆí™•ì‹¤ì„± ì‹ í˜¸ ì²´í¬
        if overlap_ratio > 0.9:
            print(f"  âš ï¸  WEAK SIGNAL: {overlap_ratio:.1%} overlap - can't distinguish correct/incorrect!")
        elif abs(correct_unique - incorrect_unique) / max(correct_unique, incorrect_unique) < 0.1:
            print(f"  âš ï¸  SIMILAR PATTERNS: Correct and incorrect use similar neurons!")

        results[f'layer_{layer_idx}'] = {
            'correct_samples': len(correct_sel),
            'incorrect_samples': len(incorrect_sel),
            'correct_unique_neurons': int(correct_unique),
            'incorrect_unique_neurons': int(incorrect_unique),
            'overlap_neurons': len(overlap),
            'overlap_ratio': float(overlap_ratio),
            'correct_diversity': float(correct_diversity),
            'incorrect_diversity': float(incorrect_diversity),
        }

    return results


# ============================================================
# 5. ì‹œê°í™”
# ============================================================

def visualize_results(neuron_usage_results, output_dir):
    """ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
    output_dir = Path(output_dir)
    n_layers = len([k for k in neuron_usage_results.keys() if k.startswith('layer_')])

    print("\n" + "="*70)
    print("5. GENERATING VISUALIZATIONS")
    print("="*70)

    # 1. ë‰´ëŸ° ì‚¬ìš© ë¶„í¬
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for layer_idx in range(min(n_layers, 4)):
        layer_key = f'layer_{layer_idx}'
        neuron_freq = neuron_usage_results[layer_key]['neuron_freq']

        ax = axes[layer_idx]
        ax.hist(neuron_freq, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
        ax.set_xlabel('Neuron Selection Frequency', fontsize=10)
        ax.set_ylabel('Count', fontsize=10)
        ax.set_title(f'Layer {layer_idx}\nGini: {neuron_usage_results[layer_key]["gini_coefficient"]:.3f}, '
                    f'Usage: {neuron_usage_results[layer_key]["usage_ratio"]:.2%}', fontsize=11)
        ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_dir / 'neuron_usage_distribution.png', dpi=150, bbox_inches='tight')
    plt.show()  # Colabì—ì„œ ë°”ë¡œ í‘œì‹œ
    plt.close()
    print("  âœ“ neuron_usage_distribution.png")

    # 2. Layerë³„ í†µê³„
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    layers = list(range(n_layers))
    gini_coeffs = [neuron_usage_results[f'layer_{i}']['gini_coefficient'] for i in layers]
    usage_ratios = [neuron_usage_results[f'layer_{i}']['usage_ratio'] for i in layers]
    entropies = [neuron_usage_results[f'layer_{i}']['entropy'] for i in layers]

    axes[0].bar(layers, gini_coeffs, color='skyblue', edgecolor='black', width=0.6)
    axes[0].set_xlabel('Layer', fontsize=11)
    axes[0].set_ylabel('Gini Coefficient', fontsize=11)
    axes[0].set_title('Neuron Usage Inequality', fontsize=12, fontweight='bold')
    axes[0].grid(alpha=0.3, axis='y')

    axes[1].bar(layers, usage_ratios, color='lightcoral', edgecolor='black', width=0.6)
    axes[1].set_xlabel('Layer', fontsize=11)
    axes[1].set_ylabel('Usage Ratio', fontsize=11)
    axes[1].set_title('Fraction of Used Neurons', fontsize=12, fontweight='bold')
    axes[1].grid(alpha=0.3, axis='y')
    axes[1].set_ylim([0, 1])

    axes[2].bar(layers, entropies, color='lightgreen', edgecolor='black', width=0.6)
    axes[2].set_xlabel('Layer', fontsize=11)
    axes[2].set_ylabel('Entropy', fontsize=11)
    axes[2].set_title('Neuron Selection Diversity', fontsize=12, fontweight='bold')
    axes[2].grid(alpha=0.3, axis='y')

    plt.tight_layout()
    plt.savefig(output_dir / 'layer_statistics.png', dpi=150, bbox_inches='tight')
    plt.show()  # Colabì—ì„œ ë°”ë¡œ í‘œì‹œ
    plt.close()
    print("  âœ“ layer_statistics.png")

    # 3. Top-50 ë‰´ëŸ° íˆíŠ¸ë§µ
    fig, ax = plt.subplots(figsize=(12, 6))

    heatmap_data = []
    for layer_idx in range(n_layers):
        layer_key = f'layer_{layer_idx}'
        neuron_freq = np.array(neuron_usage_results[layer_key]['neuron_freq'])
        top_50_indices = np.argsort(neuron_freq)[-50:]
        top_50_freq = neuron_freq[top_50_indices]
        heatmap_data.append(top_50_freq)

    heatmap_data = np.array(heatmap_data)

    sns.heatmap(heatmap_data, cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Frequency'})
    ax.set_xlabel('Top-50 Neurons (sorted by frequency)', fontsize=11)
    ax.set_ylabel('Layer', fontsize=11)
    ax.set_title('Top-50 Most Active Neurons per Layer', fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig(output_dir / 'neuron_heatmap.png', dpi=150, bbox_inches='tight')
    plt.show()  # Colabì—ì„œ ë°”ë¡œ í‘œì‹œ
    plt.close()
    print("  âœ“ neuron_heatmap.png")


# ============================================================
# 6. ë¦¬í¬íŠ¸ ìƒì„±
# ============================================================

def generate_report(all_results, output_dir, checkpoint_path):
    """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
    output_dir = Path(output_dir)
    report_path = output_dir / 'analysis_report.txt'

    print("\n" + "="*70)
    print("6. GENERATING REPORT")
    print("="*70)

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("DAWN - Dynamic Neuron Transformer Analysis Report\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Checkpoint: {checkpoint_path}\n\n")

        # 1. ë‰´ëŸ° ì‚¬ìš©
        f.write("1. NEURON USAGE ANALYSIS\n")
        f.write("-" * 80 + "\n")

        neuron_results = all_results['neuron_usage']
        for layer_key in sorted(neuron_results.keys()):
            layer_data = neuron_results[layer_key]
            f.write(f"\n{layer_key.upper()}:\n")
            f.write(f"  Used: {layer_data['used_neurons']}/{layer_data['total_neurons']} "
                   f"({layer_data['usage_ratio']:.2%})\n")
            f.write(f"  Gini: {layer_data['gini_coefficient']:.4f}, "
                   f"Entropy: {layer_data['entropy']:.4f}\n")
            f.write(f"  Top-10: {layer_data['top_10_ratio']:.2%}, "
                   f"Top-50: {layer_data['top_50_ratio']:.2%}\n")

        # 2. Layer ì°¨ì´
        f.write("\n\n2. LAYER DIFFERENCES\n")
        f.write("-" * 80 + "\n")

        layer_diff = all_results['layer_differences']
        for pair_key in sorted(layer_diff.keys()):
            pair_data = layer_diff[pair_key]
            f.write(f"\n{pair_key}: KL={pair_data['kl_divergence']:.4f}, "
                   f"Cosine={pair_data['cosine_similarity']:.4f}\n")

        # 3. í† í°-ë‰´ëŸ° ì „ë¬¸í™” (ìƒ˜í”Œ)
        f.write("\n\n3. TOKEN-NEURON SPECIALIZATION (Top 5 per layer)\n")
        f.write("-" * 80 + "\n")

        token_spec = all_results['token_neuron_specialization']
        for layer_key in sorted(token_spec.keys()):
            f.write(f"\n{layer_key.upper()}:\n")
            layer_data = token_spec[layer_key]

            sorted_tokens = sorted(
                layer_data.items(),
                key=lambda x: x[1]['concentration'],
                reverse=True
            )[:5]

            for token_str, data in sorted_tokens:
                f.write(f"  '{token_str}': {data['concentration']:.2%} concentration, "
                       f"{data['unique_neurons']} unique, top_3={data['top_3_neurons']}\n")

        # 4. ì •í™•ë„-ë¶ˆí™•ì‹¤ì„±
        f.write("\n\n4. ACCURACY-UNCERTAINTY\n")
        f.write("-" * 80 + "\n")

        uncertainty = all_results['uncertainty_accuracy']
        for layer_key in sorted(uncertainty.keys()):
            layer_data = uncertainty[layer_key]
            f.write(f"\n{layer_key}:\n")
            f.write(f"  Correct: {layer_data['correct_unique_neurons']} neurons\n")
            f.write(f"  Incorrect: {layer_data['incorrect_unique_neurons']} neurons\n")
            f.write(f"  Overlap: {layer_data['overlap_neurons']} ({layer_data['overlap_ratio']:.2%})\n")
            f.write(f"  Diversity: Correct={layer_data['correct_diversity']:.4f}, "
                   f"Incorrect={layer_data['incorrect_diversity']:.4f}\n")

        # 5. ê¶Œì¥ì‚¬í•­
        f.write("\n\n5. RECOMMENDATIONS\n")
        f.write("-" * 80 + "\n\n")

        recommendations = []

        # ë‰´ëŸ° í¬ì†Œì„± ì²´í¬
        neuron_results = all_results['neuron_usage']
        for layer_key in sorted(neuron_results.keys()):
            layer_data = neuron_results[layer_key]
            if layer_data['usage_ratio'] < 0.2:
                recommendations.append(
                    f"âš ï¸  {layer_key}: Only {layer_data['usage_ratio']:.1%} neurons used\n"
                    f"   â†’ Consider increasing n_neurons (currently {layer_data['total_neurons']})\n"
                    f"   â†’ Or reducing k (top-k selection parameter)"
                )
            if layer_data['gini_coefficient'] > 0.8:
                recommendations.append(
                    f"âš ï¸  {layer_key}: High inequality (Gini={layer_data['gini_coefficient']:.2f})\n"
                    f"   â†’ Few neurons dominate - may need better initialization\n"
                    f"   â†’ Or add regularization to encourage uniform usage"
                )

        # ë¶ˆí™•ì‹¤ì„± ì‹ í˜¸ ì²´í¬
        for layer_key in sorted(uncertainty.keys()):
            layer_data = uncertainty[layer_key]
            if layer_data['overlap_ratio'] > 0.9:
                recommendations.append(
                    f"âš ï¸  {layer_key}: Weak uncertainty signal (overlap={layer_data['overlap_ratio']:.1%})\n"
                    f"   â†’ Model can't distinguish correct/incorrect predictions\n"
                    f"   â†’ May need more training or larger model capacity"
                )

        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                f.write(f"{i}. {rec}\n\n")
        else:
            f.write("âœ“ No critical issues detected!\n")

        f.write("\n" + "=" * 80 + "\n")

    print(f"  âœ“ analysis_report.txt")


# ============================================================
# Main
# ============================================================

# ============================================================
# ì¶”ê°€ ë¶„ì„: íŒ¨í„´, ìœ„ì¹˜
# ============================================================

def analyze_pattern_usage(collector, n_patterns, n_layers):
    """FFN íŒ¨í„´ ì‚¬ìš© ë¶„ì„ â­"""
    print("\n" + "="*70)
    print("PATTERN (FFN) USAGE ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.pattern_selections[layer_idx]) == 0:
            continue

        pattern_weights = collector.pattern_selections[layer_idx]  # [N, S, n_patterns]

        # Top-k íŒ¨í„´ ì„ íƒ (ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜)
        top_patterns = pattern_weights.argmax(dim=-1).numpy()  # [N, S]
        top_patterns = top_patterns.flatten()  # [N * S]
        pattern_counts = np.bincount(top_patterns, minlength=n_patterns)
        pattern_freq = pattern_counts / pattern_counts.sum()

        # Gini coefficient
        sorted_freq = np.sort(pattern_freq)
        n = len(sorted_freq)
        cumsum = np.cumsum(sorted_freq)
        gini = (2 * np.sum((np.arange(1, n+1)) * sorted_freq) - (n + 1) * cumsum[-1]) / (n * cumsum[-1])

        # ì‚¬ìš©ë¥ 
        used_patterns = (pattern_counts > 0).sum()
        usage_ratio = used_patterns / n_patterns

        # Top-k ì§‘ì¤‘ë„
        top_10_ratio = np.sort(pattern_freq)[-10:].sum()

        print(f"\nLayer {layer_idx}:")
        print(f"  Used patterns: {used_patterns}/{n_patterns} ({usage_ratio:.2%})")
        print(f"  Gini: {gini:.4f}, Entropy: {entropy(pattern_freq + 1e-10):.4f}")
        print(f"  Top-10 patterns: {top_10_ratio:.2%}")

        # ê²½ê³ 
        if usage_ratio < 0.2:
            print(f"  âš ï¸  SPARSE: Only {usage_ratio:.1%} patterns used!")
        if gini > 0.8:
            print(f"  âš ï¸  UNEQUAL: Pattern Gini={gini:.2f}!")

        results[f'layer_{layer_idx}'] = {
            'used_patterns': int(used_patterns),
            'total_patterns': int(n_patterns),
            'usage_ratio': float(usage_ratio),
            'gini_coefficient': float(gini),
            'entropy': float(entropy(pattern_freq + 1e-10)),
            'top_10_ratio': float(top_10_ratio),
        }

    return results


def analyze_pattern_collapse_detail(collector, n_patterns, n_layers):
    """íŒ¨í„´ collapse ìƒì„¸ ë¶„ì„ - ì–´ë–¤ íŒ¨í„´ë“¤ì´ ì§€ë°°í•˜ëŠ”ê°€?"""
    print("\n" + "="*70)
    print("PATTERN COLLAPSE DETAIL ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.pattern_selections[layer_idx]) == 0:
            continue

        pattern_weights = collector.pattern_selections[layer_idx]  # [N, S, n_patterns]
        top_patterns = pattern_weights.argmax(dim=-1).numpy().flatten()  # [N * S]

        # íŒ¨í„´ë³„ ì¹´ìš´íŠ¸
        pattern_counts = np.bincount(top_patterns, minlength=n_patterns)
        total_selections = pattern_counts.sum()

        # Top-10 íŒ¨í„´ ì°¾ê¸°
        top_indices = np.argsort(pattern_counts)[::-1][:10]
        top_counts = pattern_counts[top_indices]
        top_ratios = top_counts / total_selections

        print(f"\nLayer {layer_idx}:")
        print(f"  Total selections: {total_selections}")
        print(f"  Top-10 patterns:")
        for rank, (idx, count, ratio) in enumerate(zip(top_indices, top_counts, top_ratios), 1):
            print(f"    #{rank}: Pattern {idx:3d} - {count:7d} times ({ratio:6.2%})")

        # í‰ê·  gate ê°’ ë¶„ì„ (ìƒìœ„ 5ê°œ vs í•˜ìœ„ 5ê°œ)
        avg_weights = pattern_weights.mean(dim=(0, 1)).numpy()  # [n_patterns]
        top5_avg = avg_weights[top_indices[:5]].mean()
        bottom5_indices = np.argsort(pattern_counts)[:5]
        bottom5_avg = avg_weights[bottom5_indices].mean()

        print(f"  Average gate values:")
        print(f"    Top-5 patterns: {top5_avg:.6f}")
        print(f"    Bottom-5 patterns: {bottom5_avg:.6f}")
        print(f"    Ratio: {top5_avg / (bottom5_avg + 1e-10):.2f}x")

        # Collapse ê²½ê³ 
        top1_ratio = top_ratios[0]
        top5_ratio = top_ratios[:5].sum()
        if top1_ratio > 0.5:
            print(f"  ğŸ”´ SEVERE COLLAPSE: Top-1 pattern dominates {top1_ratio:.1%}!")
        elif top5_ratio > 0.8:
            print(f"  âš ï¸  COLLAPSE: Top-5 patterns dominate {top5_ratio:.1%}!")

        results[f'layer_{layer_idx}'] = {
            'top_10_patterns': top_indices.tolist(),
            'top_10_counts': top_counts.tolist(),
            'top_10_ratios': top_ratios.tolist(),
            'top5_avg_gate': float(top5_avg),
            'bottom5_avg_gate': float(bottom5_avg),
        }

    return results


def analyze_neuron_pattern_correlation(collector, n_layers, sample_size=1000):
    """ë‰´ëŸ° Set â†’ íŒ¨í„´ ë§¤í•‘ ì¼ê´€ì„± ë¶„ì„"""
    print("\n" + "="*70)
    print("NEURON-PATTERN CORRELATION ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.neuron_selections[layer_idx]) == 0:
            continue
        if len(collector.pattern_selections[layer_idx]) == 0:
            continue

        neuron_sel = collector.neuron_selections[layer_idx]  # [N, S, k]
        pattern_weights = collector.pattern_selections[layer_idx]  # [N, S, n_patterns]

        N, S, k = neuron_sel.shape
        total_samples = N * S

        # ìƒ˜í”Œë§ (ë„ˆë¬´ í¬ë©´)
        if total_samples > sample_size:
            indices = np.random.choice(total_samples, sample_size, replace=False)
        else:
            indices = np.arange(total_samples)

        # Flatten
        neuron_sel_flat = neuron_sel.reshape(-1, k).numpy()[indices]  # [sample, k]
        pattern_sel_flat = pattern_weights.argmax(dim=-1).reshape(-1).numpy()[indices]  # [sample]

        # ë‰´ëŸ° Set â†’ íŒ¨í„´ ë§¤í•‘ ë¹ˆë„
        neuron_pattern_map = defaultdict(lambda: defaultdict(int))

        for neuron_set, pattern in zip(neuron_sel_flat, pattern_sel_flat):
            neuron_key = tuple(sorted(neuron_set.tolist()))
            neuron_pattern_map[neuron_key][pattern] += 1

        # ì¼ê´€ì„± ì¸¡ì •: ê°™ì€ ë‰´ëŸ° Setì´ í•­ìƒ ê°™ì€ íŒ¨í„´ ì„ íƒ?
        consistency_scores = []
        for neuron_key, pattern_counts in neuron_pattern_map.items():
            total = sum(pattern_counts.values())
            if total > 1:  # 2ë²ˆ ì´ìƒ ë‚˜íƒ€ë‚œ Setë§Œ
                max_count = max(pattern_counts.values())
                consistency = max_count / total
                consistency_scores.append(consistency)

        if consistency_scores:
            avg_consistency = np.mean(consistency_scores)
            median_consistency = np.median(consistency_scores)

            print(f"\nLayer {layer_idx}:")
            print(f"  Unique neuron sets: {len(neuron_pattern_map)}")
            print(f"  Sets appearing >1 time: {len(consistency_scores)}")
            print(f"  Avg consistency: {avg_consistency:.4f}")
            print(f"  Median consistency: {median_consistency:.4f}")

            if avg_consistency > 0.9:
                print(f"  âœ… HIGH: Same neuron sets â†’ same patterns ({avg_consistency:.1%})")
            elif avg_consistency < 0.5:
                print(f"  âš ï¸  LOW: Neuron-pattern mapping is inconsistent")

            results[f'layer_{layer_idx}'] = {
                'unique_neuron_sets': len(neuron_pattern_map),
                'repeated_sets': len(consistency_scores),
                'avg_consistency': float(avg_consistency),
                'median_consistency': float(median_consistency),
            }

    return results


def analyze_selection_confidence(collector, n_layers):
    """ì„ íƒ confidence ë¶„ì„ (softmax score ë¶„í¬)"""
    print("\n" + "="*70)
    print("SELECTION CONFIDENCE ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.pattern_selections[layer_idx]) == 0:
            continue

        pattern_weights = collector.pattern_selections[layer_idx]  # [N, S, n_patterns]

        # Top-k ì ìˆ˜ ë¶„ì„
        top_scores, _ = pattern_weights.topk(k=3, dim=-1)  # [N, S, 3]
        top1_scores = top_scores[:, :, 0].numpy().flatten()
        top2_scores = top_scores[:, :, 1].numpy().flatten()
        top3_scores = top_scores[:, :, 2].numpy().flatten()

        # Gap ë¶„ì„
        gap_1_2 = top1_scores - top2_scores
        gap_2_3 = top2_scores - top3_scores

        print(f"\nLayer {layer_idx}:")
        print(f"  Top-1 score: {top1_scores.mean():.6f} Â± {top1_scores.std():.6f}")
        print(f"  Top-2 score: {top2_scores.mean():.6f} Â± {top2_scores.std():.6f}")
        print(f"  Top-3 score: {top3_scores.mean():.6f} Â± {top3_scores.std():.6f}")
        print(f"  Gap (1-2): {gap_1_2.mean():.6f} Â± {gap_1_2.std():.6f}")
        print(f"  Gap (2-3): {gap_2_3.mean():.6f} Â± {gap_2_3.std():.6f}")

        # Confidence í•´ì„
        if gap_1_2.mean() < 0.01:
            print(f"  âš ï¸  LOW CONFIDENCE: Top-1/2 gap very small ({gap_1_2.mean():.6f})")
        elif gap_1_2.mean() > 0.1:
            print(f"  âœ… HIGH CONFIDENCE: Clear winner (gap={gap_1_2.mean():.4f})")

        results[f'layer_{layer_idx}'] = {
            'top1_mean': float(top1_scores.mean()),
            'top1_std': float(top1_scores.std()),
            'gap_1_2_mean': float(gap_1_2.mean()),
            'gap_1_2_std': float(gap_1_2.std()),
        }

    return results


def analyze_position_patterns(collector, n_layers, max_positions=128):
    """ì‹œí€€ìŠ¤ ìœ„ì¹˜ë³„ ë‰´ëŸ° íŒ¨í„´ ë¶„ì„ â­"""
    print("\n" + "="*70)
    print("POSITION-BASED NEURON PATTERNS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        position_stats = {}

        # ì‹œì‘ (0-15), ì¤‘ê°„ (48-63), ë (112-127)
        ranges = {
            'start': (0, 16),
            'middle': (48, 64),
            'end': (112, 128)
        }

        for range_name, (start_pos, end_pos) in ranges.items():
            all_neurons = []
            for pos in range(start_pos, min(end_pos, max_positions)):
                if pos in collector.position_neuron_map:
                    neurons = collector.position_neuron_map[pos][layer_idx]
                    all_neurons.extend(neurons)

            if all_neurons:
                unique_neurons = len(set(all_neurons))
                position_stats[range_name] = unique_neurons
            else:
                position_stats[range_name] = 0

        # ìœ„ì¹˜ë³„ ë‹¤ì–‘ì„± ì°¨ì´
        if position_stats['start'] > 0 and position_stats['end'] > 0:
            diversity_change = (position_stats['end'] - position_stats['start']) / position_stats['start']

            print(f"\nLayer {layer_idx}:")
            print(f"  Start (0-15): {position_stats['start']} unique neurons")
            print(f"  Middle (48-63): {position_stats['middle']} unique neurons")
            print(f"  End (112-127): {position_stats['end']} unique neurons")
            print(f"  Change: {diversity_change:+.1%}")

            if abs(diversity_change) > 0.5:
                print(f"  âš ï¸  LARGE CHANGE: Position strongly affects neuron selection!")

        results[f'layer_{layer_idx}'] = position_stats

    return results


def analyze_neuron_coactivation(collector, n_neurons, n_layers):
    """ë‰´ëŸ° co-activation íŒ¨í„´ ë¶„ì„ - ì–´ë–¤ ë‰´ëŸ°ë“¤ì´ í•¨ê»˜ ì„ íƒë˜ë‚˜?"""
    print("\n" + "="*70)
    print("NEURON CO-ACTIVATION ANALYSIS")
    print("="*70)

    results = {}

    for layer_idx in range(n_layers):
        if len(collector.neuron_selections[layer_idx]) == 0:
            continue

        selected_neurons = collector.neuron_selections[layer_idx]  # [N, S, k]
        N, S, k = selected_neurons.shape

        # Initialize co-activation matrix
        coactivation = torch.zeros(n_neurons, n_neurons)
        neuron_activation_count = torch.zeros(n_neurons)

        # Count co-activations
        for b in range(N):
            for s in range(S):
                neurons = selected_neurons[b, s]

                # Count individual activations
                for n in neurons:
                    neuron_activation_count[n] += 1

                # Count co-activations
                for i in range(k):
                    for j in range(k):
                        if i != j:
                            coactivation[neurons[i], neurons[j]] += 1

        # Normalize by activation counts to get co-activation probability
        # P(j | i) = coactivation[i, j] / activation_count[i]
        coactivation_prob = torch.zeros_like(coactivation)
        for i in range(n_neurons):
            if neuron_activation_count[i] > 0:
                coactivation_prob[i, :] = coactivation[i, :] / neuron_activation_count[i]

        # Find strong co-activation patterns (mutual high probability)
        strong_pairs = []
        threshold = 0.8
        for i in range(n_neurons):
            for j in range(i+1, n_neurons):
                prob_ij = coactivation_prob[i, j].item()
                prob_ji = coactivation_prob[j, i].item()
                if prob_ij > threshold and prob_ji > threshold:
                    strong_pairs.append((i, j, prob_ij, prob_ji))

        # Statistics
        active_neurons = (neuron_activation_count > 0).sum().item()
        avg_coactivation = coactivation_prob[neuron_activation_count > 0].mean().item()

        results[f'layer_{layer_idx}'] = {
            'coactivation_matrix': coactivation.numpy().tolist(),
            'coactivation_prob': coactivation_prob.numpy().tolist(),
            'activation_counts': neuron_activation_count.numpy().tolist(),
            'strong_pairs': strong_pairs[:20],  # Top 20 strong pairs
            'active_neurons': int(active_neurons),
            'avg_coactivation_prob': float(avg_coactivation)
        }

        print(f"\nLayer {layer_idx}:")
        print(f"  Active neurons: {active_neurons}/{n_neurons}")
        print(f"  Strong co-activation pairs (>{threshold}): {len(strong_pairs)}")
        print(f"  Avg co-activation probability: {avg_coactivation:.4f}")

        # ê²½ê³ 
        if len(strong_pairs) > 50:
            print(f"  âš ï¸  STRONG COUPLING: {len(strong_pairs)} neuron pairs almost always activate together!")
            print(f"     â†’ May indicate redundant neurons or over-specialized combinations")

        # Show top 5 strong pairs
        if strong_pairs:
            print(f"  Top 5 co-activation pairs:")
            for idx, (i, j, prob_ij, prob_ji) in enumerate(strong_pairs[:5], 1):
                print(f"    #{idx}: Neurons ({i}, {j}) - P(j|i)={prob_ij:.3f}, P(i|j)={prob_ji:.3f}")

    return results


def analyze_neuron_diversity(model, n_layers):
    """ë‰´ëŸ° ê°„ ìœ ì‚¬ë„ ë° effective rank ë¶„ì„ (clustering í¬í•¨)"""
    print("\n" + "="*70)
    print("NEURON DIVERSITY ANALYSIS")
    print("="*70)

    from scipy.cluster.hierarchy import linkage

    results = {}

    for layer_idx in range(n_layers):
        if hasattr(model, '_orig_mod'):
            router = model._orig_mod.layers[layer_idx].router
        else:
            router = model.layers[layer_idx].router

        # Get neurons (handle low-rank decomposition)
        if hasattr(router, 'neuron_codes'):
            # v3.2: Low-rank neurons
            neurons = torch.matmul(router.neuron_codes.data, router.neuron_basis.data)
        else:
            # v3.1 and earlier: Full-rank neurons
            neurons = router.neurons.data

        neurons_cpu = neurons.cpu()

        # ì •ê·œí™”
        neurons_norm = F.normalize(neurons, p=2, dim=1)

        # ë‰´ëŸ° ê°„ ìœ ì‚¬ë„ (ì½”ì‚¬ì¸)
        similarity = torch.matmul(neurons_norm, neurons_norm.T)  # [n_neurons, n_neurons]

        # ìê¸° ìì‹  ì œì™¸í•˜ê³  ìœ ì‚¬ë„ ê³„ì‚°
        mask = ~torch.eye(similarity.shape[0], dtype=torch.bool, device=similarity.device)
        off_diag_sim = similarity[mask]

        # Find highly similar pairs
        similar_pairs = []
        threshold = 0.9
        similarity_cpu = similarity.cpu()
        n_neurons = similarity.shape[0]
        for i in range(n_neurons):
            for j in range(i+1, n_neurons):
                if abs(similarity_cpu[i, j].item()) > threshold:
                    similar_pairs.append((i, j, similarity_cpu[i, j].item()))

        # Hierarchical clustering
        try:
            linkage_matrix = linkage(neurons_cpu.numpy(), method='ward')
        except Exception as e:
            print(f"  âš ï¸  Clustering failed: {e}")
            linkage_matrix = None

        # Effective rank (SVD ê¸°ë°˜)
        U, S, V = torch.svd(neurons)
        # Normalized singular values
        S_normalized = S / S.sum()
        # Entropy-based effective rank
        entropy_val = -(S_normalized * torch.log(S_normalized + 1e-10)).sum()
        effective_rank = torch.exp(entropy_val).item()

        # í†µê³„
        mean_sim = off_diag_sim.mean().item()
        max_sim = off_diag_sim.max().item()
        std_sim = off_diag_sim.std().item()

        # Rank ratio
        rank_ratio = effective_rank / neurons.shape[0]

        results[f'layer_{layer_idx}'] = {
            'mean_similarity': mean_sim,
            'max_similarity': max_sim,
            'std_similarity': std_sim,
            'effective_rank': effective_rank,
            'total_neurons': neurons.shape[0],
            'rank_ratio': rank_ratio,
            'similar_pairs': similar_pairs[:20],  # Top 20 similar pairs
            'linkage': linkage_matrix.tolist() if linkage_matrix is not None else None
        }

        print(f"\nLayer {layer_idx}:")
        print(f"  Mean similarity: {mean_sim:.4f}")
        print(f"  Max similarity: {max_sim:.4f}")
        print(f"  Std similarity: {std_sim:.4f}")
        print(f"  Effective rank: {effective_rank:.1f} / {neurons.shape[0]}")
        print(f"  Rank ratio: {rank_ratio:.2%}")
        print(f"  Highly similar pairs (>{threshold}): {len(similar_pairs)}")

        # ê²½ê³ 
        if mean_sim > 0.5:
            print(f"  âš ï¸  HIGH SIMILARITY: Neurons are redundant!")
        if rank_ratio < 0.5:
            print(f"  âš ï¸  LOW RANK: Limited neuron diversity!")
        if len(similar_pairs) > 50:
            print(f"  âš ï¸  REDUNDANCY: {len(similar_pairs)} highly similar neuron pairs!")

    return results


def visualize_connections(model, output_dir):
    """ë ˆì´ì–´ ê°„ connection í–‰ë ¬ ì‹œê°í™”"""
    print("\n=== Visualizing Connection Matrices ===")

    output_dir = Path(output_dir)
    conn_dir = output_dir / 'connections'
    conn_dir.mkdir(exist_ok=True)

    for i, layer in enumerate(model.layers):
        if layer.router.has_connection:
            weight = layer.router.connection.weight.data.cpu().numpy()

            plt.figure(figsize=(12, 10))
            im = plt.imshow(weight, cmap='RdBu', vmin=-0.1, vmax=0.1, aspect='auto')
            plt.title(f'Layer {i-1} â†’ Layer {i} Connection Weights', fontsize=14, fontweight='bold')
            plt.xlabel(f'Layer {i-1} Neurons', fontsize=12)
            plt.ylabel(f'Layer {i} Neurons', fontsize=12)
            plt.colorbar(im, label='Connection Weight')

            # í†µê³„ ì •ë³´ ì¶”ê°€
            stats_text = f'Mean: {weight.mean():.4f}\nStd: {weight.std():.4f}\n'
            stats_text += f'Max: {weight.max():.4f}\nMin: {weight.min():.4f}'
            plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

            plt.tight_layout()
            plt.savefig(conn_dir / f'connection_layer_{i-1}_to_{i}.png', dpi=150, bbox_inches='tight')
            plt.close()

            print(f"  Saved: connection_layer_{i-1}_to_{i}.png")


def analyze_connection_patterns(model, collector, n_layers, output_dir):
    """ì‹¤ì œ í™œì„±í™”ì—ì„œ connection íš¨ê³¼ ë¶„ì„"""
    print("\n=== Analyzing Connection Effects ===")

    results = {}

    for layer_idx in range(1, n_layers):  # Skip first layer (no connection)
        layer = model.layers[layer_idx]
        if not layer.router.has_connection:
            continue

        # ì´ì „ ë ˆì´ì–´ì™€ í˜„ì¬ ë ˆì´ì–´ì˜ ì„ íƒ íŒ¨í„´
        prev_selected = collector.neuron_selections[layer_idx - 1]  # [N, S, k]
        curr_selected = collector.neuron_selections[layer_idx]  # [N, S, k]

        # Connection weight ê°€ì ¸ì˜¤ê¸°
        weight = layer.router.connection.weight.data.cpu()  # [n_neurons, prev_n_neurons]

        # ê° í˜„ì¬ ë‰´ëŸ°ì— ëŒ€í•´, ê°€ì¥ ê°•í•˜ê²Œ ì—°ê²°ëœ ì´ì „ ë‰´ëŸ° ì°¾ê¸°
        strong_connections = []
        for curr_neuron in range(weight.shape[0]):
            top_weights, top_prev_neurons = torch.topk(weight[curr_neuron].abs(), k=5)
            if top_weights[0] > 0.01:  # ì˜ë¯¸ìˆëŠ” ì—°ê²°ë§Œ
                strong_connections.append({
                    'current': curr_neuron,
                    'previous': top_prev_neurons.tolist(),
                    'weights': top_weights.tolist()
                })

        # ì‹¤ì œë¡œ í•¨ê»˜ í™œì„±í™”ë˜ëŠ” íŒ¨í„´ ì°¾ê¸° (ì™„ì „ vectorized on GPU)
        N, S, k = prev_selected.shape
        device = prev_selected.device
        n_curr, n_prev = weight.shape

        # One-hot encode selections
        # prev_selected: [N, S, k] â†’ [N, S, n_prev]
        prev_onehot = torch.zeros(N, S, n_prev, device=device)
        prev_onehot.scatter_add_(2, prev_selected, torch.ones(N, S, k, device=device))

        # curr_selected: [N, S, k] â†’ [N, S, n_curr]
        curr_onehot = torch.zeros(N, S, n_curr, device=device)
        curr_onehot.scatter_add_(2, curr_selected, torch.ones(N, S, k, device=device))

        # Co-activation: outer product and sum over batch and sequence
        # [N, S, n_curr, 1] Ã— [N, S, 1, n_prev] â†’ [N, S, n_curr, n_prev]
        # Then sum over N, S â†’ [n_curr, n_prev]
        co_activation = torch.einsum('nsc,nsp->cp', curr_onehot, prev_onehot)

        # Normalize
        co_activation = co_activation / (N * S)

        # Connection weightì™€ co-activation ìƒê´€ê´€ê³„
        weight_on_device = weight.to(device)
        weight_flat = weight_on_device.abs().flatten()
        coact_flat = co_activation.flatten()
        correlation = torch.corrcoef(torch.stack([weight_flat, coact_flat]))[0, 1].item()

        results[f'layer_{layer_idx}'] = {
            'num_strong_connections': len(strong_connections),
            'weight_coactivation_corr': correlation,
            'strong_connections': strong_connections[:10]  # Top 10ë§Œ ì €ì¥
        }

        print(f"\n  Layer {layer_idx}:")
        print(f"    Strong connections (>0.01): {len(strong_connections)}")
        print(f"    Weight-CoActivation correlation: {correlation:.4f}")

        if abs(correlation) > 0.3:
            print(f"    âœ“ Connection weights align with actual activation patterns!")
        elif abs(correlation) < 0.1:
            print(f"    âš ï¸  Connection weights don't match activation patterns")

    return results


def visualize_neuron_roles(diversity_results, coactivation_results, model, output_dir):
    """ë‰´ëŸ° ì—­í•  ì¢…í•© ì‹œê°í™” (similarity, co-activation, clustering)"""
    print("\n=== Visualizing Neuron Roles ===")

    from scipy.cluster.hierarchy import dendrogram
    import numpy as np

    output_dir = Path(output_dir)
    roles_dir = output_dir / 'neuron_roles'
    roles_dir.mkdir(exist_ok=True)

    n_layers = len([k for k in diversity_results.keys() if k.startswith('layer_')])

    for layer_idx in range(n_layers):
        layer_key = f'layer_{layer_idx}'

        if layer_key not in diversity_results:
            continue

        div_data = diversity_results[layer_key]
        coact_data = coactivation_results.get(layer_key, {})

        # Get neurons
        if hasattr(model, '_orig_mod'):
            router = model._orig_mod.layers[layer_idx].router
        else:
            router = model.layers[layer_idx].router

        if hasattr(router, 'neuron_codes'):
            neurons = torch.matmul(router.neuron_codes.data, router.neuron_basis.data)
        else:
            neurons = router.neurons.data

        neurons_cpu = neurons.cpu()
        neurons_norm = F.normalize(neurons, p=2, dim=1)
        similarity = torch.matmul(neurons_norm, neurons_norm.T).cpu().numpy()

        # Create comprehensive figure
        fig = plt.figure(figsize=(20, 12))

        # 1. Similarity Matrix
        ax1 = plt.subplot(2, 3, 1)
        im1 = ax1.imshow(similarity, cmap='RdBu', vmin=-1, vmax=1)
        ax1.set_title(f'Layer {layer_idx}: Neuron Similarity Matrix', fontsize=12, fontweight='bold')
        ax1.set_xlabel('Neuron ID')
        ax1.set_ylabel('Neuron ID')
        plt.colorbar(im1, ax=ax1, label='Cosine Similarity')

        # Add stats text
        stats_text = f"Mean: {div_data['mean_similarity']:.3f}\n"
        stats_text += f"Effective Rank: {div_data['effective_rank']:.1f}/{div_data['total_neurons']}\n"
        stats_text += f"Rank Ratio: {div_data['rank_ratio']:.2%}"
        ax1.text(0.02, 0.98, stats_text, transform=ax1.transAxes,
                verticalalignment='top', fontsize=9,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))

        # 2. Similarity Distribution
        ax2 = plt.subplot(2, 3, 2)
        sim_off_diag = similarity[~np.eye(similarity.shape[0], dtype=bool)]
        ax2.hist(sim_off_diag, bins=50, edgecolor='black', alpha=0.7, color='steelblue')
        ax2.set_xlabel('Cosine Similarity')
        ax2.set_ylabel('Count')
        ax2.set_title('Similarity Distribution (off-diagonal)', fontsize=12, fontweight='bold')
        ax2.axvline(div_data['mean_similarity'], color='red', linestyle='--',
                   label=f"Mean: {div_data['mean_similarity']:.3f}")
        ax2.legend()
        ax2.grid(alpha=0.3)

        # 3. Dendrogram (if available)
        ax3 = plt.subplot(2, 3, 3)
        if div_data['linkage'] is not None:
            linkage_matrix = np.array(div_data['linkage'])
            dendrogram(linkage_matrix, ax=ax3, no_labels=True, color_threshold=0)
            ax3.set_title('Hierarchical Clustering', fontsize=12, fontweight='bold')
            ax3.set_ylabel('Distance')
        else:
            ax3.text(0.5, 0.5, 'Clustering unavailable', ha='center', va='center')
            ax3.set_title('Hierarchical Clustering', fontsize=12, fontweight='bold')

        # 4. Co-activation Matrix (if available)
        ax4 = plt.subplot(2, 3, 4)
        if coact_data:
            coact_prob = np.array(coact_data['coactivation_prob'])
            im4 = ax4.imshow(coact_prob, cmap='YlOrRd', vmin=0, vmax=1)
            ax4.set_title('Co-activation Probability P(j|i)', fontsize=12, fontweight='bold')
            ax4.set_xlabel('Neuron j')
            ax4.set_ylabel('Neuron i')
            plt.colorbar(im4, ax=ax4, label='Probability')
        else:
            ax4.text(0.5, 0.5, 'No co-activation data', ha='center', va='center')
            ax4.set_title('Co-activation Probability', fontsize=12, fontweight='bold')

        # 5. Activation Counts (if available)
        ax5 = plt.subplot(2, 3, 5)
        if coact_data and 'activation_counts' in coact_data:
            activation_counts = np.array(coact_data['activation_counts'])
            ax5.bar(range(len(activation_counts)), activation_counts, color='coral', alpha=0.7)
            ax5.set_xlabel('Neuron ID')
            ax5.set_ylabel('Activation Count')
            ax5.set_title('Neuron Activation Frequency', fontsize=12, fontweight='bold')
            ax5.axhline(activation_counts.mean(), color='blue', linestyle='--',
                       label=f"Mean: {activation_counts.mean():.1f}")
            ax5.legend()
            ax5.grid(alpha=0.3, axis='y')
        else:
            ax5.text(0.5, 0.5, 'No activation data', ha='center', va='center')
            ax5.set_title('Neuron Activation Frequency', fontsize=12, fontweight='bold')

        # 6. Singular Value Distribution (Rank Analysis)
        ax6 = plt.subplot(2, 3, 6)
        U, S, V = torch.svd(neurons)
        S_normalized = (S / S.sum()).numpy()
        ax6.plot(S_normalized, marker='o', markersize=3, alpha=0.7)
        ax6.set_xlabel('Singular Value Index')
        ax6.set_ylabel('Normalized Magnitude')
        ax6.set_title('Singular Value Spectrum', fontsize=12, fontweight='bold')
        ax6.set_yscale('log')
        ax6.grid(alpha=0.3)
        ax6.axhline(1.0/len(S_normalized), color='red', linestyle='--',
                   label=f'Uniform (1/{len(S_normalized)})')
        ax6.legend()

        plt.tight_layout()
        plt.savefig(roles_dir / f'neuron_roles_layer_{layer_idx}.png', dpi=150, bbox_inches='tight')
        plt.close()

        print(f"  Saved: neuron_roles_layer_{layer_idx}.png")

    # Summary across layers
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    layers = list(range(n_layers))
    mean_sims = [diversity_results[f'layer_{i}']['mean_similarity'] for i in layers]
    rank_ratios = [diversity_results[f'layer_{i}']['rank_ratio'] for i in layers]
    num_similar_pairs = [len(diversity_results[f'layer_{i}']['similar_pairs']) for i in layers]

    # Mean similarity by layer
    axes[0, 0].plot(layers, mean_sims, marker='o', linewidth=2, markersize=8)
    axes[0, 0].set_xlabel('Layer')
    axes[0, 0].set_ylabel('Mean Cosine Similarity')
    axes[0, 0].set_title('Neuron Similarity Across Layers', fontweight='bold')
    axes[0, 0].grid(alpha=0.3)
    axes[0, 0].axhline(0.5, color='red', linestyle='--', alpha=0.5, label='High redundancy threshold')
    axes[0, 0].legend()

    # Rank ratio by layer
    axes[0, 1].plot(layers, rank_ratios, marker='s', linewidth=2, markersize=8, color='green')
    axes[0, 1].set_xlabel('Layer')
    axes[0, 1].set_ylabel('Effective Rank Ratio')
    axes[0, 1].set_title('Neuron Diversity Across Layers', fontweight='bold')
    axes[0, 1].grid(alpha=0.3)
    axes[0, 1].axhline(0.5, color='red', linestyle='--', alpha=0.5, label='Low diversity threshold')
    axes[0, 1].legend()
    axes[0, 1].set_ylim([0, 1])

    # Highly similar pairs
    axes[1, 0].bar(layers, num_similar_pairs, color='orange', alpha=0.7, edgecolor='black')
    axes[1, 0].set_xlabel('Layer')
    axes[1, 0].set_ylabel('Number of Pairs')
    axes[1, 0].set_title('Highly Similar Neuron Pairs (>0.9)', fontweight='bold')
    axes[1, 0].grid(alpha=0.3, axis='y')

    # Co-activation stats (if available)
    if all(f'layer_{i}' in coactivation_results for i in layers):
        strong_coact_pairs = [len(coactivation_results[f'layer_{i}'].get('strong_pairs', [])) for i in layers]
        axes[1, 1].bar(layers, strong_coact_pairs, color='purple', alpha=0.7, edgecolor='black')
        axes[1, 1].set_xlabel('Layer')
        axes[1, 1].set_ylabel('Number of Pairs')
        axes[1, 1].set_title('Strong Co-activation Pairs (>0.8)', fontweight='bold')
        axes[1, 1].grid(alpha=0.3, axis='y')
    else:
        axes[1, 1].text(0.5, 0.5, 'No co-activation data', ha='center', va='center',
                       transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('Strong Co-activation Pairs', fontweight='bold')

    plt.tight_layout()
    plt.savefig(roles_dir / 'neuron_roles_summary.png', dpi=150, bbox_inches='tight')
    plt.close()

    print(f"  Saved: neuron_roles_summary.png")


def main():
    parser = argparse.ArgumentParser(description='Analyze DAWN checkpoint')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to checkpoint folder or .pt file')
    parser.add_argument('--num_batches', type=int, default=100,
                       help='Number of batches to analyze')
    args = parser.parse_args()

    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì²˜ë¦¬
    checkpoint_path = Path(args.checkpoint)

    if checkpoint_path.is_dir():
        # í´ë”ì¸ ê²½ìš° best_model.pt ì°¾ê¸°
        best_model_path = checkpoint_path / 'best_model.pt'
        config_path = checkpoint_path / 'config.json'
        output_dir = checkpoint_path / 'analysis'
    else:
        # íŒŒì¼ì¸ ê²½ìš°
        best_model_path = checkpoint_path
        config_path = checkpoint_path.parent / 'config.json'
        output_dir = checkpoint_path.parent / 'analysis'

    if not best_model_path.exists():
        print(f"âŒ Checkpoint not found: {best_model_path}")
        return

    output_dir.mkdir(exist_ok=True)

    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Config ë¡œë“œ
    print(f"\nLoading config: {config_path}")
    with open(config_path, 'r') as f:
        cfg = json.load(f)

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    print(f"Loading checkpoint: {best_model_path}")
    checkpoint = torch.load(best_model_path, map_location=device)

    # ë²„ì „ ê°ì§€
    checkpoint_version = checkpoint.get('model_version', 'unknown')
    current_version = DAWN.__version__
    print(f"\nğŸ“Œ Checkpoint version: {checkpoint_version}")
    print(f"ğŸ“Œ Current model version: {current_version}")

    if checkpoint_version == 'unknown':
        print("   âš ï¸  Old checkpoint (pre-versioning)")
    elif checkpoint_version != current_version:
        print(f"   âš ï¸  Version mismatch - will attempt backward compatible loading")

    # ëª¨ë¸ ìƒì„±
    print("\nCreating model...")
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    vocab_size = tokenizer.vocab_size

    model = DAWN(
        vocab_size=vocab_size,
        hidden_dim=cfg['model']['d_model'],
        num_layers=cfg['model']['n_layers'],
        n_heads=cfg['model']['n_heads'],
        n_neurons=cfg['model']['n_neurons'],
        n_patterns=cfg['model']['n_patterns'],
        k=cfg['model']['k'],
        d_ff=cfg['model'].get('d_ff', None),
        max_seq_len=cfg['model']['max_seq_len'],
        dropout=cfg['model']['dropout']
    )

    # Load state dict (handle torch.compile() prefix)
    state_dict = checkpoint['model_state_dict']

    # Remove _orig_mod. prefix if present (from torch.compile)
    if any(k.startswith('_orig_mod.') for k in state_dict.keys()):
        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}

    # Load with strict=False to handle backward compatibility (connection weights)
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

    # Check if missing keys are only connection weights (expected for old checkpoints)
    connection_keys = [k for k in missing_keys if 'connection.weight' in k]
    other_missing = [k for k in missing_keys if 'connection.weight' not in k]

    if connection_keys:
        print(f"\nâš ï¸  Loading checkpoint without inter-layer connections ({len(connection_keys)} layers)")
        print("   Connection weights initialized to zero (equivalent to pre-connection model)")

    if other_missing:
        print(f"\nâš ï¸  Warning: Missing keys (not connection-related): {other_missing}")

    if unexpected_keys:
        print(f"\nâš ï¸  Warning: Unexpected keys in checkpoint: {unexpected_keys[:5]}...")

    model = model.to(device)
    model.eval()

    n_layers = cfg['model']['n_layers']
    n_neurons = cfg['model']['n_neurons']
    n_patterns = cfg['model']['n_patterns']

    print(f"\nModel: {n_layers} layers, {n_neurons} neurons/layer")
    print(f"Validation loss: {checkpoint.get('val_loss', 'N/A')}")
    print(f"Epoch: {checkpoint.get('epoch', 'N/A')}")

    # ë°ì´í„° ë¡œë“œ
    print("\nLoading validation data...")
    _, val_loader, _ = load_data(
        cfg['data'],
        max_length=cfg['model']['max_seq_len'],
        batch_size=32
    )

    # ìˆ˜ì§‘
    print(f"\nCollecting neuron patterns from {args.num_batches} batches...")
    collector = ActivationCollector(model, n_layers)

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(val_loader, total=args.num_batches)):
            if batch_idx >= args.num_batches:
                break

            input_ids = batch['input_ids'].to(device)
            input_ids, labels = apply_mlm_masking(input_ids, tokenizer, MLM_CONFIG)

            logits, all_selected, all_patterns = model(input_ids, return_activations=True)
            collector.collect(input_ids, labels, logits, all_selected, all_patterns)

    collector.finalize()

    # ë¶„ì„
    neuron_usage_results = analyze_neuron_usage(collector, n_neurons, n_layers)
    token_spec_results = analyze_token_neuron_specialization(
        collector, tokenizer, n_neurons, n_layers, top_k_tokens=100
    )
    layer_diff_results = analyze_layer_differences(neuron_usage_results)
    uncertainty_results = analyze_uncertainty_accuracy(collector, n_layers)

    # â­ ìƒˆë¡œìš´ ë¶„ì„
    pattern_usage_results = analyze_pattern_usage(collector, n_patterns, n_layers)
    pattern_collapse_results = analyze_pattern_collapse_detail(collector, n_patterns, n_layers)
    neuron_pattern_corr_results = analyze_neuron_pattern_correlation(collector, n_layers)
    confidence_results = analyze_selection_confidence(collector, n_layers)
    position_pattern_results = analyze_position_patterns(collector, n_layers)

    # ğŸ§¬ Neuron diversity ë¶„ì„
    diversity_results = analyze_neuron_diversity(model, n_layers)

    # ğŸ¤ Co-activation ë¶„ì„
    coactivation_results = analyze_neuron_coactivation(collector, n_neurons, n_layers)

    # ğŸ”— Connection ë¶„ì„ (ìˆê³  í•™ìŠµëœ ê²½ìš°ë§Œ)
    has_connections = any(layer.router.has_connection for layer in model.layers)
    if has_connections:
        # Connectionì´ ì „ë¶€ 0ì´ë©´ (í•™ìŠµ ì•ˆëœ ê²½ìš°) ìŠ¤í‚µ
        connection_stats = model.get_connection_stats()
        has_learned_connections = any(
            stats['std'] > 0.001 for stats in connection_stats.values()
        )

        if has_learned_connections:
            visualize_connections(model, output_dir)
            connection_pattern_results = analyze_connection_patterns(model, collector, n_layers, output_dir)
        else:
            print("\nâš ï¸  Connection weights are all zero (not trained) - skipping detailed analysis")
            connection_pattern_results = {}
    else:
        print("\nâš ï¸  Model has no inter-layer connections - skipping connection analysis")
        connection_pattern_results = {}
        connection_stats = {}

    all_results = {
        'neuron_usage': neuron_usage_results,
        'token_neuron_specialization': token_spec_results,
        'layer_differences': layer_diff_results,
        'uncertainty_accuracy': uncertainty_results,
        'pattern_usage': pattern_usage_results,  # â­ NEW
        'pattern_collapse_detail': pattern_collapse_results,  # â­ NEW
        'neuron_pattern_correlation': neuron_pattern_corr_results,  # â­ NEW
        'selection_confidence': confidence_results,  # â­ NEW
        'position_patterns': position_pattern_results,  # â­ NEW
        'neuron_diversity': diversity_results,  # ğŸ§¬ NEW
        'neuron_coactivation': coactivation_results,  # ğŸ¤ NEW
        'connection_patterns': connection_pattern_results,  # ğŸ”— NEW
        'connection_stats': connection_stats,  # ğŸ”— NEW
    }

    # ì €ì¥
    print(f"\nSaving results to: {output_dir}")
    with open(output_dir / 'analysis_results.json', 'w') as f:
        json.dump(all_results, f, indent=2)
    print("  âœ“ analysis_results.json")

    # ì‹œê°í™”
    visualize_results(neuron_usage_results, output_dir)

    # ë‰´ëŸ° ì—­í•  ì‹œê°í™” (similarity, co-activation, clustering)
    visualize_neuron_roles(diversity_results, coactivation_results, model, output_dir)

    # ë¦¬í¬íŠ¸
    generate_report(all_results, output_dir, best_model_path)

    print("\n" + "="*70)
    print("âœ… ANALYSIS COMPLETE!")
    print("="*70)
    print(f"\nResults saved to: {output_dir}/")
    print("  - analysis_results.json")
    print("  - analysis_report.txt")
    print("  - neuron_usage_distribution.png")
    print("  - layer_statistics.png")
    print("  - neuron_heatmap.png")
    print("  - neuron_roles/ (similarity, co-activation, clustering)")
    print("  - connections/ (connection matrices)")


if __name__ == "__main__":
    main()
