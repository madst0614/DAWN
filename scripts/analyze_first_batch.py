"""
Ï≤´ Î∞∞Ïπò Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù Ïä§ÌÅ¨Î¶ΩÌä∏

ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Ïùò Ïã§Ï†ú Í∏∏Ïù¥, Ìå®Îî© ÎπÑÏú®, ÎßàÏä§ÌÇπ Ìå®ÌÑ¥ÏùÑ ÌôïÏù∏Ìï©ÎãàÎã§.
train_three_stage.pyÏùò Îç∞Ïù¥ÌÑ∞ Î°úÎçîÎ•º Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©Ìï©ÎãàÎã§.
"""

import sys
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import torch
from transformers import AutoTokenizer

# Import from train script
import importlib.util
spec = importlib.util.spec_from_file_location(
    "train_module",
    PROJECT_ROOT / "scripts" / "train_three_stage.py"
)
train_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(train_module)


def analyze_batch(batch, tokenizer, batch_idx=0):
    """Î∞∞Ïπò Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù"""

    print("\n" + "="*70)
    print(f"BATCH {batch_idx} ANALYSIS")
    print("="*70)

    input_ids = batch['input_ids']
    B, S = input_ids.shape

    print(f"\nBatch shape: {input_ids.shape} (batch_size={B}, seq_len={S})")

    # Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ
    print(f"\n{'='*70}")
    print("OVERALL STATISTICS")
    print("="*70)

    total_tokens = B * S
    print(f"Total tokens in batch: {total_tokens:,}")

    # Í∞Å ÏãúÌÄÄÏä§ Î∂ÑÏÑù
    print(f"\n{'='*70}")
    print("PER-SEQUENCE ANALYSIS")
    print("="*70)

    seq_lengths = []
    padding_counts = []

    for i in range(min(10, B)):  # Ï≤òÏùå 10Í∞ú ÏãúÌÄÄÏä§Îßå
        seq = input_ids[i]

        # Padding Ï∞æÍ∏∞ (tokenizer.pad_token_id)
        is_padding = (seq == tokenizer.pad_token_id)
        non_padding = (~is_padding).sum().item()
        padding = is_padding.sum().item()

        seq_lengths.append(non_padding)
        padding_counts.append(padding)

        # ÌäπÏàò ÌÜ†ÌÅ∞ Ï∞æÍ∏∞
        is_cls = (seq == tokenizer.cls_token_id).sum().item()
        is_sep = (seq == tokenizer.sep_token_id).sum().item()
        is_mask = (seq == tokenizer.mask_token_id).sum().item() if hasattr(tokenizer, 'mask_token_id') else 0

        print(f"\nSequence {i+1}:")
        print(f"  Total length:     {S}")
        print(f"  Actual tokens:    {non_padding} ({non_padding/S*100:.1f}%)")
        print(f"  Padding:          {padding} ({padding/S*100:.1f}%)")
        print(f"  [CLS] tokens:     {is_cls}")
        print(f"  [SEP] tokens:     {is_sep}")
        print(f"  [MASK] tokens:    {is_mask}")

        # Ï≤´ 30Í∞ú ÌÜ†ÌÅ∞ Ï∂úÎ†•
        print(f"  First 30 tokens:  {seq[:30].tolist()}")

        # Ïã§Ï†ú ÌÖçÏä§Ìä∏ ÎîîÏΩîÎî© (padding Ï†úÏô∏)
        if non_padding > 0:
            actual_tokens = seq[~is_padding][:50]  # Ï≤òÏùå 50Í∞úÎßå
            try:
                decoded = tokenizer.decode(actual_tokens, skip_special_tokens=False)
                print(f"  Decoded (first 50): {decoded[:150]}...")
            except:
                pass

    # Ï†ÑÏ≤¥ Î∞∞Ïπò ÌÜµÍ≥Ñ
    print(f"\n{'='*70}")
    print("BATCH STATISTICS")
    print("="*70)

    all_seq_lengths = []
    all_padding_counts = []

    for i in range(B):
        seq = input_ids[i]
        is_padding = (seq == tokenizer.pad_token_id)
        non_padding = (~is_padding).sum().item()
        padding = is_padding.sum().item()

        all_seq_lengths.append(non_padding)
        all_padding_counts.append(padding)

    avg_length = sum(all_seq_lengths) / len(all_seq_lengths)
    avg_padding = sum(all_padding_counts) / len(all_padding_counts)

    print(f"\nSequence lengths:")
    print(f"  Min:     {min(all_seq_lengths)}")
    print(f"  Max:     {max(all_seq_lengths)}")
    print(f"  Mean:    {avg_length:.1f}")
    print(f"  Median:  {sorted(all_seq_lengths)[len(all_seq_lengths)//2]}")

    print(f"\nPadding statistics:")
    print(f"  Avg padding per sequence: {avg_padding:.1f} tokens")
    print(f"  Padding ratio:            {avg_padding/S*100:.1f}%")
    print(f"  Actual content:           {avg_length/S*100:.1f}%")

    # Î∂ÑÌè¨
    print(f"\nLength distribution:")
    bins = [
        (0, 20, "0-20"),
        (20, 40, "20-40"),
        (40, 60, "40-60"),
        (60, 80, "60-80"),
        (80, 100, "80-100"),
        (100, S, f"100-{S}")
    ]

    for min_l, max_l, label in bins:
        count = sum(1 for l in all_seq_lengths if min_l <= l < max_l)
        pct = count / B * 100
        print(f"  {label:10s}: {count:4d} sequences ({pct:5.1f}%)")

    # Í∂åÏû•ÏÇ¨Ìï≠
    print(f"\n{'='*70}")
    print("RECOMMENDATIONS")
    print("="*70)

    p75_length = sorted(all_seq_lengths)[int(len(all_seq_lengths) * 0.75)]
    p90_length = sorted(all_seq_lengths)[int(len(all_seq_lengths) * 0.90)]

    print(f"\nCurrent max_seq_len: {S}")
    print(f"P75 actual length:   {p75_length} (covers 75% of sequences)")
    print(f"P90 actual length:   {p90_length} (covers 90% of sequences)")

    if avg_padding / S > 0.5:
        print(f"\n‚ö†Ô∏è  WARNING: High padding ratio ({avg_padding/S*100:.1f}%)")
        print(f"   Recommendation: Reduce max_seq_len to ~{p75_length}-{p90_length}")
        print(f"   This will reduce padding from {avg_padding/S*100:.1f}% to ~{20-30}%")
    elif avg_padding / S > 0.3:
        print(f"\n‚ö†Ô∏è  Moderate padding ratio ({avg_padding/S*100:.1f}%)")
        print(f"   Consider reducing max_seq_len to ~{p90_length} for efficiency")
    else:
        print(f"\n‚úÖ Padding ratio is acceptable ({avg_padding/S*100:.1f}%)")


def analyze_labels(input_ids, labels, tokenizer):
    """ÎùºÎ≤® Î∂ÑÏÑù (MLM ÎßàÏä§ÌÇπ ÌõÑ)"""

    print("\n" + "="*70)
    print("LABEL ANALYSIS (MLM MASKING)")
    print("="*70)

    B, S = input_ids.shape

    # Ïú†Ìö®Ìïú ÎùºÎ≤® Ïπ¥Ïö¥Ìä∏
    valid_mask = (labels != -100)
    total_tokens = labels.numel()
    valid_tokens = valid_mask.sum().item()
    masked_tokens = total_tokens - valid_tokens

    print(f"\nTotal tokens:   {total_tokens:,}")
    print(f"Valid tokens:   {valid_tokens:,} ({valid_tokens/total_tokens*100:.1f}%)")
    print(f"Masked (-100):  {masked_tokens:,} ({masked_tokens/total_tokens*100:.1f}%)")

    if masked_tokens / total_tokens > 0.9:
        print(f"\nüö® CRITICAL: Over 90% masked! Training will fail!")
        print(f"   Expected: ~15% for MLM, ~0% for CLM")
    elif masked_tokens / total_tokens > 0.5:
        print(f"\n‚ö†Ô∏è  WARNING: Over 50% masked! This is abnormal!")
        print(f"   Expected: ~15% for MLM, ~0% for CLM")
    else:
        print(f"\n‚úÖ Masking ratio looks reasonable")

    # Ï≤´ ÏãúÌÄÄÏä§ ÏÉÅÏÑ∏ Î∂ÑÏÑù
    print(f"\n{'='*70}")
    print("FIRST SEQUENCE DETAIL")
    print("="*70)

    seq_input = input_ids[0]
    seq_labels = labels[0]
    seq_valid = valid_mask[0]

    print(f"\nInput  [:30]: {seq_input[:30].tolist()}")
    print(f"Labels [:30]: {seq_labels[:30].tolist()}")
    print(f"Valid  [:30]: {seq_valid[:30].tolist()}")

    # ÎùºÎ≤®Ïù¥ ÏûàÎäî ÏúÑÏπò ÌôïÏù∏
    valid_positions = seq_valid.nonzero().squeeze()
    print(f"\nValid label positions (first 20): {valid_positions[:20].tolist()}")

    # [MASK] ÌÜ†ÌÅ∞ ÏúÑÏπò ÌôïÏù∏
    if hasattr(tokenizer, 'mask_token_id'):
        mask_positions = (seq_input == tokenizer.mask_token_id).nonzero().squeeze()
        print(f"[MASK] token positions (first 20): {mask_positions[:20].tolist()}")

    # ÏòàÏÉÅ ÌÜ†ÌÅ∞ vs Ïã§Ï†ú ÎùºÎ≤® ÎπÑÍµê
    print(f"\nFirst 10 valid tokens:")
    valid_idx = valid_positions[:10]
    for i, idx in enumerate(valid_idx):
        idx_item = idx.item() if idx.dim() > 0 else idx
        input_token = seq_input[idx_item].item()
        label_token = seq_labels[idx_item].item()
        print(f"  Pos {idx_item:3d}: input={input_token:5d}, label={label_token:5d}")


def main():
    print("="*70)
    print("FIRST BATCH DATA ANALYZER")
    print("="*70)

    # Îç∞Ïù¥ÌÑ∞ Î°úÎçî ÏÉùÏÑ±
    print("\nLoading data...")
    try:
        train_loader, val_loader, tokenizer = train_module.load_cached_data(
            tokenizer_path="bert-base-uncased",
            max_length=128,  # ÌòÑÏû¨ ÏÑ§Ï†ï
            batch_size=128   # ÌòÑÏû¨ ÏÑ§Ï†ï
        )

        print(f"‚úÖ Data loaded successfully!")
        print(f"   Tokenizer: bert-base-uncased")
        print(f"   pad_token_id: {tokenizer.pad_token_id}")
        print(f"   cls_token_id: {tokenizer.cls_token_id}")
        print(f"   sep_token_id: {tokenizer.sep_token_id}")
        if hasattr(tokenizer, 'mask_token_id'):
            print(f"   mask_token_id: {tokenizer.mask_token_id}")

    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        print("\nMake sure WikiText cache exists at:")
        print("  /content/drive/MyDrive/dawn_v4/cache/train/wikitext_5to1_texts.pkl")
        return

    # Ï≤´ Î∞∞Ïπò Í∞ÄÏ†∏Ïò§Í∏∞ (MLM ÎßàÏä§ÌÇπ Ï†Ñ)
    print("\nFetching first batch...")
    first_batch = next(iter(train_loader))

    # Î∞∞Ïπò Î∂ÑÏÑù
    analyze_batch(first_batch, tokenizer, batch_idx=1)

    # MLM ÎßàÏä§ÌÇπ Ï†ÅÏö©
    print("\n" + "="*70)
    print("APPLYING MLM MASKING")
    print("="*70)

    input_ids = first_batch['input_ids']
    input_ids_masked, labels = train_module.apply_mlm_masking(
        input_ids.clone(),
        tokenizer,
        train_module.MLM_CONFIG
    )

    # ÎùºÎ≤® Î∂ÑÏÑù
    analyze_labels(input_ids, labels, tokenizer)

    print("\n" + "="*70)
    print("ANALYSIS COMPLETE")
    print("="*70)


if __name__ == "__main__":
    main()
