"""
SPROUT Brain-Like ëª¨ë¸ ë¶„ì„ ë„êµ¬

í•™ìŠµëœ ëª¨ë¸ì˜ ë‰´ëŸ° í™œì„±í™” íŒ¨í„´, ë‹¤ì–‘ì„±, ì˜ˆì¸¡ í’ˆì§ˆ ë¶„ì„
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from transformers import BertTokenizer
from tqdm import tqdm

from src.models.sprout_brain_like import create_brain_like_sprout


def analyze_neuron_usage(model, dataloader, device, num_batches=100):
    """
    ë‰´ëŸ° ì‚¬ìš© íŒ¨í„´ ë¶„ì„

    ê° ë‰´ëŸ°ì´ ì–¼ë§ˆë‚˜ ìì£¼ ì‚¬ìš©ë˜ëŠ”ì§€ í™•ì¸
    """
    print("\n" + "="*70)
    print("ë‰´ëŸ° ì‚¬ìš© íŒ¨í„´ ë¶„ì„")
    print("="*70)

    n_neurons = model.n_neurons
    neuron_usage = torch.zeros(n_neurons)
    all_activations = []

    model.eval()
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Analyzing", total=num_batches)):
            if batch_idx >= num_batches:
                break

            tokens = batch['input_ids'].to(device)

            # ì´ˆê¸° í™œì„±í™” íŒ¨í„´
            activation = model.input_encoder(tokens)  # [batch, n_neurons]

            # ì‚¬ìš©ëœ ë‰´ëŸ° ê¸°ë¡
            active = (activation > 0.01).float()
            neuron_usage += active.sum(dim=0).cpu()

            # ì „ì²´ í™œì„±í™” ì €ì¥
            all_activations.append(activation.cpu())

    # í†µê³„ ê³„ì‚°
    total_samples = min(num_batches, len(dataloader)) * dataloader.batch_size
    usage_freq = neuron_usage / total_samples

    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
    always_used = (usage_freq > 0.95).sum().item()
    often_used = ((usage_freq > 0.5) & (usage_freq <= 0.95)).sum().item()
    sometimes_used = ((usage_freq > 0.1) & (usage_freq <= 0.5)).sum().item()
    rarely_used = ((usage_freq > 0.01) & (usage_freq <= 0.1)).sum().item()
    never_used = (usage_freq <= 0.01).sum().item()

    print(f"\nì´ {total_samples} ìƒ˜í”Œ ë¶„ì„")
    print(f"\në‰´ëŸ° ì‚¬ìš© ë¶„í¬:")
    print(f"  í•­ìƒ ì‚¬ìš© (>95%):   {always_used:4d} / {n_neurons} ({always_used/n_neurons*100:.1f}%)")
    print(f"  ìì£¼ ì‚¬ìš© (50-95%):  {often_used:4d} / {n_neurons} ({often_used/n_neurons*100:.1f}%)")
    print(f"  ê°€ë” ì‚¬ìš© (10-50%):  {sometimes_used:4d} / {n_neurons} ({sometimes_used/n_neurons*100:.1f}%)")
    print(f"  ë“œë¬¼ê²Œ (1-10%):     {rarely_used:4d} / {n_neurons} ({rarely_used/n_neurons*100:.1f}%)")
    print(f"  ê±°ì˜ ì•ˆ ì”€ (<1%):   {never_used:4d} / {n_neurons} ({never_used/n_neurons*100:.1f}%)")

    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # ì‚¬ìš© ë¹ˆë„ íˆìŠ¤í† ê·¸ë¨
    ax = axes[0]
    ax.hist(usage_freq.numpy(), bins=50, edgecolor='black')
    ax.set_xlabel('Usage Frequency')
    ax.set_ylabel('Number of Neurons')
    ax.set_title('Neuron Usage Distribution')
    ax.grid(True, alpha=0.3)

    # ì¹´í…Œê³ ë¦¬ë³„ ë§‰ëŒ€ ê·¸ë˜í”„
    ax = axes[1]
    categories = ['Always\n(>95%)', 'Often\n(50-95%)', 'Sometimes\n(10-50%)',
                  'Rarely\n(1-10%)', 'Never\n(<1%)']
    counts = [always_used, often_used, sometimes_used, rarely_used, never_used]
    colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c', '#95a5a6']

    ax.bar(categories, counts, color=colors, edgecolor='black')
    ax.set_ylabel('Number of Neurons')
    ax.set_title('Neuron Usage Categories')
    ax.grid(True, axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig('neuron_usage.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ… Saved: neuron_usage.png")

    return {
        'usage_freq': usage_freq,
        'categories': {
            'always': always_used,
            'often': often_used,
            'sometimes': sometimes_used,
            'rarely': rarely_used,
            'never': never_used
        }
    }


def analyze_input_diversity(model, tokenizer, device):
    """
    ì…ë ¥ ë‹¤ì–‘ì„± ë¶„ì„

    ì„œë¡œ ë‹¤ë¥¸ ì…ë ¥ì— ëŒ€í•´ ë‹¤ë¥¸ ë‰´ëŸ°ì´ í™œì„±í™”ë˜ëŠ”ì§€ í™•ì¸
    """
    print("\n" + "="*70)
    print("ì…ë ¥ ë‹¤ì–‘ì„± ë¶„ì„")
    print("="*70)

    # ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ë¬¸ì¥
    test_inputs = [
        "The cat is sleeping on the mat",
        "I love programming in Python",
        "Quantum mechanics is very difficult",
        "She bought a new car yesterday",
        "The weather is nice today",
        "Machine learning models require data",
        "The students are studying for exams",
        "Pizza is my favorite food",
        "Scientists discovered a new planet",
        "The concert was absolutely amazing"
    ]

    model.eval()
    activations = []

    print(f"\n{len(test_inputs)} ê°œì˜ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ë¶„ì„ ì¤‘...")

    with torch.no_grad():
        for text in test_inputs:
            tokens = tokenizer(
                text,
                return_tensors='pt',
                padding='max_length',
                max_length=32,
                truncation=True
            )['input_ids'].to(device)

            act = model.input_encoder(tokens)
            activations.append(act[0].cpu())  # [n_neurons]

    # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
    n = len(test_inputs)
    similarity_matrix = np.zeros((n, n))

    for i in range(n):
        for j in range(n):
            sim = F.cosine_similarity(
                activations[i].unsqueeze(0),
                activations[j].unsqueeze(0)
            ).item()
            similarity_matrix[i, j] = sim

    # í†µê³„
    all_sims = []
    for i in range(n):
        for j in range(i+1, n):
            all_sims.append(similarity_matrix[i, j])

    mean_sim = np.mean(all_sims)
    std_sim = np.std(all_sims)

    print(f"\ní‰ê·  ìœ ì‚¬ë„: {mean_sim:.3f} (Â± {std_sim:.3f})")
    print(f"ìµœì†Œ ìœ ì‚¬ë„: {np.min(all_sims):.3f}")
    print(f"ìµœëŒ€ ìœ ì‚¬ë„: {np.max(all_sims):.3f}")

    # í•´ì„
    if mean_sim < 0.3:
        interpretation = "âœ… ë§¤ìš° ë‹¤ì–‘í•œ í™œì„±í™” (ì¢‹ìŒ!)"
    elif mean_sim < 0.7:
        interpretation = "âœ“ ì ë‹¹íˆ ë‹¤ì–‘í•¨"
    elif mean_sim < 0.9:
        interpretation = "âš ï¸ ë¹„ìŠ·í•œ íŒ¨í„´ (ë¬¸ì œ ê°€ëŠ¥)"
    else:
        interpretation = "âŒ ê±°ì˜ ë™ì¼í•œ íŒ¨í„´ (ì‹¬ê°)"

    print(f"\ní•´ì„: {interpretation}")

    # ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 8))

    im = ax.imshow(similarity_matrix, cmap='RdYlGn_r', vmin=0, vmax=1)
    ax.set_xticks(range(n))
    ax.set_yticks(range(n))
    ax.set_xticklabels([f"Input {i+1}" for i in range(n)], rotation=45, ha='right')
    ax.set_yticklabels([f"Input {i+1}" for i in range(n)])

    # ê°’ í‘œì‹œ
    for i in range(n):
        for j in range(n):
            text = ax.text(j, i, f'{similarity_matrix[i, j]:.2f}',
                          ha="center", va="center", color="black", fontsize=8)

    ax.set_title('Activation Pattern Similarity Matrix')
    plt.colorbar(im, ax=ax, label='Cosine Similarity')
    plt.tight_layout()
    plt.savefig('input_diversity.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ… Saved: input_diversity.png")

    # ê° ë¬¸ì¥ë³„ í™œì„± ë‰´ëŸ° ì¶œë ¥
    print(f"\nê° ì…ë ¥ë³„ í™œì„± ë‰´ëŸ° (Top 10):")
    for i, text in enumerate(test_inputs):
        active_neurons = (activations[i] > 0.01).nonzero().squeeze()
        top_values, top_indices = torch.topk(activations[i], k=10)

        print(f"\n{i+1}. \"{text}\"")
        print(f"   í™œì„± ë‰´ëŸ° ìˆ˜: {len(active_neurons)}")
        print(f"   Top 10: {top_indices.tolist()}")

    return {
        'mean_similarity': mean_sim,
        'std_similarity': std_sim,
        'similarity_matrix': similarity_matrix,
        'interpretation': interpretation
    }


def analyze_prediction_quality(model, tokenizer, device):
    """
    ì˜ˆì¸¡ í’ˆì§ˆ ë¶„ì„

    Masked token ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ í™•ì¸
    """
    print("\n" + "="*70)
    print("ì˜ˆì¸¡ í’ˆì§ˆ ë¶„ì„")
    print("="*70)

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (ì •ë‹µ í¬í•¨)
    test_cases = [
        ("The cat is [MASK] on the mat", "sleeping"),
        ("I love [MASK] in Python", "programming"),
        ("The [MASK] is very difficult", "problem"),
        ("She bought a new [MASK] yesterday", "car"),
        ("The weather is [MASK] today", "nice"),
        ("Machine [MASK] models require data", "learning"),
        ("The students are [MASK] for exams", "studying"),
        ("Pizza is my favorite [MASK]", "food"),
        ("Scientists discovered a new [MASK]", "planet"),
        ("The concert was absolutely [MASK]", "amazing"),
    ]

    model.eval()
    results = []

    print(f"\n{len(test_cases)} ê°œì˜ MLM í…ŒìŠ¤íŠ¸:")

    with torch.no_grad():
        for text, answer in test_cases:
            # í† í°í™”
            tokens = tokenizer(
                text,
                return_tensors='pt',
                padding='max_length',
                max_length=32,
                truncation=True
            )['input_ids'].to(device)

            # MASK ìœ„ì¹˜ í™•ì¸ (ì¡´ì¬ ì—¬ë¶€ë§Œ)
            mask_pos = (tokens == tokenizer.mask_token_id).nonzero()
            if len(mask_pos) == 0:
                continue

            # ì˜ˆì¸¡
            # Brain-Like ëª¨ë¸: ì „ì²´ ì‹œí€€ìŠ¤ â†’ í•˜ë‚˜ì˜ ì˜ˆì¸¡ [batch, vocab_size]
            logits = model(tokens)
            pred_logits = logits[0]  # [vocab_size]

            # Top-10 ì˜ˆì¸¡
            top_values, top_indices = torch.topk(pred_logits, k=10)
            top_words = [tokenizer.decode([idx]) for idx in top_indices]

            # ì •ë‹µ ìœ„ì¹˜
            answer_id = tokenizer.convert_tokens_to_ids(answer)
            answer_rank = (pred_logits.argsort(descending=True) == answer_id).nonzero()
            if len(answer_rank) > 0:
                answer_rank = answer_rank.item() + 1
            else:
                answer_rank = -1

            results.append({
                'text': text,
                'answer': answer,
                'top_10': top_words,
                'answer_rank': answer_rank,
                'in_top_10': answer_rank <= 10 and answer_rank > 0
            })

            print(f"\nì…ë ¥: {text}")
            print(f"ì •ë‹µ: {answer} (Rank: {answer_rank if answer_rank > 0 else '>1000'})")
            print(f"Top 10 ì˜ˆì¸¡:")
            for i, word in enumerate(top_words):
                marker = "âœ…" if word.strip() == answer else ""
                print(f"  {i+1}. {word:15s} {marker}")

    # í†µê³„
    total = len(results)
    top1_correct = sum(1 for r in results if r['answer_rank'] == 1)
    top5_correct = sum(1 for r in results if 1 <= r['answer_rank'] <= 5)
    top10_correct = sum(1 for r in results if r['in_top_10'])

    print(f"\n" + "="*70)
    print(f"ì˜ˆì¸¡ ì •í™•ë„:")
    print(f"  Top-1:  {top1_correct}/{total} ({top1_correct/total*100:.1f}%)")
    print(f"  Top-5:  {top5_correct}/{total} ({top5_correct/total*100:.1f}%)")
    print(f"  Top-10: {top10_correct}/{total} ({top10_correct/total*100:.1f}%)")
    print("="*70)

    return {
        'top1_accuracy': top1_correct / total,
        'top5_accuracy': top5_correct / total,
        'top10_accuracy': top10_correct / total,
        'results': results
    }


def analyze_learning_curve(checkpoint_dir):
    """
    í•™ìŠµ ê³¡ì„  ë¶„ì„

    Epochë³„ loss/accuracy íŠ¸ë Œë“œ í™•ì¸
    """
    print("\n" + "="*70)
    print("í•™ìŠµ ê³¡ì„  ë¶„ì„")
    print("="*70)

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint_path = os.path.join(checkpoint_dir, "sprout_brain_like_best.pt")

    if not os.path.exists(checkpoint_path):
        print(f"âŒ Checkpoint not found: {checkpoint_path}")
        return None

    checkpoint = torch.load(checkpoint_path, map_location='cpu')

    # ì •ë³´ ì¶œë ¥
    print(f"\nCheckpoint ì •ë³´:")
    print(f"  Epoch: {checkpoint.get('epoch', 'N/A')}")
    print(f"  Loss: {checkpoint.get('loss', 'N/A'):.4f}")
    print(f"  Accuracy: {checkpoint.get('accuracy', 'N/A'):.2f}%")

    # ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥ëœ íˆìŠ¤í† ë¦¬ (ì‹¤ì œë¡œëŠ” ë¡œê·¸ì—ì„œ íŒŒì‹±)
    # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ
    epochs = [1, 2, 3]
    losses = [12.09, 11.27, 11.19]
    accs = [7.35, 7.64, 8.20]

    # íŠ¸ë Œë“œ ë¶„ì„
    loss_improvement = (losses[0] - losses[-1]) / losses[0] * 100
    acc_improvement = (accs[-1] - accs[0]) / accs[0] * 100

    print(f"\ní•™ìŠµ ì§„í–‰:")
    print(f"  Loss ê°œì„ : {loss_improvement:.1f}% ({losses[0]:.2f} â†’ {losses[-1]:.2f})")
    print(f"  Acc ê°œì„ : {acc_improvement:.1f}% ({accs[0]:.2f}% â†’ {accs[-1]:.2f}%)")

    # ì‹œê°í™”
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    # Loss curve
    ax1.plot(epochs, losses, 'b-o', linewidth=2, markersize=8)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.set_xticks(epochs)

    # Accuracy curve
    ax2.plot(epochs, accs, 'r-o', linewidth=2, markersize=8)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy (%)', fontsize=12)
    ax2.set_title('Training Accuracy', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.set_xticks(epochs)

    plt.tight_layout()
    plt.savefig('learning_curve.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ… Saved: learning_curve.png")

    # ì™¸ì‚½ (ë” í•™ìŠµí•˜ë©´?)
    if len(epochs) >= 3:
        from scipy.optimize import curve_fit

        def exp_decay(x, a, b, c):
            return a * np.exp(-b * x) + c

        try:
            popt, _ = curve_fit(exp_decay, epochs, losses, p0=[10, 0.1, 8])
            predicted_loss_10 = exp_decay(10, *popt)
            predicted_loss_20 = exp_decay(20, *popt)

            print(f"\nì˜ˆìƒ ì„±ëŠ¥ (ì™¸ì‚½):")
            print(f"  10 epoch: Loss â‰ˆ {predicted_loss_10:.2f}")
            print(f"  20 epoch: Loss â‰ˆ {predicted_loss_20:.2f}")
        except:
            print(f"\nâš ï¸ ì™¸ì‚½ ì‹¤íŒ¨ (ë°ì´í„° ë¶€ì¡±)")

    return {
        'epochs': epochs,
        'losses': losses,
        'accuracies': accs,
        'loss_improvement': loss_improvement,
        'acc_improvement': acc_improvement
    }


def main():
    """ë©”ì¸ ë¶„ì„ ë£¨í‹´"""
    import argparse

    parser = argparse.ArgumentParser(description="Analyze SPROUT Brain-Like Model")
    parser.add_argument("--checkpoint_dir", type=str, default="./checkpoints",
                        help="Checkpoint directory")
    parser.add_argument("--batch_size", type=int, default=32,
                        help="Batch size for analysis")
    parser.add_argument("--num_batches", type=int, default=100,
                        help="Number of batches to analyze")
    parser.add_argument("--debug", action="store_true",
                        help="Use small data for testing")

    args = parser.parse_args()

    # Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\nDevice: {device}")

    # Auto-detect checkpoint dir
    if os.path.exists("/content/drive/MyDrive/sprout_checkpoints"):
        args.checkpoint_dir = "/content/drive/MyDrive/sprout_checkpoints"
        print(f"ğŸ“‚ Using: {args.checkpoint_dir}")

    # Tokenizer
    print(f"\nLoading tokenizer...")
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    # Model
    print(f"Loading model...")
    checkpoint_path = os.path.join(args.checkpoint_dir, "sprout_brain_like_best.pt")

    if not os.path.exists(checkpoint_path):
        print(f"âŒ Checkpoint not found: {checkpoint_path}")
        print(f"Please train the model first!")
        return

    checkpoint = torch.load(checkpoint_path, map_location=device)

    model = create_brain_like_sprout(
        vocab_size=len(tokenizer),
        n_neurons=4096,
        d_state=256,
        n_interaction_steps=5
    ).to(device)

    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    print(f"âœ… Model loaded from epoch {checkpoint.get('epoch', 'N/A')}")

    # 1. í•™ìŠµ ê³¡ì„ 
    analyze_learning_curve(args.checkpoint_dir)

    # 2. ì…ë ¥ ë‹¤ì–‘ì„±
    analyze_input_diversity(model, tokenizer, device)

    # 3. ì˜ˆì¸¡ í’ˆì§ˆ
    analyze_prediction_quality(model, tokenizer, device)

    # 4. ë‰´ëŸ° ì‚¬ìš© (ë°ì´í„° í•„ìš”)
    if args.debug:
        print(f"\nâš ï¸ Debug mode: Skipping neuron usage analysis (requires dataloader)")
    else:
        print(f"\nâš ï¸ Neuron usage analysis requires training dataloader")
        print(f"   Run this script during/after training with --analyze_activation")

    print(f"\n" + "="*70)
    print("ë¶„ì„ ì™„ë£Œ!")
    print("="*70)
    print(f"\nìƒì„±ëœ íŒŒì¼:")
    print(f"  - learning_curve.png")
    print(f"  - input_diversity.png")
    print(f"\në‹¤ìŒ ë‹¨ê³„:")
    print(f"  1. ê·¸ë˜í”„ í™•ì¸")
    print(f"  2. ë” ê¸¸ê²Œ í•™ìŠµ (10-20 epochs)")
    print(f"  3. í•™ìŠµë¥  ì¡°ì • ì‹œë„")
    print("="*70)


if __name__ == "__main__":
    main()
