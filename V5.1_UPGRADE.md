# DAWN v5.1: Token Residual Network

## üéØ What Changed

**v5.0 ‚Üí v5.1**: Replace **multiplicative token modulation** with **additive token residual**

### Before (v5.0): Multiplicative Gating

```python
# Low-rank modulation
mod = token_mod_B(F.relu(token_mod_A(token_sig)))  # d_model ‚Üí 32 ‚Üí d_ff
h = h_coarse * sigmoid(mod)  # Multiplicative gating
```

**Limitations**:
- ‚ùå Low-rank bottleneck (d_model ‚Üí 32 ‚Üí d_ff)
- ‚ùå Multiplicative gating (can suppress features)
- ‚ùå Limited expressiveness for token-specific adjustments

### After (v5.1): Additive Residual

```python
# Full MLP for fine adjustments
h_fine = token_residual(token_sig)  # d_model ‚Üí 256 ‚Üí d_ff
h = h_coarse + 0.1 * h_fine  # Additive residual (scaled)
```

**Benefits**:
- ‚úÖ Full capacity (d_model ‚Üí 256 ‚Üí d_ff, no bottleneck)
- ‚úÖ Additive residual (preserves base features + refinements)
- ‚úÖ Higher expressiveness for token-level adjustments
- ‚úÖ Scaled combination (0.1) for stable training

---

## üèóÔ∏è Architecture Comparison

### v5.0: Hierarchical Basis + Low-rank Modulation

```
Input tokens [B, S, D]
    ‚Üì
Coarse: Sentence FFN (shared across tokens)
    h = GELU(x @ W_up)    [B, S, d_ff]
    ‚Üì
Fine: Low-rank token modulation
    mod = MLP_32(token_sig)    [B, S, d_ff]
    h = h * sigmoid(mod)       [Multiplicative]
    ‚Üì
Output [B, S, D]
```

**Parameters**:
- token_mod_A: d_model √ó 32 = 256 √ó 32 = 8,192
- token_mod_B: 32 √ó d_ff = 32 √ó 512 = 16,384
- **Total**: 24,576 params

### v5.1: Hierarchical Basis + Token Residual

```
Input tokens [B, S, D]
    ‚Üì
Coarse: Sentence FFN (shared across tokens)
    h_coarse = GELU(x @ W_up)    [B, S, d_ff]
    ‚Üì
Fine: Token residual network
    h_fine = MLP_256(token_sig)    [B, S, d_ff]
    h = h_coarse + 0.1 * h_fine    [Additive]
    ‚Üì
Output [B, S, D]
```

**Parameters**:
- token_residual[0]: d_model √ó 256 = 256 √ó 256 = 65,536
- token_residual[2]: 256 √ó d_ff = 256 √ó 512 = 131,072
- **Total**: 196,608 params

**Parameter increase**: 24K ‚Üí 197K (+172K, ~8x)
- But still memory-efficient overall (sentence-level FFN shared)
- Better expressiveness-efficiency tradeoff

---

## üí° Key Improvements

### 1. **Expressiveness**

**v5.0**: Low-rank modulation (32-dim bottleneck)
- Limited capacity for token-specific adjustments
- Multiplicative gating can suppress features

**v5.1**: Full token residual (256-dim hidden)
- High capacity for fine-grained adjustments
- Additive combination preserves base structure

### 2. **Training Stability**

**v5.0**: Multiplicative gating
- sigmoid(mod) can collapse to 0 (suppresses all features)
- Requires careful initialization

**v5.1**: Scaled additive residual
- h_coarse always preserved (base structure)
- 0.1 scaling prevents overwhelming coarse features
- More stable gradient flow

### 3. **Interpretability**

**v5.0**: Multiplicative gating
- Hard to interpret (gating effect non-linear)

**v5.1**: Additive residual
- Clear decomposition: coarse (shared) + fine (individual)
- Easy to visualize contribution of each component

---

## üìä Expected Performance

| Aspect | v5.0 | v5.1 | Improvement |
|--------|------|------|-------------|
| Token expressiveness | Low-rank (32) | Full (256) | **8x capacity** |
| Parameter efficiency | 24K | 197K | +172K (acceptable) |
| Training stability | Moderate | **High** | Scaled residual |
| Interpretability | Low | **High** | Clear decomposition |
| Convergence speed | Baseline | **Faster** | Better gradients |

**Expected improvements**:
- ‚úÖ Better perplexity (more expressive token adjustments)
- ‚úÖ Faster convergence (stable gradients, no gating collapse)
- ‚úÖ Better generalization (additive residual = better regularization)

---

## üîß Implementation Details

### Code Changes

**1. models/model.py - BasisFFN**

```python
# Removed (v5.0)
self.token_mod_A = nn.Linear(d_model, mod_rank)
self.token_mod_B = nn.Linear(mod_rank, d_ff)

# Added (v5.1)
self.token_residual = nn.Sequential(
    nn.Linear(d_model, 256),
    nn.ReLU(),
    nn.Linear(256, d_ff)
)
self.residual_scale = 0.1
```

**2. Forward Pass**

```python
# v5.0: Multiplicative
mod = self.token_mod_B(F.relu(self.token_mod_A(token_sig)))
h = h * torch.sigmoid(mod)

# v5.1: Additive
h_coarse = F.gelu(torch.bmm(x, W_up))
h_fine = self.token_residual(token_sig)
h = h_coarse + self.residual_scale * h_fine
```

**3. Config Changes**

```yaml
# Removed
mod_rank: 32

# Added comment
# Token residual network: d_model ‚Üí 256 ‚Üí d_ff (fixed architecture)
```

### Files Modified

```
‚úÖ models/model.py          - Replace token modulation with token residual
‚úÖ scripts/train.py         - Remove mod_rank parameter
‚úÖ configs/train_config.yaml - Update to v5.1, remove mod_rank
```

---

## üöÄ Usage

### Training from Scratch

```bash
# v5.1 is not backward compatible with v5.0 checkpoints
# Must train from scratch

python scripts/train.py \
  --config configs/train_config.yaml \
  --save_dir checkpoints/v5.1_token_residual \
  --batch_size 32 \
  --num_epochs 10
```

### Loading v5.0 Checkpoints

‚ö†Ô∏è **Not supported** - architecture change (token_mod ‚Üí token_residual)

**Options**:
1. **Recommended**: Train from scratch with v5.1
2. **Alternative**: Manually convert weights (partial load):
   - Load basis_A/B, neuron_coef_A/B (compatible)
   - Initialize token_residual randomly
   - Fine-tune for a few epochs

---

## üìà Monitoring

### What to Watch

1. **Convergence Speed**
   - v5.1 should converge faster (better gradients)
   - If slower: increase residual_scale (0.1 ‚Üí 0.2)

2. **Perplexity**
   - v5.1 should achieve lower perplexity (more expressive)
   - If worse: check token_residual initialization

3. **Gradient Flow**
   - Monitor h_coarse vs h_fine magnitudes
   - Should be: |h_coarse| >> |h_fine| (coarse dominates)
   - If |h_fine| too large: reduce residual_scale

### Debug Commands

```python
# In model forward (debugging)
print(f"h_coarse: {h_coarse.abs().mean():.4f}")
print(f"h_fine: {h_fine.abs().mean():.4f}")
print(f"Fine contribution: {(0.1 * h_fine).abs().mean():.4f}")

# Expected:
# h_coarse: ~0.5-1.0 (dominant)
# h_fine: ~0.5-1.0 (similar magnitude)
# Fine contribution: ~0.05-0.1 (10x smaller due to scaling)
```

---

## üéì Design Rationale

### Why Additive Instead of Multiplicative?

**Additive (Residual)**:
- ‚úÖ Preserves base structure (h_coarse always present)
- ‚úÖ Easier optimization (direct gradient path)
- ‚úÖ Better regularization (small residuals = small changes)
- ‚úÖ Interpretable (coarse + refinement)

**Multiplicative (Gating)**:
- ‚ùå Can suppress features (sigmoid ‚Üí 0)
- ‚ùå Non-linear interaction (harder to optimize)
- ‚ùå Less interpretable (gating effect complex)

### Why 0.1 Scaling Factor?

**Too small (0.01)**:
- h_fine has negligible effect
- Essentially reverts to v5.0 without token adjustment

**Too large (0.5+)**:
- h_fine can overwhelm h_coarse
- Loses benefit of shared sentence structure

**Just right (0.1)**:
- h_coarse provides base structure (90%)
- h_fine provides refinements (10%)
- Stable training, clear hierarchy

### Why 256 Hidden Dimension?

**Options considered**:
- 128: Too small, limits expressiveness
- 512: Too large, unnecessary parameters
- **256**: Sweet spot (d_model = 256, d_ff = 512)
  - Matches d_model (no bottleneck)
  - Smaller than d_ff (efficient)
  - Proven effective in transformer FFNs

---

## ‚úÖ Checklist

Before training v5.1:

- [ ] Install dependencies: `pip install -r requirements.txt`
- [ ] Update config: `configs/train_config.yaml` (model_version: "5.1")
- [ ] Verify model: `python test_v5.py` (should pass)
- [ ] Start fresh: New checkpoint directory (no v5.0 resume)

During training:

- [ ] Monitor convergence speed (should be faster)
- [ ] Check gradient flow (coarse >> fine)
- [ ] Watch perplexity (should be lower)

After training:

- [ ] Analyze results: `python scripts/analyze_dawn.py`
- [ ] Compare with v5.0 baseline
- [ ] Check token residual contribution (h_fine magnitudes)

---

## üî¨ Ablation Study (Recommended)

To validate v5.1 improvements, compare:

| Config | Description | Expected Result |
|--------|-------------|-----------------|
| v5.0 | Low-rank modulation (baseline) | Baseline perplexity |
| v5.1 (0.05) | Token residual, scale=0.05 | Slightly better |
| v5.1 (0.1) | Token residual, scale=0.1 | **Best** |
| v5.1 (0.2) | Token residual, scale=0.2 | Similar to 0.1 |
| v5.1 (no coarse) | Only h_fine (no sentence FFN) | Worse (loses efficiency) |
| v5.1 (no fine) | Only h_coarse (no token residual) | Worse (loses expressiveness) |

**Expected conclusion**: v5.1 (0.1) achieves best balance

---

## üìù Summary

**What**: Replace low-rank multiplicative modulation with full token residual network

**Why**:
- More expressive (full capacity vs bottleneck)
- More stable (additive vs multiplicative)
- Better interpretable (coarse + fine decomposition)

**Result**:
- Better perplexity (more expressive adjustments)
- Faster convergence (stable gradients)
- Clear architecture (efficiency + expressiveness)

**Trade-off**: +172K parameters (acceptable, still memory-efficient)

---

**Version**: v5.0 ‚Üí v5.1
**Commit**: `488ddd4` - Upgrade to DAWN v5.1: Token Residual Network
**Branch**: `claude/implement-neuron-router-012KLd9yJ7XZcnX3Bo6Ljg5V`
**Status**: ‚úÖ Committed and pushed
